{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T17:04:19.110921Z",
     "start_time": "2025-01-25T17:04:19.098918Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from graph_loader import load_graphs\n",
    "\n",
    "from part import Part\n",
    "from typing import Set\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from evaluation import MyPredictionModel\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from graph import Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T21:00:12.182256Z",
     "start_time": "2025-01-25T21:00:12.163686Z"
    }
   },
   "outputs": [],
   "source": [
    "# MPS for Metal acceleration for Mac\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps else \"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_built() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "SEED = 0\n",
    "random.seed(SEED)\n",
    "# setgrad = lambda g, *ms: [setattr(p,'requires_grad', g) for m in ms for p in m.parameters() ]  \n",
    "\n",
    "# Hyperparameter:\n",
    "input_dim = 4542        # Fest\n",
    "hidden_dim = 1024       # 128 / 1024 / 2048 / 4542   -> Input = obere schranke\n",
    "output_dim = 2271       # Fest\n",
    "learning_rate = 0.0005  # 0.01 / 0.001 / 0.0005\n",
    "num_epochs = 50         # Fest\n",
    "batch_size = 128          # 16 / 32 / 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. GraphDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T17:03:23.459236Z",
     "start_time": "2025-01-25T17:03:23.441235Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, file_path: str, train=False, validation=False, test=False, seed=42):\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"Dataset file not found at {file_path}\")\n",
    "\n",
    "        self.graphs = load_graphs(file_path)\n",
    "\n",
    "        if sum([train, validation, test]) != 1:\n",
    "            raise ValueError(\"Exactly one of 'train', 'validation', or 'test' must be True.\")\n",
    "\n",
    "\n",
    "        # Create global mapping for unique parts\n",
    "        self.family_part_dict = {}\n",
    "\n",
    "\n",
    "        unique_parts = set()\n",
    "        for graph in self.graphs:\n",
    "            parts = graph.get_parts()\n",
    "            for part in parts:\n",
    "                unique_parts.add(int(part.get_part_id()))\n",
    "                self.family_part_dict[int(part.get_part_id())] = int(part.get_family_id())\n",
    "\n",
    "        # unique parts and mapping across all graphs (not just within a certain split)\n",
    "        unique_parts = sorted(list(unique_parts))\n",
    "        self.total_global_part_to_idx = {part: idx for idx, part in enumerate(unique_parts)} # mapping part_id to index\n",
    "        self.idx_to_part_id = {idx: part for part, idx in self.total_global_part_to_idx.items()}  # Reverse mapping\n",
    "\n",
    "        self.total_num_unique_parts = max(unique_parts)+1\n",
    "        # self.total_num_unique_parts = len(unique_parts)\n",
    "\n",
    "        # Split: 70% training, 15% validation, 15% test\n",
    "        train_graphs, test_graphs = train_test_split(self.graphs, test_size=0.3, random_state=seed)\n",
    "        validation_graphs, test_graphs = train_test_split(test_graphs, test_size=0.5, random_state=seed)\n",
    "\n",
    "        # Create Train and Test Split:\n",
    "        if train:\n",
    "            self.graphs = train_graphs\n",
    "        elif validation:\n",
    "            self.graphs = validation_graphs\n",
    "        elif test:\n",
    "            self.graphs = test_graphs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Returns the parts and the corresponding graph at the given index.\n",
    "        '''\n",
    "        return self.graphs[idx].get_parts(), self.graphs[idx]\n",
    "\n",
    "    def encode_features(self, parts):\n",
    "        \"\"\"\n",
    "            1. Sortieren alle parts in graphen nach partID\n",
    "            2. Erstellen Frequency Encoding für part\n",
    "            3. Erstellen one Hot Encoding für parts\n",
    "            4. Combines Encodings in einen gestackten vektor\n",
    "        \"\"\"\n",
    "\n",
    "        # sort parts by part_id and convert to list\n",
    "        parts = sorted(parts, key=lambda part: int(part.get_part_id()))\n",
    "        parts = list(parts)\n",
    "        \n",
    "        combined_tensors = []\n",
    "        # frequency encoding\n",
    "        part_frequency_tensor = self.frequency_encoding(parts)              # Für alle Parts einmal pro Graph\n",
    "        for part in parts:\n",
    "            # one hot encoding\n",
    "            one_hot_encoded = self.one_hot_encoding(part)                   # Pro part meheree male pro Graph\n",
    "            combined_tensor = torch.cat((part_frequency_tensor, one_hot_encoded), dim=0)\n",
    "            combined_tensors.append(combined_tensor)\n",
    "        # transform list of tensors to tensor\n",
    "        combined_tensors = torch.stack(combined_tensors)\n",
    "        return combined_tensors\n",
    "\n",
    "    # TODO Werte checken - wird passen\n",
    "    def frequency_encoding(self, parts):\n",
    "        \"\"\" \n",
    "        Frequency encoding of parts in a graph including the mapping to the global part index.\n",
    "        \"\"\"\n",
    "        part_frequency_tensor = np.zeros(self.total_num_unique_parts, dtype=np.int32)\n",
    "        for part in parts:\n",
    "            part_id = int(part.get_part_id())\n",
    "            part_frequency_tensor[part_id] += 1\n",
    "\n",
    "            # mapped_id = self.total_global_part_to_idx[part_id]\n",
    "            # part_frequency_tensor[mapped_id] += 1\n",
    "        # cast to tensor\n",
    "        part_frequency_tensor = torch.tensor(part_frequency_tensor, dtype=torch.float32)\n",
    "        return part_frequency_tensor\n",
    "\n",
    "    # TODO Werte checken - wird passen\n",
    "    def one_hot_encoding(self, part):\n",
    "        \"\"\"\n",
    "        One-hot encoding of part including the mapping to the global part index.\n",
    "        \"\"\"\n",
    "        one_hot_encoded = np.zeros(self.total_num_unique_parts, dtype=np.int32)\n",
    "        part_id = int(part.get_part_id())\n",
    "        one_hot_encoded[part_id] = 1\n",
    "        # mapped_id = self.total_global_part_to_idx[part_id]\n",
    "        # one_hot_encoded[mapped_id] = 1\n",
    "        # cast to tensor\n",
    "        one_hot_encoded = torch.tensor(one_hot_encoded, dtype=torch.float32)\n",
    "        return one_hot_encoded\n",
    "\n",
    "    \n",
    "    def graph_to_neighbour_matrix(self, graph):\n",
    "        \"\"\"\n",
    "        This will not create the adjacency matrix, but rather for each node a tensor with the neighbours. \n",
    "        These tensors are then stacked to create the neighbour matrix.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        Erstellt für Graphn die gesamte Adjazenzmatrix: oben / x: Source, links / y: target\n",
    "\n",
    "        Genauso viele Spalten wie Parts in graphen (e.g. für gleiche PartIDs mehere Spalten) und insg. Reihen pro spalte für 2000 parts\n",
    "\n",
    "            58, 58, 58, 58 1621 source\n",
    "            1   2   3   4  5\n",
    "        1\n",
    "        2\n",
    "        3               One-hot encoded\n",
    "        ...\n",
    "        target\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        neighbours = graph.get_edges()\n",
    "        # sort nodes by part_id -> because our features were also sorted according to part_id\n",
    "        nodes = sorted(graph.get_nodes(), key=lambda node: int(node.get_part().get_part_id()))\n",
    "\n",
    "        neighbour_matrix = []\n",
    "\n",
    "        for node in nodes:\n",
    "            \n",
    "            neighbour_tensor = torch.zeros(self.total_num_unique_parts, dtype=torch.float32)\n",
    "            # find the neighbours of each node\n",
    "            for current_neighbour in neighbours[node]:\n",
    "                neighbour_id = int(current_neighbour.get_part().get_part_id())\n",
    "                neighbour_tensor[neighbour_id] = 1\n",
    "                # mapped_id = self.total_global_part_to_idx[neighbour_id]\n",
    "                # neighbour_tensor[mapped_id] = 1\n",
    "\n",
    "            neighbour_matrix.append(neighbour_tensor)\n",
    "        \n",
    "        # transform list of tensors to tensor\n",
    "        neighbour_matrix = torch.stack(neighbour_matrix)\n",
    "        return neighbour_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. PartDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T17:03:29.505087Z",
     "start_time": "2025-01-25T17:03:29.484845Z"
    }
   },
   "outputs": [],
   "source": [
    "class PartDataset(Dataset):\n",
    "    def __init__(self, graph_dataset):\n",
    "        \"\"\"\n",
    "        Initialisiert das PartDataset basierend auf dem bestehenden GraphDataset.\n",
    "        Erstellt ein Datenset, welches jeweils als feature die Kombination aus Frequency Encoding und One-Hot Encoding enthält und als label die Nachbarn jedes Knotens.\n",
    "        :param graph_dataset: Instanz von GraphDataset.\n",
    "        \"\"\"\n",
    "        self.features = []\n",
    "        self.labels = []\n",
    "\n",
    "        \"\"\"\n",
    "            Wir holen uns parts aus graphen, wir sortieren die PartIDs\n",
    "            Erzeugen wir frequency (für graphen) und one hot (für einzelne Parts) vektoren\n",
    "        \"\"\"\n",
    "\n",
    "        for parts, graph in graph_dataset:\n",
    "\n",
    "            # Feature 1: Frequency encoded graph using part_ids\n",
    "            part_ids_graph = graph.get_parts()\n",
    "            part_ids_graph = [int(part.get_part_id()) for part in part_ids_graph]\n",
    "            feature_graph = torch.zeros(graph_dataset.total_num_unique_parts, dtype=torch.float)\n",
    "            for part_id in part_ids_graph:\n",
    "                feature_graph[part_id] += 1\n",
    "\n",
    "            # Feature 2: One Hot encoding per Part + neighboring matrix kombiniert aufgebaut\n",
    "            edges = graph.get_edges()\n",
    "            for source_node, target_nodes in edges.items():\n",
    "                source_id = int(source_node.get_part().get_part_id())\n",
    "\n",
    "                # One-hot encode the source_id with pytorch tensors\n",
    "                feature_source_id = torch.zeros(graph_dataset.total_num_unique_parts, dtype=torch.float)\n",
    "                feature_source_id[source_id] = 1\n",
    "\n",
    "                target = torch.zeros(graph_dataset.total_num_unique_parts, dtype=torch.float)\n",
    "                for target_node in target_nodes:\n",
    "                    target_id = int(target_node.get_part().get_part_id())\n",
    "                    target[target_id] = 1\n",
    "\n",
    "                self.labels.append(target)\n",
    "                self.features.append(torch.cat([feature_graph, feature_source_id]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Gibt die Features und das Label für den angegebenen Index zurück.\n",
    "        \"\"\"\n",
    "        return self.features[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Setup DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T17:03:34.869103Z",
     "start_time": "2025-01-25T17:03:31.022914Z"
    }
   },
   "outputs": [],
   "source": [
    "training_set = GraphDataset(\"data/graphs.dat\", train = True, seed=SEED)\n",
    "validation_set = GraphDataset(\"data/graphs.dat\", validation = True, seed=SEED)\n",
    "testing_set = GraphDataset(\"data/graphs.dat\", test = True, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T17:03:39.527404Z",
     "start_time": "2025-01-25T17:03:36.306550Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ---------------------\n",
      "4542\n",
      "118     1.0\n",
      "Labels / Edges: ---------------\n",
      "38    1.0\n",
      "83    1.0\n",
      "119    1.0\n",
      "127    1.0\n",
      "686    1.0\n"
     ]
    }
   ],
   "source": [
    "# Erstelle ein PartDataset aus deinem GraphDataset\n",
    "train_part_dataset = PartDataset(training_set)      # --> Länge 59275 alle parts in allen Graphen (Alle graph - PArtID combis)\n",
    "val_part_dataset = PartDataset(validation_set)\n",
    "\n",
    "training_set.graphs[0].draw()\n",
    "example_column = 0      # Max 59275\n",
    "\n",
    "print(\"Features: ---------------------\")\n",
    "print(len(train_part_dataset.features[0]))\n",
    "for i, part in enumerate(train_part_dataset.features[example_column]):\n",
    "    if i > output_dim and train_part_dataset.features[example_column][i].item() > 0:\n",
    "        print(i - output_dim, \"   \", train_part_dataset.features[example_column][i].item())\n",
    "\n",
    "print(\"Labels / Edges: ---------------\")\n",
    "for i, edge in enumerate(train_part_dataset.labels[example_column]):\n",
    "    if train_part_dataset.labels[example_column][i].item() > 0:\n",
    "        print(i, \"  \", train_part_dataset.labels[example_column][i].item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T17:03:41.792157Z",
     "start_time": "2025-01-25T17:03:41.779156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59477\n",
      "4542\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "0.0\n",
      "59477\n",
      "2271\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Train part Dataset:\n",
    "print(len(train_part_dataset.features))\n",
    "print(len(train_part_dataset.features[0]))\n",
    "print(train_part_dataset.features[0])\n",
    "print(train_part_dataset.features[0][3].item())\n",
    "\n",
    "print(len(train_part_dataset.labels))\n",
    "print(len(train_part_dataset.labels[0]))\n",
    "print(train_part_dataset.labels[0])\n",
    "print(train_part_dataset.labels[0][3].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T17:03:48.716182Z",
     "start_time": "2025-01-25T17:03:48.560722Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source -> part_id 118\n",
      "target -> part_id 38\n",
      "target -> part_id 83\n",
      "target -> part_id 119\n",
      "target -> part_id 127\n",
      "target -> part_id 686\n",
      "--------------------------------------------------\n",
      "source -> part_id 127\n",
      "target -> part_id 118\n",
      "--------------------------------------------------\n",
      "source -> part_id 119\n",
      "target -> part_id 118\n",
      "--------------------------------------------------\n",
      "source -> part_id 38\n",
      "target -> part_id 118\n",
      "--------------------------------------------------\n",
      "source -> part_id 83\n",
      "target -> part_id 118\n",
      "--------------------------------------------------\n",
      "source -> part_id 686\n",
      "target -> part_id 118\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Print all Edges in Graph 0:\n",
    "for j in range(0,6):\n",
    "    for i, element in enumerate(train_part_dataset[j][0]):\n",
    "        if i >= (output_dim-1) and element >= 1:\n",
    "            print(f\"source -> part_id {i - output_dim}\")\n",
    "\n",
    "    # neighbours\n",
    "    for i, element in enumerate(train_part_dataset[j][1]):\n",
    "        if element > 0:\n",
    "            print(f\"target -> part_id {i}\")\n",
    "    \n",
    "    print(50*\"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neighbour Prediction\n",
    "\n",
    "Für jeden Part die wahrscheinlichsten Nachbarn vorhersagen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T21:00:16.828271Z",
     "start_time": "2025-01-25T21:00:16.805938Z"
    }
   },
   "outputs": [],
   "source": [
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')\n",
    "                if layer.bias is not None:\n",
    "                    nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "        \n",
    "\n",
    "    def train_ffnn(self, train_dataset, val_dataset, num_epochs, batch_size, learning_rate, device):\n",
    "\n",
    "        # Move model to device\n",
    "        self.to(device)\n",
    "\n",
    "        # Initialize criterion and optimizer\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Prepare DataLoaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Document Losses and Accuracies\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_accuracies = []\n",
    "        val_accuracies = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            # Training Phase:\n",
    "            self.train()\n",
    "            running_loss = 0.0\n",
    "            correct_train = 0\n",
    "            total_train = 0\n",
    "\n",
    "            for batch_inputs, batch_labels in train_loader:\n",
    "                batch_inputs, batch_labels = batch_inputs.to(device), batch_labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(batch_inputs)  # Forward pass\n",
    "                loss = criterion(outputs, batch_labels)  # Compute loss\n",
    "\n",
    "                # Backwards Pass and Update weights:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # Calculate training accuracy\n",
    "                predictions = (outputs >= 0.5).float()  # Round probabilities to 0 or 1\n",
    "                correct_train += (predictions == batch_labels).sum().item()  # Count correct predictions\n",
    "                total_train += batch_labels.numel()  # Total number of labels\n",
    "\n",
    "            avg_train_loss = running_loss / len(train_loader)\n",
    "            train_losses.append(avg_train_loss)\n",
    "            train_accuracy = correct_train / total_train  # Accuracy as ratio\n",
    "            train_accuracies.append(train_accuracy)\n",
    "\n",
    "            # Validation Phase:\n",
    "            self.eval()\n",
    "            running_val_loss = 0.0\n",
    "            correct_val = 0\n",
    "            total_val = 0\n",
    "            with torch.no_grad():\n",
    "                for val_inputs, val_targets in val_loader:\n",
    "                    val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n",
    "                    val_outputs = self(val_inputs)  # Forward pass\n",
    "                    val_loss = criterion(val_outputs, val_targets)  # Compute loss\n",
    "                    running_val_loss += val_loss.item()\n",
    "\n",
    "                    # Calculate validation accuracy\n",
    "                    val_predictions = (val_outputs >= 0.5).float()  # Round probabilities to 0 or 1\n",
    "                    correct_val += (val_predictions == val_targets).sum().item()  # Count correct predictions\n",
    "                    total_val += val_targets.numel()  # Total number of labels\n",
    "\n",
    "            avg_val_loss = running_val_loss / len(val_loader)\n",
    "            val_losses.append(avg_val_loss)\n",
    "            val_accuracy = correct_val / total_val  # Accuracy as ratio\n",
    "            val_accuracies.append(val_accuracy)\n",
    "\n",
    "            # Print epoch summary\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_train_loss:.6f}, Train Acc: {train_accuracy:.4f}, \"\n",
    "                f\"Val Loss: {avg_val_loss:.6f}, Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "        return train_losses, val_losses, train_accuracies, val_accuracies\n",
    "\n",
    "\n",
    "    def test_ffnn(self, test_dataset, batch_size, device):\n",
    "        self.eval()\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        criterion = nn.BCELoss()\n",
    "\n",
    "        running_test_loss = 0.0\n",
    "        correct_test = 0\n",
    "        total_test = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for test_inputs, test_targets in test_loader:\n",
    "                test_inputs, test_targets = test_inputs.to(device), test_targets.to(device)\n",
    "                test_outputs = self(test_inputs)\n",
    "        \n",
    "                test_loss = criterion(test_outputs, test_targets)\n",
    "                running_test_loss += test_loss.item()\n",
    "\n",
    "                # Calculate test accuracy\n",
    "                test_predictions = (test_outputs >= 0.5).float()\n",
    "                correct_test += (test_predictions == test_targets).sum().item()\n",
    "                total_test += test_targets.numel()\n",
    "        \n",
    "        avg_test_loss = running_test_loss / len(test_loader)\n",
    "        test_accuracy = correct_test / total_test\n",
    "\n",
    "        print(f\"Test Loss: {avg_test_loss:.6f}, Test Acc: {test_accuracy:.4f}\")\n",
    "        return avg_test_loss, test_accuracy\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    def train_ffnn(self, train_dataset, val_dataset, num_epochs, batch_size, learning_rate, device):\n",
    "\n",
    "        # Move model to device\n",
    "        self.to(device)\n",
    "\n",
    "        # Initialize criterion and optimizer\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Prepare DataLoaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Document Losses:\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            # Training Phase:\n",
    "            self.train()\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for batch_inputs, batch_labels in train_loader:\n",
    "                batch_inputs, batch_labels = batch_inputs.to(device), batch_labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(batch_inputs)  # Forward pass\n",
    "                loss = criterion(outputs, batch_labels)  # Compute loss\n",
    "\n",
    "                # Backwards Pass and Update weights:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            avg_train_loss = running_loss / len(train_loader)\n",
    "            train_losses.append(avg_train_loss)\n",
    "\n",
    "            # Validation Phase:\n",
    "            self.eval()\n",
    "            running_val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for val_inputs, val_targets in val_loader:\n",
    "                    val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n",
    "                    val_outputs = self(val_inputs)  # Forward pass\n",
    "                    val_loss = criterion(val_outputs, val_targets)  # Compute loss\n",
    "                    running_val_loss += val_loss.item()\n",
    "\n",
    "            avg_val_loss = running_val_loss / len(val_loader)\n",
    "            val_losses.append(avg_val_loss)\n",
    "\n",
    "            # Print epoch summary\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_train_loss}, Val Loss: {avg_val_loss}\")\n",
    "\n",
    "        return train_losses, val_losses\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T21:02:18.451570Z",
     "start_time": "2025-01-25T21:00:19.935886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 0.020524, Train Acc: 0.9959, Val Loss: 0.003501, Val Acc: 0.9993\n",
      "Epoch [2/50], Train Loss: 0.002597, Train Acc: 0.9994, Val Loss: 0.002052, Val Acc: 0.9994\n",
      "Epoch [3/50], Train Loss: 0.001455, Train Acc: 0.9996, Val Loss: 0.001162, Val Acc: 0.9996\n",
      "Epoch [4/50], Train Loss: 0.000797, Train Acc: 0.9997, Val Loss: 0.000695, Val Acc: 0.9998\n",
      "Epoch [5/50], Train Loss: 0.000471, Train Acc: 0.9999, Val Loss: 0.000459, Val Acc: 0.9999\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m ffnn \u001b[38;5;241m=\u001b[39m FFNN(input_dim, hidden_dim, output_dim)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m train_losses, val_losses, train_accuracies, val_accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mffnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_ffnn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_part_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_part_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Prepare results for saving\u001b[39;00m\n\u001b[1;32m     22\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[9], line 51\u001b[0m, in \u001b[0;36mFFNN.train_ffnn\u001b[0;34m(self, train_dataset, val_dataset, num_epochs, batch_size, learning_rate, device)\u001b[0m\n\u001b[1;32m     48\u001b[0m correct_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     49\u001b[0m total_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_inputs, batch_labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     52\u001b[0m     batch_inputs, batch_labels \u001b[38;5;241m=\u001b[39m batch_inputs\u001b[38;5;241m.\u001b[39mto(device), batch_labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     54\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ai-project/lib/python3.8/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ai-project/lib/python3.8/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ai-project/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ai-project/lib/python3.8/site-packages/torch_geometric/loader/dataloader.py:47\u001b[0m, in \u001b[0;36mCollater.__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(elem)(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)))\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, Sequence) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)]\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataLoader found invalid type: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(elem)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ai-project/lib/python3.8/site-packages/torch_geometric/loader/dataloader.py:47\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(elem)(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)))\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, Sequence) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)]\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataLoader found invalid type: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(elem)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ai-project/lib/python3.8/site-packages/torch_geometric/loader/dataloader.py:33\u001b[0m, in \u001b[0;36mCollater.__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Batch\u001b[38;5;241m.\u001b[39mfrom_data_list(\n\u001b[1;32m     28\u001b[0m         batch,\n\u001b[1;32m     29\u001b[0m         follow_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_batch,\n\u001b[1;32m     30\u001b[0m         exclude_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexclude_keys,\n\u001b[1;32m     31\u001b[0m     )\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdefault_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, TensorFrame):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_frame\u001b[38;5;241m.\u001b[39mcat(batch, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ai-project/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ai-project/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ai-project/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv \n",
    "import os\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ffnn = FFNN(input_dim, hidden_dim, output_dim).to(device)\n",
    "\n",
    "# Train the model\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = ffnn.train_ffnn(\n",
    "    train_dataset=train_part_dataset,\n",
    "    val_dataset=val_part_dataset,\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "\n",
    "# Prepare results for saving\n",
    "results = []\n",
    "for epoch in range(len(train_losses)):\n",
    "    results.append({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": train_losses[epoch],\n",
    "        \"train_accuracy\": train_accuracies[epoch],\n",
    "        \"val_loss\": val_losses[epoch],\n",
    "        \"val_accuracy\": val_accuracies[epoch]\n",
    "    })\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "output_folder = \"hyperparameter_tuning\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save results to CSV\n",
    "output_file = os.path.join(output_folder, f\"{hidden_dim}-{batch_size}-{learning_rate}-training_results.csv\")\n",
    "with open(output_file, \"w\", newline=\"\") as csvfile:\n",
    "    fieldnames = [\"epoch\", \"train_loss\", \"train_accuracy\", \"val_loss\", \"val_accuracy\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    writer.writerows(results)\n",
    "\n",
    "print(\"Training results saved to training_results.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# Save the trained model\n",
    "file_path = \"ffnn_model.pth\"\n",
    "torch.save(ffnn.state_dict(), file_path)\n",
    "print(f\"Model saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate FNN Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T20:14:55.028747Z",
     "start_time": "2025-01-25T20:14:54.979674Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 tensor(1.)\n",
      "83 tensor(1.)\n",
      "118 tensor(1.)\n",
      "119 tensor(1.)\n",
      "127 tensor(1.)\n",
      "686 tensor(1.)\n",
      "2389 tensor(1.)\n",
      "-----------\n",
      "1411 1.0   ---   0.9995076656341553\n",
      "1411 1.0   ---   0.9995076656341553\n"
     ]
    }
   ],
   "source": [
    "for i, el in enumerate(train_part_dataset[0][0]):\n",
    "    if el >= 1:\n",
    "        print(i, el)\n",
    "\n",
    "output = ffnn(train_part_dataset.features[2389].to(device))\n",
    "target = train_part_dataset.labels[2389]\n",
    "\n",
    "probs = ffnn(train_part_dataset[6][0].to(device))\n",
    "probs.max()\n",
    "\n",
    "print(\"-----------\")\n",
    "for i in range(0, output_dim):\n",
    "    if target[i].item() > 0:\n",
    "        print(i, target[i].item(), \"  ---  \", output[i].item())\n",
    "    if output[i].item() > 0.5:\n",
    "        print(i, target[i].item(), \"  ---  \", output[i].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jonas Graph Prediction\n",
    "\n",
    "Nachdem wir nun eine Möglichkeit haben, pro Knoten/Part die Wahrscheinlichkeit für den/die direkten Nachbarn zu berechnen, müssen wir daraus nur noch den Gaphen zusammenbauen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T17:04:10.210844Z",
     "start_time": "2025-01-25T17:04:10.194844Z"
    }
   },
   "outputs": [],
   "source": [
    "class GraphPrediction(MyPredictionModel):\n",
    "    def __init__(self, model, dataset):\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.total_global_part_to_idx = dataset.total_global_part_to_idx  # PartID -> Index mapping\n",
    "    \n",
    "    def predict_graph(self, parts: Set[Part]) -> Graph:\n",
    "        '''We get a set of parts and build a fully connected graph out of this. The edges are given a weight obtained from the model.\n",
    "        Then we want to minimize the fully connected graph to a graph with the fewest edges possible. \n",
    "        We do this by only keeping the edges with the highest weights. This is the same as keeping the minimal 1-weight edges.\n",
    "        This can be done by getting the minimal spanning tree of the graph. \n",
    "        '''\n",
    "\n",
    "        # ich sortiere die Parts nach ihrer ID\n",
    "        parts = sorted(parts, key=lambda part: int(part.get_part_id()))\n",
    "        parts = list(parts)\n",
    "\n",
    "        # ich encode die parts -> \n",
    "        part_frequency_tensor = self.dataset.frequency_encoding(parts)\n",
    "        predicted_neighbours = []\n",
    "        for part in parts:\n",
    "            one_hot_encoded = self.dataset.one_hot_encoding(part)\n",
    "            combined_tensor = torch.cat((part_frequency_tensor, one_hot_encoded), dim=0)\n",
    "            # ich setze die parts in mein Modell ein und erhalte dann eine Matrix mit den Wahrscheinlichkeiten der Kanten\n",
    "            neighbour_prediction = ffnn(combined_tensor.to(device))\n",
    "            # print(f\"neighbour_prediction max: {neighbour_prediction.max()}\")\n",
    "            predicted_neighbours.append(neighbour_prediction)\n",
    "        \n",
    "        # cast to tensor\n",
    "        predicted_neighbours = torch.stack(predicted_neighbours) # with global mapping of part ids\n",
    "\n",
    "        # print(f\"MAX {predicted_neighbours.max()}\")\n",
    "\n",
    "        # TODO: GLOBAL MAPPING FOR PART IDS\n",
    "        # erstelle einen fully connected graph\n",
    "        fully_connected_graph = self.build_graph_with_weights(parts, predicted_neighbours) \n",
    "        # self.draw_tree(fully_connected_graph)\n",
    "\n",
    "        # erstelle einen minimal spanning tree\n",
    "        minimal_spanning_tree = self.build_minimum_spanning_tree(fully_connected_graph)\n",
    "        # self.draw_tree(minimal_spanning_tree)\n",
    "\n",
    "        # self.draw_tree(minimal_spanning_tree, title=\"Minimum Spanning Tree\")\n",
    "\n",
    "        predicted_graph = self.convert_nx_graph_to_custom_graph(minimal_spanning_tree, parts)\n",
    "\n",
    "        # predicted_graph.draw()\n",
    "\n",
    "        return predicted_graph\n",
    "\n",
    "\n",
    "    # Angenommen: parts ist eine Liste der Eingabe-Parts mit Länge 6\n",
    "    # Angenommen: predicted_neighbours ist die 6 x 1089 Tensor-Matrix mit den Wahrscheinlichkeiten\n",
    "    # predicted_neighbours[i][j] = Wahrscheinlichkeit, dass Part i mit Part j verbunden ist\n",
    "    def convert_nx_graph_to_custom_graph(self, nx_graph, parts):\n",
    "        # TODO: DOPPELTE PARTS WERDEN NUR EINMAL HINZUGEFÜGT\n",
    "        #TODO: GLOBAL MAPPING FOR PART IDS\n",
    "        custom_graph = Graph()\n",
    "        \n",
    "        # Add edges (undirected) to the custom graph\n",
    "\n",
    "        for u, v, edge_data in nx_graph.edges(data=True):\n",
    "            # Extract the global IDs for the nodes (u and v)\n",
    "            index_source = int(u.split('-')[1])  # Extract Index in parts list  from \"Part-{i}-{part_id}\" format\n",
    "            index_target = int(v.split('-')[1])  # Extract Index in parts list from \"Part-{j}-{part_id}\" format\n",
    "            \n",
    "            # Add an undirected edge between the two Parts\n",
    "            custom_graph.add_undirected_edge(parts[index_source], parts[index_target])\n",
    "            # custom_graph.draw()\n",
    "        \n",
    "        return custom_graph\n",
    "\n",
    "\n",
    "    def build_graph_with_weights(self, parts, predicted_neighbours):\n",
    "        # TODO: global mapping for parts is missing - for predicted_neighbours it is implemented\n",
    "        G = nx.Graph()  # Create an undirected graph\n",
    "\n",
    "        # Add all parts as nodes with unique identifiers\n",
    "        for i, part in enumerate(parts):\n",
    "            G.add_node(f\"Part-{i}-{part.get_part_id()}\", part=part)\n",
    "\n",
    "        # Add edges between the nodes (only for input parts)\n",
    "        for i, part in enumerate(parts):\n",
    "            for j, other_part in enumerate(parts):\n",
    "                if i != j:  # Avoid self-loops\n",
    "                    weight = 1 - predicted_neighbours[i][j].item()  # Convert probability to distance\n",
    "                    # Add an edge between the nodes with the computed weight\n",
    "                    G.add_edge(f\"Part-{i}-{part.get_part_id()}\", f\"Part-{j}-{other_part.get_part_id()}\", weight=weight)\n",
    "\n",
    "        return G\n",
    "\n",
    "    def build_minimum_spanning_tree(self, graph):\n",
    "        # Compute the Minimum Spanning Tree (MST)\n",
    "        mst = nx.minimum_spanning_tree(graph, weight=\"weight\")\n",
    "        # self.draw_tree(mst)\n",
    "        return mst\n",
    "\n",
    "\n",
    "    def draw_tree(self, tree, title=\"Minimum Spanning Tree\"):\n",
    "        # Generate layout positions for the tree (spring layout works well for small graphs)\n",
    "        pos = nx.spring_layout(tree)  # You can use other layouts like nx.kamada_kawai_layout(tree)\n",
    "\n",
    "        # Draw the graph\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        nx.draw(tree, pos, with_labels=True, node_size=500, font_size=10, node_color=\"skyblue\", edge_color=\"gray\")\n",
    "\n",
    "        # Draw edge weights\n",
    "        edge_labels = nx.get_edge_attributes(tree, 'weight')\n",
    "        nx.draw_networkx_edge_labels(tree, pos, edge_labels=edge_labels, font_size=9)\n",
    "\n",
    "        # Add a title\n",
    "        plt.title(title)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Builder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem wir nun eine Möglichkeit haben, pro Knoten/Part die Wahrscheinlichkeit für den/die direkten Nachbarn zu berechnen, müssen wir daraus nur noch den Gaphen zusammenbauen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T20:45:49.555971Z",
     "start_time": "2025-01-25T20:45:49.533078Z"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "\n",
    "class GraphBuilder(MyPredictionModel):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = None\n",
    "\n",
    "    def load_model(self, input_dim, hidden_dim, output_dim, file_path: str) -> MyPredictionModel:\n",
    "\n",
    "        # Ensure the model is initialized with the same architecture\n",
    "        model = FFNN(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "\n",
    "        # Load the saved state_dict with weights_only=True for security\n",
    "        state_dict = torch.load(file_path, weights_only=True)\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "        # Set the model to evaluation mode (important for inference)\n",
    "        model.eval()\n",
    "\n",
    "        # Store it in the class instance\n",
    "        self.model = model\n",
    "        return model\n",
    "\n",
    "    def predict_graph(self, parts: Set[Part]) -> Graph:\n",
    "\n",
    "        # 41, 41, 93, 163, 163, 1240\n",
    "        part_ids = []\n",
    "        for part in parts:\n",
    "            part_ids.append(part.get_part_id())\n",
    "\n",
    "        # Generate Probability Matrix:\n",
    "        output_matrix = self.__query_probabilities(part_ids)\n",
    "\n",
    "        # Convert the output_matrix to a dictionary\n",
    "        probabilities_dict = {}             # probabilities_dict[source][target]\n",
    "        for i, part_id in enumerate(part_ids):\n",
    "            column_probabilities = output_matrix[:, i]\n",
    "            probabilities_dict[part_id] = {row_idx: column_probabilities[row_idx].item() for row_idx in range(output_matrix.size(0))}\n",
    "\n",
    "        # print(\"Hier die Probability---------\", probabilities_dict)\n",
    "\n",
    "        # Build up the Graph:\n",
    "        predicted_graph = self.__build_predicted_graph(parts, probabilities_dict)\n",
    "\n",
    "        return predicted_graph\n",
    "\n",
    "    def __query_probabilities(self, part_ids: list):\n",
    "\n",
    "        # Generate Frequency Encoding:\n",
    "        frequency_vector = torch.zeros(output_dim, dtype=torch.float32)\n",
    "        for part_id in part_ids:\n",
    "            frequency_vector[int(part_id)] += 1\n",
    "\n",
    "        # Initialize a tensor to hold the outputs (2271 rows, 6 columns)\n",
    "        output_matrix = torch.zeros(output_dim, len(part_ids), dtype=torch.float32)  # Shape: (2271, 6)\n",
    "\n",
    "        # Generate One Hot Encoding and combined Vector:\n",
    "        for i, part_id in enumerate(part_ids):\n",
    "            one_hot_vector = torch.zeros(output_dim, dtype=torch.float32) # Initialize a zero vector for one-hot encoding\n",
    "            one_hot_vector[int(part_id)] = 1.0                      # Set the part_id index to 1\n",
    "            combined_vector = torch.cat([frequency_vector, one_hot_vector], dim=0)\n",
    "\n",
    "            # Query Model\n",
    "            model_output_per_part = ffnn(combined_vector.unsqueeze(0).to(device))\n",
    "\n",
    "            # Squeeze the output and assign it to the corresponding column in the matrix\n",
    "            output_matrix[:, i] = model_output_per_part.squeeze(0).cpu()\n",
    "\n",
    "        return output_matrix\n",
    "\n",
    "    # Function to create the predicted graph\n",
    "    def __build_predicted_graph(self, parts: list, edge_probabilities: dict) -> Graph:\n",
    "\n",
    "        # Initialize Empty Graph with parts\n",
    "        G = self.__create_empty_graph(parts)\n",
    "\n",
    "        for source_node in G.nodes:\n",
    "            for target_node in G.nodes:\n",
    "                if source_node == target_node:\n",
    "                    continue\n",
    "                source_id = source_node.get_part_id()\n",
    "                target_id = int(target_node.get_part_id())\n",
    "                edge_prob = edge_probabilities[source_id].get(target_id, 0)\n",
    "\n",
    "                # Add edges\n",
    "                weight = 1 - edge_prob\n",
    "                G.add_edge(source_node, target_node, weight=weight)\n",
    "\n",
    "        # Minimum spanning Tree\n",
    "        mst = nx.minimum_spanning_tree(G)\n",
    "\n",
    "        # Build graph\n",
    "        predicted_graph = Graph()\n",
    "        for edge in mst.edges():\n",
    "            predicted_graph.add_undirected_edge(edge[0], edge[1])\n",
    "\n",
    "        return predicted_graph\n",
    "\n",
    "    def __create_empty_graph(self, parts: Set[Part]) -> Graph:\n",
    "        graph = nx.Graph()\n",
    "        graph.add_nodes_from(parts)\n",
    "        return graph\n",
    "\n",
    "    def draw_tree(self, tree, title=\"Minimum Spanning Tree\"):\n",
    "        # Generate layout positions for the tree (spring layout works well for small graphs)\n",
    "        pos = nx.spring_layout(tree)  # You can use other layouts like nx.kamada_kawai_layout(tree)\n",
    "\n",
    "        # Draw the graph\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        nx.draw(tree, pos, with_labels=True, node_size=500, font_size=10, node_color=\"skyblue\", edge_color=\"gray\")\n",
    "\n",
    "        # Draw edge weights\n",
    "        edge_labels = nx.get_edge_attributes(tree, 'weight')\n",
    "        nx.draw_networkx_edge_labels(tree, pos, edge_labels=edge_labels, font_size=9)\n",
    "\n",
    "        # Add a title\n",
    "        plt.title(title)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T20:45:52.787572Z",
     "start_time": "2025-01-25T20:45:52.671276Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create TestSet:\n",
    "example_parts_set1 = set()  # Correct initialization of a set\n",
    "example_parts_set1.add(Part(part_id='41', family_id=24))\n",
    "example_parts_set1.add(Part(part_id='41', family_id=24))\n",
    "example_parts_set1.add(Part(part_id='93', family_id=15))\n",
    "example_parts_set1.add(Part(part_id='163', family_id=23))\n",
    "example_parts_set1.add(Part(part_id='163', family_id=23))\n",
    "example_parts_set1.add(Part(part_id='1240', family_id=0))\n",
    "\n",
    "\"\"\"example_parts_set2 = set()  # Correct initialization of a set\n",
    "example_parts_set2.add(Part(part_id='83', family_id=24))\n",
    "example_parts_set2.add(Part(part_id='359', family_id=15))\n",
    "example_parts_set2.add(Part(part_id='77', family_id=23))\n",
    "example_parts_set2.add(Part(part_id='82', family_id=24))\n",
    "example_parts_set2.add(Part(part_id='82', family_id=15))\n",
    "example_parts_set2.add(Part(part_id='58', family_id=0))\n",
    "example_parts_set2.add(Part(part_id='121', family_id=12))\n",
    "example_parts_set2.add(Part(part_id='35', family_id=6))\n",
    "example_parts_set2.add(Part(part_id='747', family_id=33))\n",
    "example_parts_set2.add(Part(part_id='58', family_id=0))\"\"\"\n",
    "\n",
    "# Setup Graph Builder:\n",
    "model_path = \"ffnn_model.pth\"\n",
    "graphbuilder = GraphBuilder()\n",
    "graphbuilder.load_model(input_dim, hidden_dim, output_dim, model_path)\n",
    "\n",
    "graph = graphbuilder.predict_graph(example_parts_set1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with the GraphBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T20:10:13.051759Z",
     "start_time": "2025-01-25T20:10:13.029855Z"
    }
   },
   "outputs": [],
   "source": [
    "validation_list = []\n",
    "for parts, graph in validation_set:\n",
    "    tuple = (parts, graph)\n",
    "    validation_list.append(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T20:48:28.000078Z",
     "start_time": "2025-01-25T20:45:55.370118Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of NeighbourGraphPredictionModel:  96.875\n",
      "Evaluation result saved to hyperparameter_tuning/1024-128-0.0005-edge_accuracy.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "from evaluation import evaluate\n",
    "\n",
    "# Initialize and load the model\n",
    "builder = GraphBuilder()\n",
    "builder.load_model(input_dim, hidden_dim, output_dim, model_path)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = evaluate(builder, validation_list)\n",
    "print(\"Accuracy of NeighbourGraphPredictionModel: \", accuracy)\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "output_folder = \"hyperparameter_tuning\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save accuracy to CSV\n",
    "output_file = os.path.join(output_folder, f\"{hidden_dim}-{batch_size}-{learning_rate}-edge_accuracy.csv\")\n",
    "with open(output_file, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"edge_accuracy\"])  # Write the header\n",
    "    writer.writerow([accuracy])  # Write the accuracy value\n",
    "\n",
    "print(f\"Evaluation result saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 0.055651, Train Acc: 0.9915, Val Loss: 0.037722, Val Acc: 0.9993\n",
      "Epoch [2/50], Train Loss: 0.037819, Train Acc: 0.9994, Val Loss: 0.036024, Val Acc: 0.9995\n",
      "Epoch [3/50], Train Loss: 0.035660, Train Acc: 0.9995, Val Loss: 0.033248, Val Acc: 0.9995\n",
      "Epoch [4/50], Train Loss: 0.031602, Train Acc: 0.9995, Val Loss: 0.026600, Val Acc: 0.9995\n",
      "Epoch [5/50], Train Loss: 0.012683, Train Acc: 0.9995, Val Loss: 0.001678, Val Acc: 0.9996\n",
      "Epoch [6/50], Train Loss: 0.000872, Train Acc: 0.9997, Val Loss: 0.000608, Val Acc: 0.9998\n",
      "Epoch [7/50], Train Loss: 0.000378, Train Acc: 0.9999, Val Loss: 0.000395, Val Acc: 0.9999\n",
      "Epoch [8/50], Train Loss: 0.000250, Train Acc: 0.9999, Val Loss: 0.000318, Val Acc: 0.9999\n",
      "Epoch [9/50], Train Loss: 0.000196, Train Acc: 0.9999, Val Loss: 0.000307, Val Acc: 0.9999\n",
      "Epoch [10/50], Train Loss: 0.000171, Train Acc: 0.9999, Val Loss: 0.000272, Val Acc: 0.9999\n",
      "Epoch [11/50], Train Loss: 0.000149, Train Acc: 0.9999, Val Loss: 0.000262, Val Acc: 0.9999\n",
      "Epoch [12/50], Train Loss: 0.000140, Train Acc: 1.0000, Val Loss: 0.000257, Val Acc: 0.9999\n",
      "Epoch [13/50], Train Loss: 0.000133, Train Acc: 1.0000, Val Loss: 0.000255, Val Acc: 0.9999\n",
      "Epoch [14/50], Train Loss: 0.000123, Train Acc: 1.0000, Val Loss: 0.000251, Val Acc: 0.9999\n",
      "Epoch [15/50], Train Loss: 0.000120, Train Acc: 1.0000, Val Loss: 0.000234, Val Acc: 0.9999\n",
      "Epoch [16/50], Train Loss: 0.000119, Train Acc: 1.0000, Val Loss: 0.000239, Val Acc: 0.9999\n",
      "Epoch [17/50], Train Loss: 0.000114, Train Acc: 1.0000, Val Loss: 0.000229, Val Acc: 0.9999\n",
      "Epoch [18/50], Train Loss: 0.000108, Train Acc: 1.0000, Val Loss: 0.000224, Val Acc: 0.9999\n",
      "Epoch [19/50], Train Loss: 0.000106, Train Acc: 1.0000, Val Loss: 0.000231, Val Acc: 0.9999\n",
      "Epoch [20/50], Train Loss: 0.000106, Train Acc: 1.0000, Val Loss: 0.000238, Val Acc: 0.9999\n",
      "Epoch [21/50], Train Loss: 0.000114, Train Acc: 1.0000, Val Loss: 0.000234, Val Acc: 0.9999\n",
      "Epoch [22/50], Train Loss: 0.000106, Train Acc: 1.0000, Val Loss: 0.000224, Val Acc: 0.9999\n",
      "Epoch [23/50], Train Loss: 0.000102, Train Acc: 1.0000, Val Loss: 0.000249, Val Acc: 0.9999\n",
      "Epoch [24/50], Train Loss: 0.000107, Train Acc: 1.0000, Val Loss: 0.000242, Val Acc: 0.9999\n",
      "Epoch [25/50], Train Loss: 0.000107, Train Acc: 1.0000, Val Loss: 0.000234, Val Acc: 0.9999\n",
      "Epoch [26/50], Train Loss: 0.000107, Train Acc: 1.0000, Val Loss: 0.000250, Val Acc: 0.9999\n",
      "Epoch [27/50], Train Loss: 0.000112, Train Acc: 1.0000, Val Loss: 0.000251, Val Acc: 0.9999\n",
      "Epoch [28/50], Train Loss: 0.000109, Train Acc: 1.0000, Val Loss: 0.000250, Val Acc: 0.9999\n",
      "Epoch [29/50], Train Loss: 0.000107, Train Acc: 1.0000, Val Loss: 0.000232, Val Acc: 0.9999\n",
      "Epoch [30/50], Train Loss: 0.000099, Train Acc: 1.0000, Val Loss: 0.000243, Val Acc: 0.9999\n",
      "Epoch [31/50], Train Loss: 0.000100, Train Acc: 1.0000, Val Loss: 0.000252, Val Acc: 0.9999\n",
      "Epoch [32/50], Train Loss: 0.000099, Train Acc: 1.0000, Val Loss: 0.000238, Val Acc: 0.9999\n",
      "Epoch [33/50], Train Loss: 0.000103, Train Acc: 1.0000, Val Loss: 0.000238, Val Acc: 0.9999\n",
      "Epoch [34/50], Train Loss: 0.000100, Train Acc: 1.0000, Val Loss: 0.000272, Val Acc: 0.9999\n",
      "Epoch [35/50], Train Loss: 0.000103, Train Acc: 1.0000, Val Loss: 0.000232, Val Acc: 0.9999\n",
      "Epoch [36/50], Train Loss: 0.000094, Train Acc: 1.0000, Val Loss: 0.000244, Val Acc: 0.9999\n",
      "Epoch [37/50], Train Loss: 0.000092, Train Acc: 1.0000, Val Loss: 0.000245, Val Acc: 0.9999\n",
      "Epoch [38/50], Train Loss: 0.000094, Train Acc: 1.0000, Val Loss: 0.000245, Val Acc: 0.9999\n",
      "Epoch [39/50], Train Loss: 0.000092, Train Acc: 1.0000, Val Loss: 0.000279, Val Acc: 0.9999\n",
      "Epoch [40/50], Train Loss: 0.000093, Train Acc: 1.0000, Val Loss: 0.000247, Val Acc: 0.9999\n",
      "Epoch [41/50], Train Loss: 0.000098, Train Acc: 1.0000, Val Loss: 0.000262, Val Acc: 0.9999\n",
      "Epoch [42/50], Train Loss: 0.000103, Train Acc: 1.0000, Val Loss: 0.000269, Val Acc: 0.9999\n",
      "Epoch [43/50], Train Loss: 0.000113, Train Acc: 1.0000, Val Loss: 0.000323, Val Acc: 0.9999\n",
      "Epoch [44/50], Train Loss: 0.000128, Train Acc: 1.0000, Val Loss: 0.000286, Val Acc: 0.9999\n",
      "Epoch [45/50], Train Loss: 0.000118, Train Acc: 1.0000, Val Loss: 0.000282, Val Acc: 0.9999\n",
      "Epoch [46/50], Train Loss: 0.000109, Train Acc: 1.0000, Val Loss: 0.000268, Val Acc: 0.9999\n",
      "Epoch [47/50], Train Loss: 0.000115, Train Acc: 1.0000, Val Loss: 0.000269, Val Acc: 0.9999\n",
      "Epoch [48/50], Train Loss: 0.000109, Train Acc: 1.0000, Val Loss: 0.000262, Val Acc: 0.9999\n",
      "Epoch [49/50], Train Loss: 0.000105, Train Acc: 1.0000, Val Loss: 0.000270, Val Acc: 0.9999\n",
      "Epoch [50/50], Train Loss: 0.000104, Train Acc: 1.0000, Val Loss: 0.000273, Val Acc: 0.9999\n",
      "Model saved to ffnn_model_128_512_0.01.pth\n",
      "Evaluation result saved to hyperparameter/128-512-0.01-edge_accuracy.csv\n",
      "Epoch [1/50], Train Loss: 0.136735, Train Acc: 0.9517, Val Loss: 0.005557, Val Acc: 0.9993\n",
      "Epoch [2/50], Train Loss: 0.005177, Train Acc: 0.9993, Val Loss: 0.005089, Val Acc: 0.9993\n",
      "Epoch [3/50], Train Loss: 0.004786, Train Acc: 0.9993, Val Loss: 0.004726, Val Acc: 0.9993\n",
      "Epoch [4/50], Train Loss: 0.004343, Train Acc: 0.9993, Val Loss: 0.004180, Val Acc: 0.9993\n",
      "Epoch [5/50], Train Loss: 0.003818, Train Acc: 0.9993, Val Loss: 0.003640, Val Acc: 0.9993\n",
      "Epoch [6/50], Train Loss: 0.003300, Train Acc: 0.9993, Val Loss: 0.003192, Val Acc: 0.9994\n",
      "Epoch [7/50], Train Loss: 0.002845, Train Acc: 0.9994, Val Loss: 0.002765, Val Acc: 0.9994\n",
      "Epoch [8/50], Train Loss: 0.002449, Train Acc: 0.9994, Val Loss: 0.002409, Val Acc: 0.9994\n",
      "Epoch [9/50], Train Loss: 0.002113, Train Acc: 0.9994, Val Loss: 0.002096, Val Acc: 0.9995\n",
      "Epoch [10/50], Train Loss: 0.001821, Train Acc: 0.9995, Val Loss: 0.001837, Val Acc: 0.9995\n",
      "Epoch [11/50], Train Loss: 0.001573, Train Acc: 0.9995, Val Loss: 0.001597, Val Acc: 0.9995\n",
      "Epoch [12/50], Train Loss: 0.001357, Train Acc: 0.9996, Val Loss: 0.001398, Val Acc: 0.9996\n",
      "Epoch [13/50], Train Loss: 0.001163, Train Acc: 0.9996, Val Loss: 0.001202, Val Acc: 0.9996\n",
      "Epoch [14/50], Train Loss: 0.001001, Train Acc: 0.9997, Val Loss: 0.001068, Val Acc: 0.9997\n",
      "Epoch [15/50], Train Loss: 0.000865, Train Acc: 0.9997, Val Loss: 0.000923, Val Acc: 0.9997\n",
      "Epoch [16/50], Train Loss: 0.000752, Train Acc: 0.9998, Val Loss: 0.000827, Val Acc: 0.9997\n",
      "Epoch [17/50], Train Loss: 0.000656, Train Acc: 0.9998, Val Loss: 0.000735, Val Acc: 0.9998\n",
      "Epoch [18/50], Train Loss: 0.000573, Train Acc: 0.9998, Val Loss: 0.000662, Val Acc: 0.9998\n",
      "Epoch [19/50], Train Loss: 0.000506, Train Acc: 0.9998, Val Loss: 0.000596, Val Acc: 0.9998\n",
      "Epoch [20/50], Train Loss: 0.000450, Train Acc: 0.9999, Val Loss: 0.000537, Val Acc: 0.9998\n",
      "Epoch [21/50], Train Loss: 0.000401, Train Acc: 0.9999, Val Loss: 0.000493, Val Acc: 0.9999\n",
      "Epoch [22/50], Train Loss: 0.000361, Train Acc: 0.9999, Val Loss: 0.000449, Val Acc: 0.9999\n",
      "Epoch [23/50], Train Loss: 0.000327, Train Acc: 0.9999, Val Loss: 0.000431, Val Acc: 0.9999\n",
      "Epoch [24/50], Train Loss: 0.000299, Train Acc: 0.9999, Val Loss: 0.000387, Val Acc: 0.9999\n",
      "Epoch [25/50], Train Loss: 0.000275, Train Acc: 0.9999, Val Loss: 0.000362, Val Acc: 0.9999\n",
      "Epoch [26/50], Train Loss: 0.000252, Train Acc: 0.9999, Val Loss: 0.000343, Val Acc: 0.9999\n",
      "Epoch [27/50], Train Loss: 0.000233, Train Acc: 0.9999, Val Loss: 0.000322, Val Acc: 0.9999\n",
      "Epoch [28/50], Train Loss: 0.000217, Train Acc: 0.9999, Val Loss: 0.000316, Val Acc: 0.9999\n",
      "Epoch [29/50], Train Loss: 0.000204, Train Acc: 0.9999, Val Loss: 0.000294, Val Acc: 0.9999\n",
      "Epoch [30/50], Train Loss: 0.000190, Train Acc: 0.9999, Val Loss: 0.000274, Val Acc: 0.9999\n",
      "Epoch [31/50], Train Loss: 0.000180, Train Acc: 0.9999, Val Loss: 0.000265, Val Acc: 0.9999\n",
      "Epoch [32/50], Train Loss: 0.000170, Train Acc: 0.9999, Val Loss: 0.000264, Val Acc: 0.9999\n",
      "Epoch [33/50], Train Loss: 0.000160, Train Acc: 1.0000, Val Loss: 0.000243, Val Acc: 0.9999\n",
      "Epoch [34/50], Train Loss: 0.000151, Train Acc: 1.0000, Val Loss: 0.000246, Val Acc: 0.9999\n",
      "Epoch [35/50], Train Loss: 0.000145, Train Acc: 1.0000, Val Loss: 0.000236, Val Acc: 0.9999\n",
      "Epoch [36/50], Train Loss: 0.000140, Train Acc: 1.0000, Val Loss: 0.000227, Val Acc: 0.9999\n",
      "Epoch [37/50], Train Loss: 0.000133, Train Acc: 1.0000, Val Loss: 0.000228, Val Acc: 0.9999\n",
      "Epoch [38/50], Train Loss: 0.000128, Train Acc: 1.0000, Val Loss: 0.000215, Val Acc: 0.9999\n",
      "Epoch [39/50], Train Loss: 0.000122, Train Acc: 1.0000, Val Loss: 0.000212, Val Acc: 0.9999\n",
      "Epoch [40/50], Train Loss: 0.000119, Train Acc: 1.0000, Val Loss: 0.000209, Val Acc: 0.9999\n",
      "Epoch [41/50], Train Loss: 0.000114, Train Acc: 1.0000, Val Loss: 0.000210, Val Acc: 0.9999\n",
      "Epoch [42/50], Train Loss: 0.000111, Train Acc: 1.0000, Val Loss: 0.000199, Val Acc: 0.9999\n",
      "Epoch [43/50], Train Loss: 0.000108, Train Acc: 1.0000, Val Loss: 0.000197, Val Acc: 0.9999\n",
      "Epoch [44/50], Train Loss: 0.000104, Train Acc: 1.0000, Val Loss: 0.000195, Val Acc: 0.9999\n",
      "Epoch [45/50], Train Loss: 0.000101, Train Acc: 1.0000, Val Loss: 0.000200, Val Acc: 0.9999\n",
      "Epoch [46/50], Train Loss: 0.000099, Train Acc: 1.0000, Val Loss: 0.000191, Val Acc: 0.9999\n",
      "Epoch [47/50], Train Loss: 0.000096, Train Acc: 1.0000, Val Loss: 0.000187, Val Acc: 0.9999\n",
      "Epoch [48/50], Train Loss: 0.000095, Train Acc: 1.0000, Val Loss: 0.000186, Val Acc: 0.9999\n",
      "Epoch [49/50], Train Loss: 0.000092, Train Acc: 1.0000, Val Loss: 0.000187, Val Acc: 0.9999\n",
      "Epoch [50/50], Train Loss: 0.000090, Train Acc: 1.0000, Val Loss: 0.000181, Val Acc: 0.9999\n",
      "Model saved to ffnn_model_128_512_0.001.pth\n",
      "Evaluation result saved to hyperparameter/128-512-0.001-edge_accuracy.csv\n",
      "Epoch [1/50], Train Loss: 0.232769, Train Acc: 0.9129, Val Loss: 0.006455, Val Acc: 0.9993\n",
      "Epoch [2/50], Train Loss: 0.005686, Train Acc: 0.9993, Val Loss: 0.005407, Val Acc: 0.9993\n",
      "Epoch [3/50], Train Loss: 0.005148, Train Acc: 0.9993, Val Loss: 0.005140, Val Acc: 0.9993\n",
      "Epoch [4/50], Train Loss: 0.004909, Train Acc: 0.9993, Val Loss: 0.004938, Val Acc: 0.9993\n",
      "Epoch [5/50], Train Loss: 0.004708, Train Acc: 0.9993, Val Loss: 0.004715, Val Acc: 0.9993\n",
      "Epoch [6/50], Train Loss: 0.004504, Train Acc: 0.9993, Val Loss: 0.004501, Val Acc: 0.9993\n",
      "Epoch [7/50], Train Loss: 0.004272, Train Acc: 0.9993, Val Loss: 0.004276, Val Acc: 0.9993\n",
      "Epoch [8/50], Train Loss: 0.004031, Train Acc: 0.9993, Val Loss: 0.003998, Val Acc: 0.9993\n",
      "Epoch [9/50], Train Loss: 0.003782, Train Acc: 0.9993, Val Loss: 0.003772, Val Acc: 0.9993\n",
      "Epoch [10/50], Train Loss: 0.003549, Train Acc: 0.9993, Val Loss: 0.003531, Val Acc: 0.9993\n",
      "Epoch [11/50], Train Loss: 0.003330, Train Acc: 0.9993, Val Loss: 0.003336, Val Acc: 0.9993\n",
      "Epoch [12/50], Train Loss: 0.003119, Train Acc: 0.9993, Val Loss: 0.003120, Val Acc: 0.9993\n",
      "Epoch [13/50], Train Loss: 0.002923, Train Acc: 0.9994, Val Loss: 0.002944, Val Acc: 0.9994\n",
      "Epoch [14/50], Train Loss: 0.002730, Train Acc: 0.9994, Val Loss: 0.002742, Val Acc: 0.9994\n",
      "Epoch [15/50], Train Loss: 0.002539, Train Acc: 0.9994, Val Loss: 0.002567, Val Acc: 0.9994\n",
      "Epoch [16/50], Train Loss: 0.002361, Train Acc: 0.9994, Val Loss: 0.002404, Val Acc: 0.9994\n",
      "Epoch [17/50], Train Loss: 0.002190, Train Acc: 0.9994, Val Loss: 0.002236, Val Acc: 0.9994\n",
      "Epoch [18/50], Train Loss: 0.002029, Train Acc: 0.9995, Val Loss: 0.002070, Val Acc: 0.9995\n",
      "Epoch [19/50], Train Loss: 0.001876, Train Acc: 0.9995, Val Loss: 0.001914, Val Acc: 0.9995\n",
      "Epoch [20/50], Train Loss: 0.001731, Train Acc: 0.9995, Val Loss: 0.001783, Val Acc: 0.9995\n",
      "Epoch [21/50], Train Loss: 0.001596, Train Acc: 0.9995, Val Loss: 0.001640, Val Acc: 0.9995\n",
      "Epoch [22/50], Train Loss: 0.001466, Train Acc: 0.9996, Val Loss: 0.001528, Val Acc: 0.9996\n",
      "Epoch [23/50], Train Loss: 0.001348, Train Acc: 0.9996, Val Loss: 0.001413, Val Acc: 0.9996\n",
      "Epoch [24/50], Train Loss: 0.001239, Train Acc: 0.9996, Val Loss: 0.001308, Val Acc: 0.9996\n",
      "Epoch [25/50], Train Loss: 0.001141, Train Acc: 0.9996, Val Loss: 0.001230, Val Acc: 0.9996\n",
      "Epoch [26/50], Train Loss: 0.001048, Train Acc: 0.9997, Val Loss: 0.001131, Val Acc: 0.9997\n",
      "Epoch [27/50], Train Loss: 0.000964, Train Acc: 0.9997, Val Loss: 0.001039, Val Acc: 0.9997\n",
      "Epoch [28/50], Train Loss: 0.000889, Train Acc: 0.9997, Val Loss: 0.000980, Val Acc: 0.9997\n",
      "Epoch [29/50], Train Loss: 0.000820, Train Acc: 0.9997, Val Loss: 0.000916, Val Acc: 0.9997\n",
      "Epoch [30/50], Train Loss: 0.000758, Train Acc: 0.9998, Val Loss: 0.000864, Val Acc: 0.9997\n",
      "Epoch [31/50], Train Loss: 0.000700, Train Acc: 0.9998, Val Loss: 0.000792, Val Acc: 0.9998\n",
      "Epoch [32/50], Train Loss: 0.000647, Train Acc: 0.9998, Val Loss: 0.000736, Val Acc: 0.9998\n",
      "Epoch [33/50], Train Loss: 0.000600, Train Acc: 0.9998, Val Loss: 0.000702, Val Acc: 0.9998\n",
      "Epoch [34/50], Train Loss: 0.000557, Train Acc: 0.9998, Val Loss: 0.000658, Val Acc: 0.9998\n",
      "Epoch [35/50], Train Loss: 0.000517, Train Acc: 0.9998, Val Loss: 0.000618, Val Acc: 0.9998\n",
      "Epoch [36/50], Train Loss: 0.000483, Train Acc: 0.9999, Val Loss: 0.000582, Val Acc: 0.9998\n",
      "Epoch [37/50], Train Loss: 0.000450, Train Acc: 0.9999, Val Loss: 0.000543, Val Acc: 0.9998\n",
      "Epoch [38/50], Train Loss: 0.000420, Train Acc: 0.9999, Val Loss: 0.000517, Val Acc: 0.9998\n",
      "Epoch [39/50], Train Loss: 0.000392, Train Acc: 0.9999, Val Loss: 0.000486, Val Acc: 0.9998\n",
      "Epoch [40/50], Train Loss: 0.000368, Train Acc: 0.9999, Val Loss: 0.000459, Val Acc: 0.9999\n",
      "Epoch [41/50], Train Loss: 0.000346, Train Acc: 0.9999, Val Loss: 0.000438, Val Acc: 0.9999\n",
      "Epoch [42/50], Train Loss: 0.000325, Train Acc: 0.9999, Val Loss: 0.000426, Val Acc: 0.9999\n",
      "Epoch [43/50], Train Loss: 0.000306, Train Acc: 0.9999, Val Loss: 0.000399, Val Acc: 0.9999\n",
      "Epoch [44/50], Train Loss: 0.000289, Train Acc: 0.9999, Val Loss: 0.000380, Val Acc: 0.9999\n",
      "Epoch [45/50], Train Loss: 0.000272, Train Acc: 0.9999, Val Loss: 0.000362, Val Acc: 0.9999\n",
      "Epoch [46/50], Train Loss: 0.000258, Train Acc: 0.9999, Val Loss: 0.000342, Val Acc: 0.9999\n",
      "Epoch [47/50], Train Loss: 0.000245, Train Acc: 0.9999, Val Loss: 0.000334, Val Acc: 0.9999\n",
      "Epoch [48/50], Train Loss: 0.000232, Train Acc: 0.9999, Val Loss: 0.000318, Val Acc: 0.9999\n",
      "Epoch [49/50], Train Loss: 0.000221, Train Acc: 0.9999, Val Loss: 0.000310, Val Acc: 0.9999\n",
      "Epoch [50/50], Train Loss: 0.000210, Train Acc: 0.9999, Val Loss: 0.000301, Val Acc: 0.9999\n",
      "Model saved to ffnn_model_128_512_0.0005.pth\n",
      "Evaluation result saved to hyperparameter/128-512-0.0005-edge_accuracy.csv\n",
      "Epoch [1/50], Train Loss: 0.071548, Train Acc: 0.9837, Val Loss: 0.039803, Val Acc: 0.9993\n",
      "Epoch [2/50], Train Loss: 0.039255, Train Acc: 0.9993, Val Loss: 0.038801, Val Acc: 0.9993\n",
      "Epoch [3/50], Train Loss: 0.039216, Train Acc: 0.9994, Val Loss: 0.038858, Val Acc: 0.9994\n",
      "Epoch [4/50], Train Loss: 0.039027, Train Acc: 0.9994, Val Loss: 0.038647, Val Acc: 0.9994\n",
      "Epoch [5/50], Train Loss: 0.038497, Train Acc: 0.9995, Val Loss: 0.037439, Val Acc: 0.9995\n",
      "Epoch [6/50], Train Loss: 0.037137, Train Acc: 0.9995, Val Loss: 0.036341, Val Acc: 0.9995\n",
      "Epoch [7/50], Train Loss: 0.035901, Train Acc: 0.9995, Val Loss: 0.034850, Val Acc: 0.9995\n",
      "Epoch [8/50], Train Loss: 0.034369, Train Acc: 0.9995, Val Loss: 0.033036, Val Acc: 0.9995\n",
      "Epoch [9/50], Train Loss: 0.032440, Train Acc: 0.9995, Val Loss: 0.030974, Val Acc: 0.9995\n",
      "Epoch [10/50], Train Loss: 0.030136, Train Acc: 0.9995, Val Loss: 0.028268, Val Acc: 0.9995\n",
      "Epoch [11/50], Train Loss: 0.025828, Train Acc: 0.9995, Val Loss: 0.020634, Val Acc: 0.9995\n",
      "Epoch [12/50], Train Loss: 0.009396, Train Acc: 0.9995, Val Loss: 0.001845, Val Acc: 0.9995\n",
      "Epoch [13/50], Train Loss: 0.001063, Train Acc: 0.9997, Val Loss: 0.000776, Val Acc: 0.9998\n",
      "Epoch [14/50], Train Loss: 0.000483, Train Acc: 0.9998, Val Loss: 0.000485, Val Acc: 0.9999\n",
      "Epoch [15/50], Train Loss: 0.000305, Train Acc: 0.9999, Val Loss: 0.000373, Val Acc: 0.9999\n",
      "Epoch [16/50], Train Loss: 0.000231, Train Acc: 0.9999, Val Loss: 0.000318, Val Acc: 0.9999\n",
      "Epoch [17/50], Train Loss: 0.000183, Train Acc: 0.9999, Val Loss: 0.000290, Val Acc: 0.9999\n",
      "Epoch [18/50], Train Loss: 0.000164, Train Acc: 0.9999, Val Loss: 0.000285, Val Acc: 0.9999\n",
      "Epoch [19/50], Train Loss: 0.000146, Train Acc: 1.0000, Val Loss: 0.000270, Val Acc: 0.9999\n",
      "Epoch [20/50], Train Loss: 0.000131, Train Acc: 1.0000, Val Loss: 0.000254, Val Acc: 0.9999\n",
      "Epoch [21/50], Train Loss: 0.000119, Train Acc: 1.0000, Val Loss: 0.000247, Val Acc: 0.9999\n",
      "Epoch [22/50], Train Loss: 0.000116, Train Acc: 1.0000, Val Loss: 0.000247, Val Acc: 0.9999\n",
      "Epoch [23/50], Train Loss: 0.000110, Train Acc: 1.0000, Val Loss: 0.000246, Val Acc: 0.9999\n",
      "Epoch [24/50], Train Loss: 0.000110, Train Acc: 1.0000, Val Loss: 0.000240, Val Acc: 0.9999\n",
      "Epoch [25/50], Train Loss: 0.000110, Train Acc: 1.0000, Val Loss: 0.000240, Val Acc: 0.9999\n",
      "Epoch [26/50], Train Loss: 0.000102, Train Acc: 1.0000, Val Loss: 0.000233, Val Acc: 0.9999\n",
      "Epoch [27/50], Train Loss: 0.000099, Train Acc: 1.0000, Val Loss: 0.000226, Val Acc: 0.9999\n",
      "Epoch [28/50], Train Loss: 0.000106, Train Acc: 1.0000, Val Loss: 0.000235, Val Acc: 0.9999\n",
      "Epoch [29/50], Train Loss: 0.000096, Train Acc: 1.0000, Val Loss: 0.000228, Val Acc: 0.9999\n",
      "Epoch [30/50], Train Loss: 0.000096, Train Acc: 1.0000, Val Loss: 0.000225, Val Acc: 0.9999\n",
      "Epoch [31/50], Train Loss: 0.000095, Train Acc: 1.0000, Val Loss: 0.000235, Val Acc: 0.9999\n",
      "Epoch [32/50], Train Loss: 0.000089, Train Acc: 1.0000, Val Loss: 0.000230, Val Acc: 0.9999\n",
      "Epoch [33/50], Train Loss: 0.000089, Train Acc: 1.0000, Val Loss: 0.000232, Val Acc: 0.9999\n",
      "Epoch [34/50], Train Loss: 0.000095, Train Acc: 1.0000, Val Loss: 0.000227, Val Acc: 0.9999\n",
      "Epoch [35/50], Train Loss: 0.000088, Train Acc: 1.0000, Val Loss: 0.000235, Val Acc: 0.9999\n",
      "Epoch [36/50], Train Loss: 0.000086, Train Acc: 1.0000, Val Loss: 0.000228, Val Acc: 0.9999\n",
      "Epoch [37/50], Train Loss: 0.000084, Train Acc: 1.0000, Val Loss: 0.000236, Val Acc: 0.9999\n",
      "Epoch [38/50], Train Loss: 0.000082, Train Acc: 1.0000, Val Loss: 0.000228, Val Acc: 0.9999\n",
      "Epoch [39/50], Train Loss: 0.000088, Train Acc: 1.0000, Val Loss: 0.000229, Val Acc: 0.9999\n",
      "Epoch [40/50], Train Loss: 0.000088, Train Acc: 1.0000, Val Loss: 0.000225, Val Acc: 0.9999\n",
      "Epoch [41/50], Train Loss: 0.000089, Train Acc: 1.0000, Val Loss: 0.000226, Val Acc: 0.9999\n",
      "Epoch [42/50], Train Loss: 0.000090, Train Acc: 1.0000, Val Loss: 0.000233, Val Acc: 0.9999\n",
      "Epoch [43/50], Train Loss: 0.000084, Train Acc: 1.0000, Val Loss: 0.000248, Val Acc: 0.9999\n",
      "Epoch [44/50], Train Loss: 0.000088, Train Acc: 1.0000, Val Loss: 0.000218, Val Acc: 0.9999\n",
      "Epoch [45/50], Train Loss: 0.000083, Train Acc: 1.0000, Val Loss: 0.000226, Val Acc: 0.9999\n",
      "Epoch [46/50], Train Loss: 0.000080, Train Acc: 1.0000, Val Loss: 0.000230, Val Acc: 0.9999\n",
      "Epoch [47/50], Train Loss: 0.000085, Train Acc: 1.0000, Val Loss: 0.000231, Val Acc: 0.9999\n",
      "Epoch [48/50], Train Loss: 0.000085, Train Acc: 1.0000, Val Loss: 0.000236, Val Acc: 0.9999\n",
      "Epoch [49/50], Train Loss: 0.000085, Train Acc: 1.0000, Val Loss: 0.000229, Val Acc: 0.9999\n",
      "Epoch [50/50], Train Loss: 0.000082, Train Acc: 1.0000, Val Loss: 0.000229, Val Acc: 0.9999\n",
      "Model saved to ffnn_model_128_1024_0.01.pth\n",
      "Evaluation result saved to hyperparameter/128-1024-0.01-edge_accuracy.csv\n",
      "Epoch [1/50], Train Loss: 0.264294, Train Acc: 0.9012, Val Loss: 0.006874, Val Acc: 0.9993\n",
      "Epoch [2/50], Train Loss: 0.005991, Train Acc: 0.9993, Val Loss: 0.005551, Val Acc: 0.9993\n",
      "Epoch [3/50], Train Loss: 0.005261, Train Acc: 0.9993, Val Loss: 0.005237, Val Acc: 0.9993\n",
      "Epoch [4/50], Train Loss: 0.005016, Train Acc: 0.9993, Val Loss: 0.005032, Val Acc: 0.9993\n",
      "Epoch [5/50], Train Loss: 0.004809, Train Acc: 0.9993, Val Loss: 0.004813, Val Acc: 0.9993\n",
      "Epoch [6/50], Train Loss: 0.004543, Train Acc: 0.9993, Val Loss: 0.004544, Val Acc: 0.9993\n",
      "Epoch [7/50], Train Loss: 0.004255, Train Acc: 0.9993, Val Loss: 0.004233, Val Acc: 0.9993\n",
      "Epoch [8/50], Train Loss: 0.003943, Train Acc: 0.9993, Val Loss: 0.003934, Val Acc: 0.9993\n",
      "Epoch [9/50], Train Loss: 0.003642, Train Acc: 0.9993, Val Loss: 0.003641, Val Acc: 0.9993\n",
      "Epoch [10/50], Train Loss: 0.003362, Train Acc: 0.9993, Val Loss: 0.003363, Val Acc: 0.9993\n",
      "Epoch [11/50], Train Loss: 0.003110, Train Acc: 0.9993, Val Loss: 0.003102, Val Acc: 0.9993\n",
      "Epoch [12/50], Train Loss: 0.002854, Train Acc: 0.9994, Val Loss: 0.002867, Val Acc: 0.9994\n",
      "Epoch [13/50], Train Loss: 0.002623, Train Acc: 0.9994, Val Loss: 0.002636, Val Acc: 0.9994\n",
      "Epoch [14/50], Train Loss: 0.002400, Train Acc: 0.9994, Val Loss: 0.002419, Val Acc: 0.9994\n",
      "Epoch [15/50], Train Loss: 0.002185, Train Acc: 0.9994, Val Loss: 0.002210, Val Acc: 0.9994\n",
      "Epoch [16/50], Train Loss: 0.001989, Train Acc: 0.9995, Val Loss: 0.002027, Val Acc: 0.9995\n",
      "Epoch [17/50], Train Loss: 0.001808, Train Acc: 0.9995, Val Loss: 0.001845, Val Acc: 0.9995\n",
      "Epoch [18/50], Train Loss: 0.001645, Train Acc: 0.9995, Val Loss: 0.001689, Val Acc: 0.9995\n",
      "Epoch [19/50], Train Loss: 0.001488, Train Acc: 0.9995, Val Loss: 0.001548, Val Acc: 0.9995\n",
      "Epoch [20/50], Train Loss: 0.001354, Train Acc: 0.9996, Val Loss: 0.001419, Val Acc: 0.9996\n",
      "Epoch [21/50], Train Loss: 0.001231, Train Acc: 0.9996, Val Loss: 0.001300, Val Acc: 0.9996\n",
      "Epoch [22/50], Train Loss: 0.001118, Train Acc: 0.9996, Val Loss: 0.001194, Val Acc: 0.9996\n",
      "Epoch [23/50], Train Loss: 0.001016, Train Acc: 0.9997, Val Loss: 0.001102, Val Acc: 0.9997\n",
      "Epoch [24/50], Train Loss: 0.000927, Train Acc: 0.9997, Val Loss: 0.001012, Val Acc: 0.9997\n",
      "Epoch [25/50], Train Loss: 0.000847, Train Acc: 0.9997, Val Loss: 0.000934, Val Acc: 0.9997\n",
      "Epoch [26/50], Train Loss: 0.000775, Train Acc: 0.9998, Val Loss: 0.000867, Val Acc: 0.9997\n",
      "Epoch [27/50], Train Loss: 0.000712, Train Acc: 0.9998, Val Loss: 0.000810, Val Acc: 0.9997\n",
      "Epoch [28/50], Train Loss: 0.000659, Train Acc: 0.9998, Val Loss: 0.000751, Val Acc: 0.9998\n",
      "Epoch [29/50], Train Loss: 0.000609, Train Acc: 0.9998, Val Loss: 0.000709, Val Acc: 0.9998\n",
      "Epoch [30/50], Train Loss: 0.000564, Train Acc: 0.9998, Val Loss: 0.000660, Val Acc: 0.9998\n",
      "Epoch [31/50], Train Loss: 0.000524, Train Acc: 0.9998, Val Loss: 0.000623, Val Acc: 0.9998\n",
      "Epoch [32/50], Train Loss: 0.000491, Train Acc: 0.9999, Val Loss: 0.000590, Val Acc: 0.9998\n",
      "Epoch [33/50], Train Loss: 0.000458, Train Acc: 0.9999, Val Loss: 0.000555, Val Acc: 0.9998\n",
      "Epoch [34/50], Train Loss: 0.000429, Train Acc: 0.9999, Val Loss: 0.000532, Val Acc: 0.9998\n",
      "Epoch [35/50], Train Loss: 0.000403, Train Acc: 0.9999, Val Loss: 0.000503, Val Acc: 0.9998\n",
      "Epoch [36/50], Train Loss: 0.000380, Train Acc: 0.9999, Val Loss: 0.000484, Val Acc: 0.9999\n",
      "Epoch [37/50], Train Loss: 0.000358, Train Acc: 0.9999, Val Loss: 0.000462, Val Acc: 0.9999\n",
      "Epoch [38/50], Train Loss: 0.000337, Train Acc: 0.9999, Val Loss: 0.000438, Val Acc: 0.9999\n",
      "Epoch [39/50], Train Loss: 0.000319, Train Acc: 0.9999, Val Loss: 0.000420, Val Acc: 0.9999\n",
      "Epoch [40/50], Train Loss: 0.000304, Train Acc: 0.9999, Val Loss: 0.000409, Val Acc: 0.9999\n",
      "Epoch [41/50], Train Loss: 0.000287, Train Acc: 0.9999, Val Loss: 0.000391, Val Acc: 0.9999\n",
      "Epoch [42/50], Train Loss: 0.000275, Train Acc: 0.9999, Val Loss: 0.000378, Val Acc: 0.9999\n",
      "Epoch [43/50], Train Loss: 0.000264, Train Acc: 0.9999, Val Loss: 0.000362, Val Acc: 0.9999\n",
      "Epoch [44/50], Train Loss: 0.000251, Train Acc: 0.9999, Val Loss: 0.000353, Val Acc: 0.9999\n",
      "Epoch [45/50], Train Loss: 0.000240, Train Acc: 0.9999, Val Loss: 0.000346, Val Acc: 0.9999\n",
      "Epoch [46/50], Train Loss: 0.000231, Train Acc: 0.9999, Val Loss: 0.000330, Val Acc: 0.9999\n",
      "Epoch [47/50], Train Loss: 0.000221, Train Acc: 0.9999, Val Loss: 0.000324, Val Acc: 0.9999\n",
      "Epoch [48/50], Train Loss: 0.000213, Train Acc: 0.9999, Val Loss: 0.000319, Val Acc: 0.9999\n",
      "Epoch [49/50], Train Loss: 0.000205, Train Acc: 0.9999, Val Loss: 0.000304, Val Acc: 0.9999\n",
      "Epoch [50/50], Train Loss: 0.000197, Train Acc: 0.9999, Val Loss: 0.000301, Val Acc: 0.9999\n",
      "Model saved to ffnn_model_128_1024_0.001.pth\n",
      "Evaluation result saved to hyperparameter/128-1024-0.001-edge_accuracy.csv\n",
      "Epoch [1/50], Train Loss: 0.445196, Train Acc: 0.8311, Val Loss: 0.043303, Val Acc: 0.9992\n",
      "Epoch [2/50], Train Loss: 0.012082, Train Acc: 0.9993, Val Loss: 0.006627, Val Acc: 0.9993\n",
      "Epoch [3/50], Train Loss: 0.006017, Train Acc: 0.9993, Val Loss: 0.005745, Val Acc: 0.9993\n",
      "Epoch [4/50], Train Loss: 0.005435, Train Acc: 0.9993, Val Loss: 0.005382, Val Acc: 0.9993\n",
      "Epoch [5/50], Train Loss: 0.005166, Train Acc: 0.9993, Val Loss: 0.005183, Val Acc: 0.9993\n",
      "Epoch [6/50], Train Loss: 0.005008, Train Acc: 0.9993, Val Loss: 0.005055, Val Acc: 0.9993\n",
      "Epoch [7/50], Train Loss: 0.004883, Train Acc: 0.9993, Val Loss: 0.004957, Val Acc: 0.9993\n",
      "Epoch [8/50], Train Loss: 0.004779, Train Acc: 0.9993, Val Loss: 0.004855, Val Acc: 0.9993\n",
      "Epoch [9/50], Train Loss: 0.004670, Train Acc: 0.9993, Val Loss: 0.004733, Val Acc: 0.9993\n",
      "Epoch [10/50], Train Loss: 0.004550, Train Acc: 0.9993, Val Loss: 0.004622, Val Acc: 0.9993\n",
      "Epoch [11/50], Train Loss: 0.004428, Train Acc: 0.9993, Val Loss: 0.004499, Val Acc: 0.9993\n",
      "Epoch [12/50], Train Loss: 0.004297, Train Acc: 0.9993, Val Loss: 0.004360, Val Acc: 0.9993\n",
      "Epoch [13/50], Train Loss: 0.004150, Train Acc: 0.9993, Val Loss: 0.004214, Val Acc: 0.9993\n",
      "Epoch [14/50], Train Loss: 0.004006, Train Acc: 0.9993, Val Loss: 0.004058, Val Acc: 0.9993\n",
      "Epoch [15/50], Train Loss: 0.003841, Train Acc: 0.9993, Val Loss: 0.003893, Val Acc: 0.9993\n",
      "Epoch [16/50], Train Loss: 0.003680, Train Acc: 0.9993, Val Loss: 0.003735, Val Acc: 0.9993\n",
      "Epoch [17/50], Train Loss: 0.003525, Train Acc: 0.9993, Val Loss: 0.003570, Val Acc: 0.9993\n",
      "Epoch [18/50], Train Loss: 0.003359, Train Acc: 0.9993, Val Loss: 0.003424, Val Acc: 0.9993\n",
      "Epoch [19/50], Train Loss: 0.003214, Train Acc: 0.9993, Val Loss: 0.003271, Val Acc: 0.9993\n",
      "Epoch [20/50], Train Loss: 0.003066, Train Acc: 0.9994, Val Loss: 0.003140, Val Acc: 0.9994\n",
      "Epoch [21/50], Train Loss: 0.002929, Train Acc: 0.9994, Val Loss: 0.003002, Val Acc: 0.9994\n",
      "Epoch [22/50], Train Loss: 0.002796, Train Acc: 0.9994, Val Loss: 0.002874, Val Acc: 0.9994\n",
      "Epoch [23/50], Train Loss: 0.002670, Train Acc: 0.9994, Val Loss: 0.002758, Val Acc: 0.9994\n",
      "Epoch [24/50], Train Loss: 0.002548, Train Acc: 0.9994, Val Loss: 0.002631, Val Acc: 0.9994\n",
      "Epoch [25/50], Train Loss: 0.002434, Train Acc: 0.9994, Val Loss: 0.002517, Val Acc: 0.9994\n",
      "Epoch [26/50], Train Loss: 0.002316, Train Acc: 0.9994, Val Loss: 0.002406, Val Acc: 0.9994\n",
      "Epoch [27/50], Train Loss: 0.002207, Train Acc: 0.9994, Val Loss: 0.002301, Val Acc: 0.9994\n",
      "Epoch [28/50], Train Loss: 0.002108, Train Acc: 0.9994, Val Loss: 0.002196, Val Acc: 0.9994\n",
      "Epoch [29/50], Train Loss: 0.002005, Train Acc: 0.9995, Val Loss: 0.002096, Val Acc: 0.9995\n",
      "Epoch [30/50], Train Loss: 0.001905, Train Acc: 0.9995, Val Loss: 0.002002, Val Acc: 0.9995\n",
      "Epoch [31/50], Train Loss: 0.001814, Train Acc: 0.9995, Val Loss: 0.001914, Val Acc: 0.9995\n",
      "Epoch [32/50], Train Loss: 0.001729, Train Acc: 0.9995, Val Loss: 0.001821, Val Acc: 0.9995\n",
      "Epoch [33/50], Train Loss: 0.001649, Train Acc: 0.9995, Val Loss: 0.001749, Val Acc: 0.9995\n",
      "Epoch [34/50], Train Loss: 0.001571, Train Acc: 0.9995, Val Loss: 0.001668, Val Acc: 0.9995\n",
      "Epoch [35/50], Train Loss: 0.001494, Train Acc: 0.9996, Val Loss: 0.001597, Val Acc: 0.9995\n",
      "Epoch [36/50], Train Loss: 0.001427, Train Acc: 0.9996, Val Loss: 0.001527, Val Acc: 0.9996\n",
      "Epoch [37/50], Train Loss: 0.001362, Train Acc: 0.9996, Val Loss: 0.001463, Val Acc: 0.9996\n",
      "Epoch [38/50], Train Loss: 0.001300, Train Acc: 0.9996, Val Loss: 0.001402, Val Acc: 0.9996\n",
      "Epoch [39/50], Train Loss: 0.001241, Train Acc: 0.9996, Val Loss: 0.001346, Val Acc: 0.9996\n",
      "Epoch [40/50], Train Loss: 0.001185, Train Acc: 0.9996, Val Loss: 0.001288, Val Acc: 0.9996\n",
      "Epoch [41/50], Train Loss: 0.001133, Train Acc: 0.9996, Val Loss: 0.001242, Val Acc: 0.9996\n",
      "Epoch [42/50], Train Loss: 0.001083, Train Acc: 0.9997, Val Loss: 0.001187, Val Acc: 0.9996\n",
      "Epoch [43/50], Train Loss: 0.001031, Train Acc: 0.9997, Val Loss: 0.001141, Val Acc: 0.9997\n",
      "Epoch [44/50], Train Loss: 0.000986, Train Acc: 0.9997, Val Loss: 0.001096, Val Acc: 0.9997\n",
      "Epoch [45/50], Train Loss: 0.000946, Train Acc: 0.9997, Val Loss: 0.001053, Val Acc: 0.9997\n",
      "Epoch [46/50], Train Loss: 0.000901, Train Acc: 0.9997, Val Loss: 0.001015, Val Acc: 0.9997\n",
      "Epoch [47/50], Train Loss: 0.000859, Train Acc: 0.9997, Val Loss: 0.000976, Val Acc: 0.9997\n",
      "Epoch [48/50], Train Loss: 0.000826, Train Acc: 0.9997, Val Loss: 0.000935, Val Acc: 0.9997\n",
      "Epoch [49/50], Train Loss: 0.000786, Train Acc: 0.9998, Val Loss: 0.000901, Val Acc: 0.9997\n",
      "Epoch [50/50], Train Loss: 0.000751, Train Acc: 0.9998, Val Loss: 0.000864, Val Acc: 0.9997\n",
      "Model saved to ffnn_model_128_1024_0.0005.pth\n",
      "Evaluation result saved to hyperparameter/128-1024-0.0005-edge_accuracy.csv\n",
      "Epoch [1/50], Train Loss: 0.103781, Train Acc: 0.9683, Val Loss: 0.037422, Val Acc: 0.9993\n",
      "Epoch [2/50], Train Loss: 0.038714, Train Acc: 0.9993, Val Loss: 0.038749, Val Acc: 0.9993\n",
      "Epoch [3/50], Train Loss: 0.038376, Train Acc: 0.9993, Val Loss: 0.037531, Val Acc: 0.9993\n",
      "Epoch [4/50], Train Loss: 0.037716, Train Acc: 0.9993, Val Loss: 0.037480, Val Acc: 0.9993\n",
      "Epoch [5/50], Train Loss: 0.037484, Train Acc: 0.9993, Val Loss: 0.036747, Val Acc: 0.9994\n",
      "Epoch [6/50], Train Loss: 0.036965, Train Acc: 0.9994, Val Loss: 0.035904, Val Acc: 0.9994\n",
      "Epoch [7/50], Train Loss: 0.036501, Train Acc: 0.9994, Val Loss: 0.036057, Val Acc: 0.9994\n",
      "Epoch [8/50], Train Loss: 0.036599, Train Acc: 0.9995, Val Loss: 0.036037, Val Acc: 0.9995\n",
      "Epoch [9/50], Train Loss: 0.036420, Train Acc: 0.9995, Val Loss: 0.035728, Val Acc: 0.9995\n",
      "Epoch [10/50], Train Loss: 0.036512, Train Acc: 0.9995, Val Loss: 0.035931, Val Acc: 0.9995\n",
      "Epoch [11/50], Train Loss: 0.036216, Train Acc: 0.9995, Val Loss: 0.035672, Val Acc: 0.9995\n",
      "Epoch [12/50], Train Loss: 0.036063, Train Acc: 0.9995, Val Loss: 0.035538, Val Acc: 0.9995\n",
      "Epoch [13/50], Train Loss: 0.035970, Train Acc: 0.9995, Val Loss: 0.035565, Val Acc: 0.9995\n",
      "Epoch [14/50], Train Loss: 0.036120, Train Acc: 0.9995, Val Loss: 0.035097, Val Acc: 0.9995\n",
      "Epoch [15/50], Train Loss: 0.035798, Train Acc: 0.9995, Val Loss: 0.035152, Val Acc: 0.9995\n",
      "Epoch [16/50], Train Loss: 0.035501, Train Acc: 0.9995, Val Loss: 0.034999, Val Acc: 0.9995\n",
      "Epoch [17/50], Train Loss: 0.035413, Train Acc: 0.9995, Val Loss: 0.034808, Val Acc: 0.9995\n",
      "Epoch [18/50], Train Loss: 0.035028, Train Acc: 0.9995, Val Loss: 0.034336, Val Acc: 0.9995\n",
      "Epoch [19/50], Train Loss: 0.034836, Train Acc: 0.9995, Val Loss: 0.033916, Val Acc: 0.9995\n",
      "Epoch [20/50], Train Loss: 0.034563, Train Acc: 0.9995, Val Loss: 0.033662, Val Acc: 0.9995\n",
      "Epoch [21/50], Train Loss: 0.033711, Train Acc: 0.9995, Val Loss: 0.033180, Val Acc: 0.9995\n",
      "Epoch [22/50], Train Loss: 0.033459, Train Acc: 0.9995, Val Loss: 0.032715, Val Acc: 0.9995\n",
      "Epoch [23/50], Train Loss: 0.033147, Train Acc: 0.9996, Val Loss: 0.032565, Val Acc: 0.9996\n",
      "Epoch [24/50], Train Loss: 0.032709, Train Acc: 0.9996, Val Loss: 0.032162, Val Acc: 0.9996\n",
      "Epoch [25/50], Train Loss: 0.032084, Train Acc: 0.9996, Val Loss: 0.031475, Val Acc: 0.9995\n",
      "Epoch [26/50], Train Loss: 0.031246, Train Acc: 0.9996, Val Loss: 0.030703, Val Acc: 0.9996\n",
      "Epoch [27/50], Train Loss: 0.030824, Train Acc: 0.9996, Val Loss: 0.030296, Val Acc: 0.9996\n",
      "Epoch [28/50], Train Loss: 0.030369, Train Acc: 0.9996, Val Loss: 0.029750, Val Acc: 0.9996\n",
      "Epoch [29/50], Train Loss: 0.030013, Train Acc: 0.9996, Val Loss: 0.029313, Val Acc: 0.9996\n",
      "Epoch [30/50], Train Loss: 0.029718, Train Acc: 0.9996, Val Loss: 0.029208, Val Acc: 0.9996\n",
      "Epoch [31/50], Train Loss: 0.029445, Train Acc: 0.9996, Val Loss: 0.028749, Val Acc: 0.9996\n",
      "Epoch [32/50], Train Loss: 0.029240, Train Acc: 0.9996, Val Loss: 0.028528, Val Acc: 0.9996\n",
      "Epoch [33/50], Train Loss: 0.028676, Train Acc: 0.9996, Val Loss: 0.027645, Val Acc: 0.9996\n",
      "Epoch [34/50], Train Loss: 0.027321, Train Acc: 0.9996, Val Loss: 0.026219, Val Acc: 0.9996\n",
      "Epoch [35/50], Train Loss: 0.025567, Train Acc: 0.9996, Val Loss: 0.024237, Val Acc: 0.9996\n",
      "Epoch [36/50], Train Loss: 0.022785, Train Acc: 0.9996, Val Loss: 0.020041, Val Acc: 0.9996\n",
      "Epoch [37/50], Train Loss: 0.011139, Train Acc: 0.9995, Val Loss: 0.003139, Val Acc: 0.9994\n",
      "Epoch [38/50], Train Loss: 0.001816, Train Acc: 0.9996, Val Loss: 0.001243, Val Acc: 0.9996\n",
      "Epoch [39/50], Train Loss: 0.000803, Train Acc: 0.9997, Val Loss: 0.000710, Val Acc: 0.9998\n",
      "Epoch [40/50], Train Loss: 0.000449, Train Acc: 0.9999, Val Loss: 0.000501, Val Acc: 0.9998\n",
      "Epoch [41/50], Train Loss: 0.000306, Train Acc: 0.9999, Val Loss: 0.000414, Val Acc: 0.9999\n",
      "Epoch [42/50], Train Loss: 0.000240, Train Acc: 0.9999, Val Loss: 0.000360, Val Acc: 0.9999\n",
      "Epoch [43/50], Train Loss: 0.000205, Train Acc: 0.9999, Val Loss: 0.000312, Val Acc: 0.9999\n",
      "Epoch [44/50], Train Loss: 0.000168, Train Acc: 0.9999, Val Loss: 0.000291, Val Acc: 0.9999\n",
      "Epoch [45/50], Train Loss: 0.000153, Train Acc: 1.0000, Val Loss: 0.000271, Val Acc: 0.9999\n",
      "Epoch [46/50], Train Loss: 0.000134, Train Acc: 1.0000, Val Loss: 0.000261, Val Acc: 0.9999\n",
      "Epoch [47/50], Train Loss: 0.000127, Train Acc: 1.0000, Val Loss: 0.000252, Val Acc: 0.9999\n",
      "Epoch [48/50], Train Loss: 0.000118, Train Acc: 1.0000, Val Loss: 0.000249, Val Acc: 0.9999\n",
      "Epoch [49/50], Train Loss: 0.000119, Train Acc: 1.0000, Val Loss: 0.000254, Val Acc: 0.9999\n",
      "Epoch [50/50], Train Loss: 0.000115, Train Acc: 1.0000, Val Loss: 0.000242, Val Acc: 0.9999\n",
      "Model saved to ffnn_model_128_2048_0.01.pth\n",
      "Evaluation result saved to hyperparameter/128-2048-0.01-edge_accuracy.csv\n",
      "Epoch [1/50], Train Loss: 0.511436, Train Acc: 0.8002, Val Loss: 0.108942, Val Acc: 0.9936\n",
      "Epoch [2/50], Train Loss: 0.021633, Train Acc: 0.9985, Val Loss: 0.006729, Val Acc: 0.9993\n",
      "Epoch [3/50], Train Loss: 0.006261, Train Acc: 0.9993, Val Loss: 0.005910, Val Acc: 0.9993\n",
      "Epoch [4/50], Train Loss: 0.005587, Train Acc: 0.9993, Val Loss: 0.005434, Val Acc: 0.9993\n",
      "Epoch [5/50], Train Loss: 0.005258, Train Acc: 0.9993, Val Loss: 0.005236, Val Acc: 0.9993\n",
      "Epoch [6/50], Train Loss: 0.005117, Train Acc: 0.9993, Val Loss: 0.005121, Val Acc: 0.9993\n",
      "Epoch [7/50], Train Loss: 0.004970, Train Acc: 0.9993, Val Loss: 0.005015, Val Acc: 0.9993\n",
      "Epoch [8/50], Train Loss: 0.004880, Train Acc: 0.9993, Val Loss: 0.004947, Val Acc: 0.9993\n",
      "Epoch [9/50], Train Loss: 0.004775, Train Acc: 0.9993, Val Loss: 0.004828, Val Acc: 0.9993\n",
      "Epoch [10/50], Train Loss: 0.004688, Train Acc: 0.9993, Val Loss: 0.004748, Val Acc: 0.9993\n",
      "Epoch [11/50], Train Loss: 0.004571, Train Acc: 0.9993, Val Loss: 0.004608, Val Acc: 0.9993\n",
      "Epoch [12/50], Train Loss: 0.004437, Train Acc: 0.9993, Val Loss: 0.004513, Val Acc: 0.9993\n",
      "Epoch [13/50], Train Loss: 0.004292, Train Acc: 0.9993, Val Loss: 0.004327, Val Acc: 0.9993\n",
      "Epoch [14/50], Train Loss: 0.004134, Train Acc: 0.9993, Val Loss: 0.004190, Val Acc: 0.9993\n",
      "Epoch [15/50], Train Loss: 0.003977, Train Acc: 0.9993, Val Loss: 0.004040, Val Acc: 0.9993\n",
      "Epoch [16/50], Train Loss: 0.003836, Train Acc: 0.9993, Val Loss: 0.003892, Val Acc: 0.9993\n",
      "Epoch [17/50], Train Loss: 0.003689, Train Acc: 0.9993, Val Loss: 0.003752, Val Acc: 0.9993\n",
      "Epoch [18/50], Train Loss: 0.003521, Train Acc: 0.9993, Val Loss: 0.003561, Val Acc: 0.9993\n",
      "Epoch [19/50], Train Loss: 0.003374, Train Acc: 0.9993, Val Loss: 0.003428, Val Acc: 0.9993\n",
      "Epoch [20/50], Train Loss: 0.003230, Train Acc: 0.9993, Val Loss: 0.003266, Val Acc: 0.9993\n",
      "Epoch [21/50], Train Loss: 0.003065, Train Acc: 0.9994, Val Loss: 0.003131, Val Acc: 0.9994\n",
      "Epoch [22/50], Train Loss: 0.002918, Train Acc: 0.9994, Val Loss: 0.002996, Val Acc: 0.9994\n",
      "Epoch [23/50], Train Loss: 0.002796, Train Acc: 0.9994, Val Loss: 0.002884, Val Acc: 0.9994\n",
      "Epoch [24/50], Train Loss: 0.002675, Train Acc: 0.9994, Val Loss: 0.002746, Val Acc: 0.9994\n",
      "Epoch [25/50], Train Loss: 0.002541, Train Acc: 0.9994, Val Loss: 0.002623, Val Acc: 0.9994\n",
      "Epoch [26/50], Train Loss: 0.002431, Train Acc: 0.9994, Val Loss: 0.002522, Val Acc: 0.9994\n",
      "Epoch [27/50], Train Loss: 0.002331, Train Acc: 0.9994, Val Loss: 0.002417, Val Acc: 0.9994\n",
      "Epoch [28/50], Train Loss: 0.002227, Train Acc: 0.9994, Val Loss: 0.002322, Val Acc: 0.9994\n",
      "Epoch [29/50], Train Loss: 0.002125, Train Acc: 0.9994, Val Loss: 0.002223, Val Acc: 0.9994\n",
      "Epoch [30/50], Train Loss: 0.002036, Train Acc: 0.9994, Val Loss: 0.002120, Val Acc: 0.9994\n",
      "Epoch [31/50], Train Loss: 0.001956, Train Acc: 0.9995, Val Loss: 0.002031, Val Acc: 0.9995\n",
      "Epoch [32/50], Train Loss: 0.001854, Train Acc: 0.9995, Val Loss: 0.001947, Val Acc: 0.9995\n",
      "Epoch [33/50], Train Loss: 0.001774, Train Acc: 0.9995, Val Loss: 0.001854, Val Acc: 0.9995\n",
      "Epoch [34/50], Train Loss: 0.001691, Train Acc: 0.9995, Val Loss: 0.001775, Val Acc: 0.9995\n",
      "Epoch [35/50], Train Loss: 0.001613, Train Acc: 0.9995, Val Loss: 0.001699, Val Acc: 0.9995\n",
      "Epoch [36/50], Train Loss: 0.001523, Train Acc: 0.9995, Val Loss: 0.001627, Val Acc: 0.9995\n",
      "Epoch [37/50], Train Loss: 0.001459, Train Acc: 0.9996, Val Loss: 0.001549, Val Acc: 0.9996\n",
      "Epoch [38/50], Train Loss: 0.001383, Train Acc: 0.9996, Val Loss: 0.001484, Val Acc: 0.9996\n",
      "Epoch [39/50], Train Loss: 0.001323, Train Acc: 0.9996, Val Loss: 0.001416, Val Acc: 0.9996\n",
      "Epoch [40/50], Train Loss: 0.001254, Train Acc: 0.9996, Val Loss: 0.001353, Val Acc: 0.9996\n",
      "Epoch [41/50], Train Loss: 0.001194, Train Acc: 0.9996, Val Loss: 0.001292, Val Acc: 0.9996\n",
      "Epoch [42/50], Train Loss: 0.001136, Train Acc: 0.9996, Val Loss: 0.001235, Val Acc: 0.9996\n",
      "Epoch [43/50], Train Loss: 0.001086, Train Acc: 0.9997, Val Loss: 0.001188, Val Acc: 0.9996\n",
      "Epoch [44/50], Train Loss: 0.001041, Train Acc: 0.9997, Val Loss: 0.001141, Val Acc: 0.9997\n",
      "Epoch [45/50], Train Loss: 0.000992, Train Acc: 0.9997, Val Loss: 0.001089, Val Acc: 0.9997\n",
      "Epoch [46/50], Train Loss: 0.000953, Train Acc: 0.9997, Val Loss: 0.001055, Val Acc: 0.9997\n",
      "Epoch [47/50], Train Loss: 0.000907, Train Acc: 0.9997, Val Loss: 0.001008, Val Acc: 0.9997\n",
      "Epoch [48/50], Train Loss: 0.000868, Train Acc: 0.9997, Val Loss: 0.000967, Val Acc: 0.9997\n",
      "Epoch [49/50], Train Loss: 0.000830, Train Acc: 0.9997, Val Loss: 0.000933, Val Acc: 0.9997\n",
      "Epoch [50/50], Train Loss: 0.000790, Train Acc: 0.9997, Val Loss: 0.000898, Val Acc: 0.9997\n",
      "Model saved to ffnn_model_128_2048_0.001.pth\n",
      "Evaluation result saved to hyperparameter/128-2048-0.001-edge_accuracy.csv\n",
      "Epoch [1/50], Train Loss: 0.632923, Train Acc: 0.6931, Val Loss: 0.482122, Val Acc: 0.8899\n",
      "Epoch [2/50], Train Loss: 0.219801, Train Acc: 0.9685, Val Loss: 0.034554, Val Acc: 0.9993\n",
      "Epoch [3/50], Train Loss: 0.014675, Train Acc: 0.9993, Val Loss: 0.008013, Val Acc: 0.9993\n",
      "Epoch [4/50], Train Loss: 0.007042, Train Acc: 0.9993, Val Loss: 0.006557, Val Acc: 0.9993\n",
      "Epoch [5/50], Train Loss: 0.006184, Train Acc: 0.9993, Val Loss: 0.006006, Val Acc: 0.9993\n",
      "Epoch [6/50], Train Loss: 0.005769, Train Acc: 0.9993, Val Loss: 0.005714, Val Acc: 0.9993\n",
      "Epoch [7/50], Train Loss: 0.005543, Train Acc: 0.9993, Val Loss: 0.005512, Val Acc: 0.9993\n",
      "Epoch [8/50], Train Loss: 0.005436, Train Acc: 0.9993, Val Loss: 0.005422, Val Acc: 0.9993\n",
      "Epoch [9/50], Train Loss: 0.005244, Train Acc: 0.9993, Val Loss: 0.005261, Val Acc: 0.9993\n",
      "Epoch [10/50], Train Loss: 0.005162, Train Acc: 0.9993, Val Loss: 0.005201, Val Acc: 0.9993\n",
      "Epoch [11/50], Train Loss: 0.005096, Train Acc: 0.9993, Val Loss: 0.005140, Val Acc: 0.9993\n",
      "Epoch [12/50], Train Loss: 0.005018, Train Acc: 0.9993, Val Loss: 0.005079, Val Acc: 0.9993\n",
      "Epoch [13/50], Train Loss: 0.004934, Train Acc: 0.9993, Val Loss: 0.005020, Val Acc: 0.9993\n",
      "Epoch [14/50], Train Loss: 0.004871, Train Acc: 0.9993, Val Loss: 0.004943, Val Acc: 0.9993\n",
      "Epoch [15/50], Train Loss: 0.004823, Train Acc: 0.9993, Val Loss: 0.004897, Val Acc: 0.9993\n",
      "Epoch [16/50], Train Loss: 0.004749, Train Acc: 0.9993, Val Loss: 0.004837, Val Acc: 0.9993\n",
      "Epoch [17/50], Train Loss: 0.004688, Train Acc: 0.9993, Val Loss: 0.004781, Val Acc: 0.9993\n",
      "Epoch [18/50], Train Loss: 0.004609, Train Acc: 0.9993, Val Loss: 0.004692, Val Acc: 0.9993\n",
      "Epoch [19/50], Train Loss: 0.004552, Train Acc: 0.9993, Val Loss: 0.004631, Val Acc: 0.9993\n",
      "Epoch [20/50], Train Loss: 0.004470, Train Acc: 0.9993, Val Loss: 0.004577, Val Acc: 0.9993\n",
      "Epoch [21/50], Train Loss: 0.004414, Train Acc: 0.9993, Val Loss: 0.004486, Val Acc: 0.9993\n",
      "Epoch [22/50], Train Loss: 0.004369, Train Acc: 0.9993, Val Loss: 0.004434, Val Acc: 0.9993\n",
      "Epoch [23/50], Train Loss: 0.004289, Train Acc: 0.9993, Val Loss: 0.004356, Val Acc: 0.9993\n",
      "Epoch [24/50], Train Loss: 0.004184, Train Acc: 0.9993, Val Loss: 0.004298, Val Acc: 0.9993\n",
      "Epoch [25/50], Train Loss: 0.004118, Train Acc: 0.9993, Val Loss: 0.004201, Val Acc: 0.9993\n",
      "Epoch [26/50], Train Loss: 0.004041, Train Acc: 0.9993, Val Loss: 0.004121, Val Acc: 0.9993\n",
      "Epoch [27/50], Train Loss: 0.003959, Train Acc: 0.9993, Val Loss: 0.004040, Val Acc: 0.9993\n",
      "Epoch [28/50], Train Loss: 0.003881, Train Acc: 0.9993, Val Loss: 0.003953, Val Acc: 0.9993\n",
      "Epoch [29/50], Train Loss: 0.003790, Train Acc: 0.9993, Val Loss: 0.003883, Val Acc: 0.9993\n",
      "Epoch [30/50], Train Loss: 0.003705, Train Acc: 0.9993, Val Loss: 0.003801, Val Acc: 0.9993\n",
      "Epoch [31/50], Train Loss: 0.003633, Train Acc: 0.9993, Val Loss: 0.003722, Val Acc: 0.9993\n",
      "Epoch [32/50], Train Loss: 0.003545, Train Acc: 0.9993, Val Loss: 0.003638, Val Acc: 0.9993\n",
      "Epoch [33/50], Train Loss: 0.003451, Train Acc: 0.9993, Val Loss: 0.003550, Val Acc: 0.9993\n",
      "Epoch [34/50], Train Loss: 0.003373, Train Acc: 0.9993, Val Loss: 0.003456, Val Acc: 0.9993\n",
      "Epoch [35/50], Train Loss: 0.003280, Train Acc: 0.9993, Val Loss: 0.003386, Val Acc: 0.9993\n",
      "Epoch [36/50], Train Loss: 0.003200, Train Acc: 0.9993, Val Loss: 0.003300, Val Acc: 0.9993\n",
      "Epoch [37/50], Train Loss: 0.003134, Train Acc: 0.9993, Val Loss: 0.003212, Val Acc: 0.9993\n",
      "Epoch [38/50], Train Loss: 0.003053, Train Acc: 0.9994, Val Loss: 0.003141, Val Acc: 0.9994\n",
      "Epoch [39/50], Train Loss: 0.002971, Train Acc: 0.9994, Val Loss: 0.003068, Val Acc: 0.9994\n",
      "Epoch [40/50], Train Loss: 0.002900, Train Acc: 0.9994, Val Loss: 0.002996, Val Acc: 0.9994\n",
      "Epoch [41/50], Train Loss: 0.002841, Train Acc: 0.9994, Val Loss: 0.002934, Val Acc: 0.9994\n",
      "Epoch [42/50], Train Loss: 0.002770, Train Acc: 0.9994, Val Loss: 0.002860, Val Acc: 0.9994\n",
      "Epoch [43/50], Train Loss: 0.002705, Train Acc: 0.9994, Val Loss: 0.002785, Val Acc: 0.9994\n",
      "Epoch [44/50], Train Loss: 0.002641, Train Acc: 0.9994, Val Loss: 0.002741, Val Acc: 0.9994\n",
      "Epoch [45/50], Train Loss: 0.002578, Train Acc: 0.9994, Val Loss: 0.002669, Val Acc: 0.9994\n",
      "Epoch [46/50], Train Loss: 0.002516, Train Acc: 0.9994, Val Loss: 0.002619, Val Acc: 0.9994\n",
      "Epoch [47/50], Train Loss: 0.002459, Train Acc: 0.9994, Val Loss: 0.002556, Val Acc: 0.9994\n",
      "Epoch [48/50], Train Loss: 0.002392, Train Acc: 0.9994, Val Loss: 0.002495, Val Acc: 0.9994\n",
      "Epoch [49/50], Train Loss: 0.002350, Train Acc: 0.9994, Val Loss: 0.002438, Val Acc: 0.9994\n",
      "Epoch [50/50], Train Loss: 0.002278, Train Acc: 0.9994, Val Loss: 0.002389, Val Acc: 0.9994\n",
      "Model saved to ffnn_model_128_2048_0.0005.pth\n",
      "Evaluation result saved to hyperparameter/128-2048-0.0005-edge_accuracy.csv\n",
      "Epoch [1/50], Train Loss: 0.073593, Train Acc: 0.9949, Val Loss: 0.067697, Val Acc: 0.9993\n",
      "Epoch [2/50], Train Loss: 0.083371, Train Acc: 0.9991, Val Loss: 0.061863, Val Acc: 0.9993\n",
      "Epoch [3/50], Train Loss: 0.052763, Train Acc: 0.9993, Val Loss: 0.019915, Val Acc: 0.9993\n",
      "Epoch [4/50], Train Loss: 0.004436, Train Acc: 0.9994, Val Loss: 0.001167, Val Acc: 0.9997\n",
      "Epoch [5/50], Train Loss: 0.000642, Train Acc: 0.9998, Val Loss: 0.000496, Val Acc: 0.9998\n",
      "Epoch [6/50], Train Loss: 0.000300, Train Acc: 0.9999, Val Loss: 0.000367, Val Acc: 0.9999\n",
      "Epoch [7/50], Train Loss: 0.000205, Train Acc: 0.9999, Val Loss: 0.000278, Val Acc: 0.9999\n",
      "Epoch [8/50], Train Loss: 0.000164, Train Acc: 0.9999, Val Loss: 0.000268, Val Acc: 0.9999\n",
      "Epoch [9/50], Train Loss: 0.000141, Train Acc: 1.0000, Val Loss: 0.000248, Val Acc: 0.9999\n",
      "Epoch [10/50], Train Loss: 0.000126, Train Acc: 1.0000, Val Loss: 0.000229, Val Acc: 0.9999\n",
      "Epoch [11/50], Train Loss: 0.000117, Train Acc: 1.0000, Val Loss: 0.000217, Val Acc: 0.9999\n",
      "Epoch [12/50], Train Loss: 0.000112, Train Acc: 1.0000, Val Loss: 0.000219, Val Acc: 0.9999\n",
      "Epoch [13/50], Train Loss: 0.000104, Train Acc: 1.0000, Val Loss: 0.000229, Val Acc: 0.9999\n",
      "Epoch [14/50], Train Loss: 0.000102, Train Acc: 1.0000, Val Loss: 0.000216, Val Acc: 0.9999\n",
      "Epoch [15/50], Train Loss: 0.000101, Train Acc: 1.0000, Val Loss: 0.000232, Val Acc: 0.9999\n",
      "Epoch [16/50], Train Loss: 0.000098, Train Acc: 1.0000, Val Loss: 0.000215, Val Acc: 0.9999\n",
      "Epoch [17/50], Train Loss: 0.000099, Train Acc: 1.0000, Val Loss: 0.000214, Val Acc: 0.9999\n",
      "Epoch [18/50], Train Loss: 0.000094, Train Acc: 1.0000, Val Loss: 0.000218, Val Acc: 0.9999\n",
      "Epoch [19/50], Train Loss: 0.000103, Train Acc: 1.0000, Val Loss: 0.000242, Val Acc: 0.9999\n",
      "Epoch [20/50], Train Loss: 0.000105, Train Acc: 1.0000, Val Loss: 0.000244, Val Acc: 0.9999\n",
      "Epoch [21/50], Train Loss: 0.000104, Train Acc: 1.0000, Val Loss: 0.000244, Val Acc: 0.9999\n",
      "Epoch [22/50], Train Loss: 0.000115, Train Acc: 1.0000, Val Loss: 0.000236, Val Acc: 0.9999\n",
      "Epoch [23/50], Train Loss: 0.000101, Train Acc: 1.0000, Val Loss: 0.000233, Val Acc: 0.9999\n",
      "Epoch [24/50], Train Loss: 0.000095, Train Acc: 1.0000, Val Loss: 0.000228, Val Acc: 0.9999\n",
      "Epoch [25/50], Train Loss: 0.000091, Train Acc: 1.0000, Val Loss: 0.000227, Val Acc: 0.9999\n",
      "Epoch [26/50], Train Loss: 0.000089, Train Acc: 1.0000, Val Loss: 0.000230, Val Acc: 0.9999\n",
      "Epoch [27/50], Train Loss: 0.000085, Train Acc: 1.0000, Val Loss: 0.000225, Val Acc: 0.9999\n",
      "Epoch [28/50], Train Loss: 0.000087, Train Acc: 1.0000, Val Loss: 0.000234, Val Acc: 0.9999\n",
      "Epoch [29/50], Train Loss: 0.000090, Train Acc: 1.0000, Val Loss: 0.000236, Val Acc: 0.9999\n",
      "Epoch [30/50], Train Loss: 0.000091, Train Acc: 1.0000, Val Loss: 0.000270, Val Acc: 0.9999\n",
      "Epoch [31/50], Train Loss: 0.000097, Train Acc: 1.0000, Val Loss: 0.000262, Val Acc: 0.9999\n",
      "Epoch [32/50], Train Loss: 0.000102, Train Acc: 1.0000, Val Loss: 0.000246, Val Acc: 0.9999\n",
      "Epoch [33/50], Train Loss: 0.000099, Train Acc: 1.0000, Val Loss: 0.000291, Val Acc: 0.9999\n",
      "Epoch [34/50], Train Loss: 0.000096, Train Acc: 1.0000, Val Loss: 0.000260, Val Acc: 0.9999\n",
      "Epoch [35/50], Train Loss: 0.000100, Train Acc: 1.0000, Val Loss: 0.000278, Val Acc: 0.9999\n",
      "Epoch [36/50], Train Loss: 0.000092, Train Acc: 1.0000, Val Loss: 0.000270, Val Acc: 0.9999\n",
      "Epoch [37/50], Train Loss: 0.000094, Train Acc: 1.0000, Val Loss: 0.000286, Val Acc: 0.9999\n",
      "Epoch [38/50], Train Loss: 0.000103, Train Acc: 1.0000, Val Loss: 0.000277, Val Acc: 0.9999\n",
      "Epoch [39/50], Train Loss: 0.000097, Train Acc: 1.0000, Val Loss: 0.000275, Val Acc: 0.9999\n",
      "Epoch [40/50], Train Loss: 0.000095, Train Acc: 1.0000, Val Loss: 0.000299, Val Acc: 0.9999\n",
      "Epoch [41/50], Train Loss: 0.000089, Train Acc: 1.0000, Val Loss: 0.000290, Val Acc: 0.9999\n",
      "Epoch [42/50], Train Loss: 0.000084, Train Acc: 1.0000, Val Loss: 0.000269, Val Acc: 0.9999\n",
      "Epoch [43/50], Train Loss: 0.000091, Train Acc: 1.0000, Val Loss: 0.000270, Val Acc: 0.9999\n",
      "Epoch [44/50], Train Loss: 0.000085, Train Acc: 1.0000, Val Loss: 0.000266, Val Acc: 0.9999\n",
      "Epoch [45/50], Train Loss: 0.000083, Train Acc: 1.0000, Val Loss: 0.000307, Val Acc: 0.9999\n",
      "Epoch [46/50], Train Loss: 0.000091, Train Acc: 1.0000, Val Loss: 0.000297, Val Acc: 0.9999\n",
      "Epoch [47/50], Train Loss: 0.000091, Train Acc: 1.0000, Val Loss: 0.000298, Val Acc: 0.9999\n",
      "Epoch [48/50], Train Loss: 0.000091, Train Acc: 1.0000, Val Loss: 0.000291, Val Acc: 0.9999\n",
      "Epoch [49/50], Train Loss: 0.000088, Train Acc: 1.0000, Val Loss: 0.000288, Val Acc: 0.9999\n",
      "Epoch [50/50], Train Loss: 0.000092, Train Acc: 1.0000, Val Loss: 0.000305, Val Acc: 0.9999\n",
      "Model saved to ffnn_model_1024_512_0.01.pth\n",
      "Evaluation result saved to hyperparameter/1024-512-0.01-edge_accuracy.csv\n",
      "Epoch [1/50], Train Loss: 0.043482, Train Acc: 0.9911, Val Loss: 0.005943, Val Acc: 0.9993\n",
      "Epoch [2/50], Train Loss: 0.004370, Train Acc: 0.9993, Val Loss: 0.003584, Val Acc: 0.9993\n",
      "Epoch [3/50], Train Loss: 0.002909, Train Acc: 0.9993, Val Loss: 0.002611, Val Acc: 0.9994\n",
      "Epoch [4/50], Train Loss: 0.002129, Train Acc: 0.9994, Val Loss: 0.001907, Val Acc: 0.9995\n",
      "Epoch [5/50], Train Loss: 0.001486, Train Acc: 0.9995, Val Loss: 0.001341, Val Acc: 0.9996\n",
      "Epoch [6/50], Train Loss: 0.001011, Train Acc: 0.9997, Val Loss: 0.000959, Val Acc: 0.9997\n",
      "Epoch [7/50], Train Loss: 0.000696, Train Acc: 0.9998, Val Loss: 0.000691, Val Acc: 0.9998\n",
      "Epoch [8/50], Train Loss: 0.000499, Train Acc: 0.9998, Val Loss: 0.000553, Val Acc: 0.9998\n",
      "Epoch [9/50], Train Loss: 0.000373, Train Acc: 0.9999, Val Loss: 0.000432, Val Acc: 0.9999\n",
      "Epoch [10/50], Train Loss: 0.000294, Train Acc: 0.9999, Val Loss: 0.000355, Val Acc: 0.9999\n",
      "Epoch [11/50], Train Loss: 0.000239, Train Acc: 0.9999, Val Loss: 0.000315, Val Acc: 0.9999\n",
      "Epoch [12/50], Train Loss: 0.000201, Train Acc: 0.9999, Val Loss: 0.000274, Val Acc: 0.9999\n",
      "Epoch [13/50], Train Loss: 0.000177, Train Acc: 0.9999, Val Loss: 0.000246, Val Acc: 0.9999\n",
      "Epoch [14/50], Train Loss: 0.000156, Train Acc: 1.0000, Val Loss: 0.000236, Val Acc: 0.9999\n",
      "Epoch [15/50], Train Loss: 0.000141, Train Acc: 1.0000, Val Loss: 0.000225, Val Acc: 0.9999\n",
      "Epoch [16/50], Train Loss: 0.000132, Train Acc: 1.0000, Val Loss: 0.000213, Val Acc: 0.9999\n",
      "Epoch [17/50], Train Loss: 0.000121, Train Acc: 1.0000, Val Loss: 0.000194, Val Acc: 0.9999\n",
      "Epoch [18/50], Train Loss: 0.000112, Train Acc: 1.0000, Val Loss: 0.000191, Val Acc: 0.9999\n",
      "Epoch [19/50], Train Loss: 0.000107, Train Acc: 1.0000, Val Loss: 0.000186, Val Acc: 0.9999\n",
      "Epoch [20/50], Train Loss: 0.000104, Train Acc: 1.0000, Val Loss: 0.000178, Val Acc: 0.9999\n",
      "Epoch [21/50], Train Loss: 0.000102, Train Acc: 1.0000, Val Loss: 0.000188, Val Acc: 0.9999\n",
      "Epoch [22/50], Train Loss: 0.000096, Train Acc: 1.0000, Val Loss: 0.000181, Val Acc: 0.9999\n",
      "Epoch [23/50], Train Loss: 0.000092, Train Acc: 1.0000, Val Loss: 0.000175, Val Acc: 0.9999\n",
      "Epoch [24/50], Train Loss: 0.000090, Train Acc: 1.0000, Val Loss: 0.000173, Val Acc: 0.9999\n",
      "Epoch [25/50], Train Loss: 0.000089, Train Acc: 1.0000, Val Loss: 0.000179, Val Acc: 0.9999\n",
      "Epoch [26/50], Train Loss: 0.000088, Train Acc: 1.0000, Val Loss: 0.000161, Val Acc: 0.9999\n",
      "Epoch [27/50], Train Loss: 0.000085, Train Acc: 1.0000, Val Loss: 0.000166, Val Acc: 0.9999\n",
      "Epoch [28/50], Train Loss: 0.000083, Train Acc: 1.0000, Val Loss: 0.000166, Val Acc: 0.9999\n",
      "Epoch [29/50], Train Loss: 0.000082, Train Acc: 1.0000, Val Loss: 0.000167, Val Acc: 0.9999\n",
      "Epoch [30/50], Train Loss: 0.000081, Train Acc: 1.0000, Val Loss: 0.000160, Val Acc: 1.0000\n",
      "Epoch [31/50], Train Loss: 0.000081, Train Acc: 1.0000, Val Loss: 0.000163, Val Acc: 1.0000\n",
      "Epoch [32/50], Train Loss: 0.000079, Train Acc: 1.0000, Val Loss: 0.000159, Val Acc: 1.0000\n",
      "Epoch [33/50], Train Loss: 0.000076, Train Acc: 1.0000, Val Loss: 0.000157, Val Acc: 1.0000\n",
      "Epoch [34/50], Train Loss: 0.000075, Train Acc: 1.0000, Val Loss: 0.000159, Val Acc: 1.0000\n",
      "Epoch [35/50], Train Loss: 0.000076, Train Acc: 1.0000, Val Loss: 0.000153, Val Acc: 1.0000\n",
      "Epoch [36/50], Train Loss: 0.000074, Train Acc: 1.0000, Val Loss: 0.000153, Val Acc: 1.0000\n",
      "Epoch [37/50], Train Loss: 0.000075, Train Acc: 1.0000, Val Loss: 0.000143, Val Acc: 1.0000\n",
      "Epoch [38/50], Train Loss: 0.000075, Train Acc: 1.0000, Val Loss: 0.000148, Val Acc: 1.0000\n",
      "Epoch [39/50], Train Loss: 0.000072, Train Acc: 1.0000, Val Loss: 0.000146, Val Acc: 1.0000\n",
      "Epoch [40/50], Train Loss: 0.000073, Train Acc: 1.0000, Val Loss: 0.000156, Val Acc: 1.0000\n",
      "Epoch [41/50], Train Loss: 0.000073, Train Acc: 1.0000, Val Loss: 0.000140, Val Acc: 1.0000\n",
      "Epoch [42/50], Train Loss: 0.000071, Train Acc: 1.0000, Val Loss: 0.000148, Val Acc: 1.0000\n",
      "Epoch [43/50], Train Loss: 0.000071, Train Acc: 1.0000, Val Loss: 0.000150, Val Acc: 1.0000\n",
      "Epoch [44/50], Train Loss: 0.000071, Train Acc: 1.0000, Val Loss: 0.000140, Val Acc: 1.0000\n",
      "Epoch [45/50], Train Loss: 0.000071, Train Acc: 1.0000, Val Loss: 0.000147, Val Acc: 1.0000\n",
      "Epoch [46/50], Train Loss: 0.000072, Train Acc: 1.0000, Val Loss: 0.000151, Val Acc: 1.0000\n",
      "Epoch [47/50], Train Loss: 0.000069, Train Acc: 1.0000, Val Loss: 0.000145, Val Acc: 1.0000\n",
      "Epoch [48/50], Train Loss: 0.000067, Train Acc: 1.0000, Val Loss: 0.000139, Val Acc: 1.0000\n",
      "Epoch [49/50], Train Loss: 0.000068, Train Acc: 1.0000, Val Loss: 0.000139, Val Acc: 1.0000\n",
      "Epoch [50/50], Train Loss: 0.000065, Train Acc: 1.0000, Val Loss: 0.000136, Val Acc: 1.0000\n",
      "Model saved to ffnn_model_1024_512_0.001.pth\n",
      "Evaluation result saved to hyperparameter/1024-512-0.001-edge_accuracy.csv\n",
      "Epoch [1/50], Train Loss: 0.069075, Train Acc: 0.9850, Val Loss: 0.005373, Val Acc: 0.9993\n",
      "Epoch [2/50], Train Loss: 0.004835, Train Acc: 0.9993, Val Loss: 0.004611, Val Acc: 0.9993\n",
      "Epoch [3/50], Train Loss: 0.004075, Train Acc: 0.9993, Val Loss: 0.003815, Val Acc: 0.9993\n",
      "Epoch [4/50], Train Loss: 0.003310, Train Acc: 0.9993, Val Loss: 0.003128, Val Acc: 0.9993\n",
      "Epoch [5/50], Train Loss: 0.002731, Train Acc: 0.9993, Val Loss: 0.002625, Val Acc: 0.9994\n",
      "Epoch [6/50], Train Loss: 0.002279, Train Acc: 0.9994, Val Loss: 0.002190, Val Acc: 0.9994\n",
      "Epoch [7/50], Train Loss: 0.001878, Train Acc: 0.9995, Val Loss: 0.001804, Val Acc: 0.9995\n",
      "Epoch [8/50], Train Loss: 0.001517, Train Acc: 0.9995, Val Loss: 0.001459, Val Acc: 0.9996\n",
      "Epoch [9/50], Train Loss: 0.001217, Train Acc: 0.9996, Val Loss: 0.001198, Val Acc: 0.9996\n",
      "Epoch [10/50], Train Loss: 0.000973, Train Acc: 0.9997, Val Loss: 0.000978, Val Acc: 0.9997\n",
      "Epoch [11/50], Train Loss: 0.000782, Train Acc: 0.9997, Val Loss: 0.000809, Val Acc: 0.9997\n",
      "Epoch [12/50], Train Loss: 0.000635, Train Acc: 0.9998, Val Loss: 0.000683, Val Acc: 0.9998\n",
      "Epoch [13/50], Train Loss: 0.000521, Train Acc: 0.9998, Val Loss: 0.000584, Val Acc: 0.9998\n",
      "Epoch [14/50], Train Loss: 0.000435, Train Acc: 0.9999, Val Loss: 0.000504, Val Acc: 0.9998\n",
      "Epoch [15/50], Train Loss: 0.000368, Train Acc: 0.9999, Val Loss: 0.000435, Val Acc: 0.9999\n",
      "Epoch [16/50], Train Loss: 0.000317, Train Acc: 0.9999, Val Loss: 0.000392, Val Acc: 0.9999\n",
      "Epoch [17/50], Train Loss: 0.000274, Train Acc: 0.9999, Val Loss: 0.000350, Val Acc: 0.9999\n",
      "Epoch [18/50], Train Loss: 0.000243, Train Acc: 0.9999, Val Loss: 0.000317, Val Acc: 0.9999\n",
      "Epoch [19/50], Train Loss: 0.000216, Train Acc: 0.9999, Val Loss: 0.000291, Val Acc: 0.9999\n",
      "Epoch [20/50], Train Loss: 0.000195, Train Acc: 0.9999, Val Loss: 0.000269, Val Acc: 0.9999\n",
      "Epoch [21/50], Train Loss: 0.000176, Train Acc: 1.0000, Val Loss: 0.000253, Val Acc: 0.9999\n",
      "Epoch [22/50], Train Loss: 0.000162, Train Acc: 1.0000, Val Loss: 0.000243, Val Acc: 0.9999\n",
      "Epoch [23/50], Train Loss: 0.000150, Train Acc: 1.0000, Val Loss: 0.000226, Val Acc: 0.9999\n",
      "Epoch [24/50], Train Loss: 0.000138, Train Acc: 1.0000, Val Loss: 0.000224, Val Acc: 0.9999\n",
      "Epoch [25/50], Train Loss: 0.000131, Train Acc: 1.0000, Val Loss: 0.000205, Val Acc: 0.9999\n",
      "Epoch [26/50], Train Loss: 0.000123, Train Acc: 1.0000, Val Loss: 0.000198, Val Acc: 0.9999\n",
      "Epoch [27/50], Train Loss: 0.000117, Train Acc: 1.0000, Val Loss: 0.000192, Val Acc: 0.9999\n",
      "Epoch [28/50], Train Loss: 0.000112, Train Acc: 1.0000, Val Loss: 0.000193, Val Acc: 0.9999\n",
      "Epoch [29/50], Train Loss: 0.000106, Train Acc: 1.0000, Val Loss: 0.000184, Val Acc: 0.9999\n",
      "Epoch [30/50], Train Loss: 0.000102, Train Acc: 1.0000, Val Loss: 0.000177, Val Acc: 0.9999\n",
      "Epoch [31/50], Train Loss: 0.000098, Train Acc: 1.0000, Val Loss: 0.000172, Val Acc: 0.9999\n",
      "Epoch [32/50], Train Loss: 0.000095, Train Acc: 1.0000, Val Loss: 0.000173, Val Acc: 0.9999\n",
      "Epoch [33/50], Train Loss: 0.000093, Train Acc: 1.0000, Val Loss: 0.000161, Val Acc: 0.9999\n",
      "Epoch [34/50], Train Loss: 0.000090, Train Acc: 1.0000, Val Loss: 0.000165, Val Acc: 0.9999\n",
      "Epoch [35/50], Train Loss: 0.000088, Train Acc: 1.0000, Val Loss: 0.000160, Val Acc: 0.9999\n",
      "Epoch [36/50], Train Loss: 0.000086, Train Acc: 1.0000, Val Loss: 0.000166, Val Acc: 0.9999\n",
      "Epoch [37/50], Train Loss: 0.000084, Train Acc: 1.0000, Val Loss: 0.000156, Val Acc: 0.9999\n",
      "Epoch [38/50], Train Loss: 0.000083, Train Acc: 1.0000, Val Loss: 0.000155, Val Acc: 0.9999\n",
      "Epoch [39/50], Train Loss: 0.000080, Train Acc: 1.0000, Val Loss: 0.000158, Val Acc: 0.9999\n",
      "Epoch [40/50], Train Loss: 0.000081, Train Acc: 1.0000, Val Loss: 0.000153, Val Acc: 0.9999\n",
      "Epoch [41/50], Train Loss: 0.000078, Train Acc: 1.0000, Val Loss: 0.000156, Val Acc: 0.9999\n",
      "Epoch [42/50], Train Loss: 0.000077, Train Acc: 1.0000, Val Loss: 0.000159, Val Acc: 0.9999\n",
      "Epoch [43/50], Train Loss: 0.000077, Train Acc: 1.0000, Val Loss: 0.000154, Val Acc: 0.9999\n",
      "Epoch [44/50], Train Loss: 0.000075, Train Acc: 1.0000, Val Loss: 0.000148, Val Acc: 1.0000\n",
      "Epoch [45/50], Train Loss: 0.000073, Train Acc: 1.0000, Val Loss: 0.000148, Val Acc: 1.0000\n",
      "Epoch [46/50], Train Loss: 0.000074, Train Acc: 1.0000, Val Loss: 0.000148, Val Acc: 1.0000\n",
      "Epoch [47/50], Train Loss: 0.000072, Train Acc: 1.0000, Val Loss: 0.000149, Val Acc: 0.9999\n",
      "Epoch [48/50], Train Loss: 0.000073, Train Acc: 1.0000, Val Loss: 0.000146, Val Acc: 1.0000\n",
      "Epoch [49/50], Train Loss: 0.000070, Train Acc: 1.0000, Val Loss: 0.000139, Val Acc: 1.0000\n",
      "Epoch [50/50], Train Loss: 0.000070, Train Acc: 1.0000, Val Loss: 0.000139, Val Acc: 1.0000\n",
      "Model saved to ffnn_model_1024_512_0.0005.pth\n",
      "Evaluation result saved to hyperparameter/1024-512-0.0005-edge_accuracy.csv\n",
      "Epoch [1/50], Train Loss: 0.078717, Train Acc: 0.9907, Val Loss: 0.068689, Val Acc: 0.9993\n",
      "Epoch [2/50], Train Loss: 0.070011, Train Acc: 0.9993, Val Loss: 0.068155, Val Acc: 0.9993\n",
      "Epoch [3/50], Train Loss: 0.068071, Train Acc: 0.9993, Val Loss: 0.067666, Val Acc: 0.9993\n",
      "Epoch [4/50], Train Loss: 0.074938, Train Acc: 0.9992, Val Loss: 0.065948, Val Acc: 0.9993\n",
      "Epoch [5/50], Train Loss: 0.067871, Train Acc: 0.9993, Val Loss: 0.061241, Val Acc: 0.9993\n",
      "Epoch [6/50], Train Loss: 0.061811, Train Acc: 0.9993, Val Loss: 0.056561, Val Acc: 0.9990\n",
      "Epoch [7/50], Train Loss: 0.048540, Train Acc: 0.9992, Val Loss: 0.022560, Val Acc: 0.9993\n",
      "Epoch [8/50], Train Loss: 0.005945, Train Acc: 0.9994, Val Loss: 0.001630, Val Acc: 0.9995\n",
      "Epoch [9/50], Train Loss: 0.001054, Train Acc: 0.9997, Val Loss: 0.000788, Val Acc: 0.9998\n",
      "Epoch [10/50], Train Loss: 0.000547, Train Acc: 0.9999, Val Loss: 0.000536, Val Acc: 0.9999\n",
      "Epoch [11/50], Train Loss: 0.000374, Train Acc: 0.9999, Val Loss: 0.000395, Val Acc: 0.9999\n",
      "Epoch [12/50], Train Loss: 0.000250, Train Acc: 0.9999, Val Loss: 0.000311, Val Acc: 0.9999\n",
      "Epoch [13/50], Train Loss: 0.000179, Train Acc: 0.9999, Val Loss: 0.000289, Val Acc: 0.9999\n",
      "Epoch [14/50], Train Loss: 0.000168, Train Acc: 1.0000, Val Loss: 0.000267, Val Acc: 0.9999\n",
      "Epoch [15/50], Train Loss: 0.000144, Train Acc: 1.0000, Val Loss: 0.000256, Val Acc: 0.9999\n",
      "Epoch [16/50], Train Loss: 0.000156, Train Acc: 1.0000, Val Loss: 0.000247, Val Acc: 0.9999\n",
      "Epoch [17/50], Train Loss: 0.000127, Train Acc: 1.0000, Val Loss: 0.000242, Val Acc: 0.9999\n",
      "Epoch [18/50], Train Loss: 0.000118, Train Acc: 1.0000, Val Loss: 0.000249, Val Acc: 0.9999\n",
      "Epoch [19/50], Train Loss: 0.000122, Train Acc: 1.0000, Val Loss: 0.000240, Val Acc: 0.9999\n",
      "Epoch [20/50], Train Loss: 0.000112, Train Acc: 1.0000, Val Loss: 0.000236, Val Acc: 0.9999\n",
      "Epoch [21/50], Train Loss: 0.000110, Train Acc: 1.0000, Val Loss: 0.000234, Val Acc: 0.9999\n",
      "Epoch [22/50], Train Loss: 0.000115, Train Acc: 1.0000, Val Loss: 0.000237, Val Acc: 0.9999\n",
      "Epoch [23/50], Train Loss: 0.000117, Train Acc: 1.0000, Val Loss: 0.000235, Val Acc: 0.9999\n",
      "Epoch [24/50], Train Loss: 0.000108, Train Acc: 1.0000, Val Loss: 0.000231, Val Acc: 0.9999\n",
      "Epoch [25/50], Train Loss: 0.000113, Train Acc: 1.0000, Val Loss: 0.000224, Val Acc: 0.9999\n",
      "Epoch [26/50], Train Loss: 0.000103, Train Acc: 1.0000, Val Loss: 0.000233, Val Acc: 0.9999\n",
      "Epoch [27/50], Train Loss: 0.000102, Train Acc: 1.0000, Val Loss: 0.000227, Val Acc: 0.9999\n",
      "Epoch [28/50], Train Loss: 0.000104, Train Acc: 1.0000, Val Loss: 0.000229, Val Acc: 0.9999\n",
      "Epoch [29/50], Train Loss: 0.000095, Train Acc: 1.0000, Val Loss: 0.000215, Val Acc: 0.9999\n",
      "Epoch [30/50], Train Loss: 0.000100, Train Acc: 1.0000, Val Loss: 0.000223, Val Acc: 0.9999\n",
      "Epoch [31/50], Train Loss: 0.000093, Train Acc: 1.0000, Val Loss: 0.000226, Val Acc: 0.9999\n",
      "Epoch [32/50], Train Loss: 0.000094, Train Acc: 1.0000, Val Loss: 0.000229, Val Acc: 0.9999\n",
      "Epoch [33/50], Train Loss: 0.000093, Train Acc: 1.0000, Val Loss: 0.000227, Val Acc: 0.9999\n",
      "Epoch [34/50], Train Loss: 0.000096, Train Acc: 1.0000, Val Loss: 0.000226, Val Acc: 0.9999\n",
      "Epoch [35/50], Train Loss: 0.000090, Train Acc: 1.0000, Val Loss: 0.000226, Val Acc: 0.9999\n",
      "Epoch [36/50], Train Loss: 0.000101, Train Acc: 1.0000, Val Loss: 0.000230, Val Acc: 0.9999\n",
      "Epoch [37/50], Train Loss: 0.000089, Train Acc: 1.0000, Val Loss: 0.000210, Val Acc: 0.9999\n",
      "Epoch [38/50], Train Loss: 0.000093, Train Acc: 1.0000, Val Loss: 0.000227, Val Acc: 0.9999\n",
      "Epoch [39/50], Train Loss: 0.000093, Train Acc: 1.0000, Val Loss: 0.000234, Val Acc: 0.9999\n",
      "Epoch [40/50], Train Loss: 0.000095, Train Acc: 1.0000, Val Loss: 0.000233, Val Acc: 0.9999\n",
      "Epoch [41/50], Train Loss: 0.000093, Train Acc: 1.0000, Val Loss: 0.000237, Val Acc: 0.9999\n",
      "Epoch [42/50], Train Loss: 0.000089, Train Acc: 1.0000, Val Loss: 0.000223, Val Acc: 0.9999\n",
      "Epoch [43/50], Train Loss: 0.000094, Train Acc: 1.0000, Val Loss: 0.000240, Val Acc: 0.9999\n",
      "Epoch [44/50], Train Loss: 0.000099, Train Acc: 1.0000, Val Loss: 0.000254, Val Acc: 0.9999\n",
      "Epoch [45/50], Train Loss: 0.000107, Train Acc: 1.0000, Val Loss: 0.000258, Val Acc: 0.9999\n",
      "Epoch [46/50], Train Loss: 0.000101, Train Acc: 1.0000, Val Loss: 0.000257, Val Acc: 0.9999\n",
      "Epoch [47/50], Train Loss: 0.000107, Train Acc: 1.0000, Val Loss: 0.000254, Val Acc: 0.9999\n",
      "Epoch [48/50], Train Loss: 0.000102, Train Acc: 1.0000, Val Loss: 0.000248, Val Acc: 0.9999\n",
      "Epoch [49/50], Train Loss: 0.000099, Train Acc: 1.0000, Val Loss: 0.000249, Val Acc: 0.9999\n",
      "Epoch [50/50], Train Loss: 0.000099, Train Acc: 1.0000, Val Loss: 0.000248, Val Acc: 0.9999\n",
      "Model saved to ffnn_model_1024_1024_0.01.pth\n",
      "Evaluation result saved to hyperparameter/1024-1024-0.01-edge_accuracy.csv\n",
      "Epoch [1/50], Train Loss: 0.078270, Train Acc: 0.9833, Val Loss: 0.009952, Val Acc: 0.9993\n",
      "Epoch [2/50], Train Loss: 0.007233, Train Acc: 0.9993, Val Loss: 0.005749, Val Acc: 0.9993\n",
      "Epoch [3/50], Train Loss: 0.004733, Train Acc: 0.9993, Val Loss: 0.004288, Val Acc: 0.9993\n",
      "Epoch [4/50], Train Loss: 0.003676, Train Acc: 0.9993, Val Loss: 0.003419, Val Acc: 0.9993\n",
      "Epoch [5/50], Train Loss: 0.003002, Train Acc: 0.9993, Val Loss: 0.002863, Val Acc: 0.9993\n",
      "Epoch [6/50], Train Loss: 0.002513, Train Acc: 0.9994, Val Loss: 0.002419, Val Acc: 0.9994\n",
      "Epoch [7/50], Train Loss: 0.002084, Train Acc: 0.9994, Val Loss: 0.001991, Val Acc: 0.9994\n",
      "Epoch [8/50], Train Loss: 0.001696, Train Acc: 0.9995, Val Loss: 0.001636, Val Acc: 0.9995\n",
      "Epoch [9/50], Train Loss: 0.001360, Train Acc: 0.9996, Val Loss: 0.001318, Val Acc: 0.9996\n",
      "Epoch [10/50], Train Loss: 0.001084, Train Acc: 0.9996, Val Loss: 0.001081, Val Acc: 0.9997\n",
      "Epoch [11/50], Train Loss: 0.000868, Train Acc: 0.9997, Val Loss: 0.000888, Val Acc: 0.9997\n",
      "Epoch [12/50], Train Loss: 0.000698, Train Acc: 0.9998, Val Loss: 0.000734, Val Acc: 0.9998\n",
      "Epoch [13/50], Train Loss: 0.000571, Train Acc: 0.9998, Val Loss: 0.000622, Val Acc: 0.9998\n",
      "Epoch [14/50], Train Loss: 0.000474, Train Acc: 0.9999, Val Loss: 0.000529, Val Acc: 0.9998\n",
      "Epoch [15/50], Train Loss: 0.000396, Train Acc: 0.9999, Val Loss: 0.000461, Val Acc: 0.9999\n",
      "Epoch [16/50], Train Loss: 0.000339, Train Acc: 0.9999, Val Loss: 0.000411, Val Acc: 0.9999\n",
      "Epoch [17/50], Train Loss: 0.000295, Train Acc: 0.9999, Val Loss: 0.000364, Val Acc: 0.9999\n",
      "Epoch [18/50], Train Loss: 0.000260, Train Acc: 0.9999, Val Loss: 0.000330, Val Acc: 0.9999\n",
      "Epoch [19/50], Train Loss: 0.000232, Train Acc: 0.9999, Val Loss: 0.000304, Val Acc: 0.9999\n",
      "Epoch [20/50], Train Loss: 0.000208, Train Acc: 0.9999, Val Loss: 0.000288, Val Acc: 0.9999\n",
      "Epoch [21/50], Train Loss: 0.000189, Train Acc: 0.9999, Val Loss: 0.000261, Val Acc: 0.9999\n",
      "Epoch [22/50], Train Loss: 0.000171, Train Acc: 1.0000, Val Loss: 0.000252, Val Acc: 0.9999\n",
      "Epoch [23/50], Train Loss: 0.000159, Train Acc: 1.0000, Val Loss: 0.000238, Val Acc: 0.9999\n",
      "Epoch [24/50], Train Loss: 0.000148, Train Acc: 1.0000, Val Loss: 0.000227, Val Acc: 0.9999\n",
      "Epoch [25/50], Train Loss: 0.000139, Train Acc: 1.0000, Val Loss: 0.000222, Val Acc: 0.9999\n",
      "Epoch [26/50], Train Loss: 0.000131, Train Acc: 1.0000, Val Loss: 0.000210, Val Acc: 0.9999\n",
      "Epoch [27/50], Train Loss: 0.000123, Train Acc: 1.0000, Val Loss: 0.000205, Val Acc: 0.9999\n",
      "Epoch [28/50], Train Loss: 0.000120, Train Acc: 1.0000, Val Loss: 0.000196, Val Acc: 0.9999\n",
      "Epoch [29/50], Train Loss: 0.000116, Train Acc: 1.0000, Val Loss: 0.000195, Val Acc: 0.9999\n",
      "Epoch [30/50], Train Loss: 0.000109, Train Acc: 1.0000, Val Loss: 0.000183, Val Acc: 0.9999\n",
      "Epoch [31/50], Train Loss: 0.000106, Train Acc: 1.0000, Val Loss: 0.000188, Val Acc: 0.9999\n",
      "Epoch [32/50], Train Loss: 0.000104, Train Acc: 1.0000, Val Loss: 0.000184, Val Acc: 0.9999\n",
      "Epoch [33/50], Train Loss: 0.000102, Train Acc: 1.0000, Val Loss: 0.000178, Val Acc: 0.9999\n",
      "Epoch [34/50], Train Loss: 0.000094, Train Acc: 1.0000, Val Loss: 0.000178, Val Acc: 0.9999\n",
      "Epoch [35/50], Train Loss: 0.000094, Train Acc: 1.0000, Val Loss: 0.000174, Val Acc: 0.9999\n",
      "Epoch [36/50], Train Loss: 0.000091, Train Acc: 1.0000, Val Loss: 0.000174, Val Acc: 0.9999\n",
      "Epoch [37/50], Train Loss: 0.000091, Train Acc: 1.0000, Val Loss: 0.000166, Val Acc: 0.9999\n",
      "Epoch [38/50], Train Loss: 0.000087, Train Acc: 1.0000, Val Loss: 0.000174, Val Acc: 0.9999\n",
      "Epoch [39/50], Train Loss: 0.000088, Train Acc: 1.0000, Val Loss: 0.000165, Val Acc: 0.9999\n",
      "Epoch [40/50], Train Loss: 0.000082, Train Acc: 1.0000, Val Loss: 0.000161, Val Acc: 0.9999\n",
      "Epoch [41/50], Train Loss: 0.000080, Train Acc: 1.0000, Val Loss: 0.000162, Val Acc: 0.9999\n",
      "Epoch [42/50], Train Loss: 0.000082, Train Acc: 1.0000, Val Loss: 0.000165, Val Acc: 0.9999\n",
      "Epoch [43/50], Train Loss: 0.000087, Train Acc: 1.0000, Val Loss: 0.000166, Val Acc: 0.9999\n",
      "Epoch [44/50], Train Loss: 0.000078, Train Acc: 1.0000, Val Loss: 0.000156, Val Acc: 0.9999\n",
      "Epoch [45/50], Train Loss: 0.000076, Train Acc: 1.0000, Val Loss: 0.000155, Val Acc: 0.9999\n",
      "Epoch [46/50], Train Loss: 0.000077, Train Acc: 1.0000, Val Loss: 0.000162, Val Acc: 0.9999\n",
      "Epoch [47/50], Train Loss: 0.000077, Train Acc: 1.0000, Val Loss: 0.000153, Val Acc: 0.9999\n",
      "Epoch [48/50], Train Loss: 0.000084, Train Acc: 1.0000, Val Loss: 0.000157, Val Acc: 0.9999\n",
      "Epoch [49/50], Train Loss: 0.000080, Train Acc: 1.0000, Val Loss: 0.000151, Val Acc: 1.0000\n",
      "Epoch [50/50], Train Loss: 0.000073, Train Acc: 1.0000, Val Loss: 0.000154, Val Acc: 0.9999\n",
      "Model saved to ffnn_model_1024_1024_0.001.pth\n",
      "Evaluation result saved to hyperparameter/1024-1024-0.001-edge_accuracy.csv\n",
      "Epoch [1/50], Train Loss: 0.129035, Train Acc: 0.9708, Val Loss: 0.007424, Val Acc: 0.9993\n",
      "Epoch [2/50], Train Loss: 0.006021, Train Acc: 0.9993, Val Loss: 0.005342, Val Acc: 0.9993\n",
      "Epoch [3/50], Train Loss: 0.004944, Train Acc: 0.9993, Val Loss: 0.004851, Val Acc: 0.9993\n",
      "Epoch [4/50], Train Loss: 0.004548, Train Acc: 0.9993, Val Loss: 0.004480, Val Acc: 0.9993\n",
      "Epoch [5/50], Train Loss: 0.004164, Train Acc: 0.9993, Val Loss: 0.004060, Val Acc: 0.9993\n",
      "Epoch [6/50], Train Loss: 0.003730, Train Acc: 0.9993, Val Loss: 0.003634, Val Acc: 0.9993\n",
      "Epoch [7/50], Train Loss: 0.003318, Train Acc: 0.9993, Val Loss: 0.003259, Val Acc: 0.9993\n",
      "Epoch [8/50], Train Loss: 0.002971, Train Acc: 0.9993, Val Loss: 0.002943, Val Acc: 0.9993\n",
      "Epoch [9/50], Train Loss: 0.002675, Train Acc: 0.9994, Val Loss: 0.002659, Val Acc: 0.9994\n",
      "Epoch [10/50], Train Loss: 0.002416, Train Acc: 0.9994, Val Loss: 0.002409, Val Acc: 0.9994\n",
      "Epoch [11/50], Train Loss: 0.002171, Train Acc: 0.9994, Val Loss: 0.002170, Val Acc: 0.9994\n",
      "Epoch [12/50], Train Loss: 0.001943, Train Acc: 0.9995, Val Loss: 0.001949, Val Acc: 0.9995\n",
      "Epoch [13/50], Train Loss: 0.001725, Train Acc: 0.9995, Val Loss: 0.001735, Val Acc: 0.9995\n",
      "Epoch [14/50], Train Loss: 0.001523, Train Acc: 0.9995, Val Loss: 0.001546, Val Acc: 0.9995\n",
      "Epoch [15/50], Train Loss: 0.001345, Train Acc: 0.9996, Val Loss: 0.001377, Val Acc: 0.9996\n",
      "Epoch [16/50], Train Loss: 0.001184, Train Acc: 0.9996, Val Loss: 0.001225, Val Acc: 0.9996\n",
      "Epoch [17/50], Train Loss: 0.001041, Train Acc: 0.9997, Val Loss: 0.001092, Val Acc: 0.9997\n",
      "Epoch [18/50], Train Loss: 0.000915, Train Acc: 0.9997, Val Loss: 0.000977, Val Acc: 0.9997\n",
      "Epoch [19/50], Train Loss: 0.000807, Train Acc: 0.9997, Val Loss: 0.000875, Val Acc: 0.9997\n",
      "Epoch [20/50], Train Loss: 0.000714, Train Acc: 0.9998, Val Loss: 0.000785, Val Acc: 0.9997\n",
      "Epoch [21/50], Train Loss: 0.000635, Train Acc: 0.9998, Val Loss: 0.000712, Val Acc: 0.9998\n",
      "Epoch [22/50], Train Loss: 0.000566, Train Acc: 0.9998, Val Loss: 0.000648, Val Acc: 0.9998\n",
      "Epoch [23/50], Train Loss: 0.000507, Train Acc: 0.9998, Val Loss: 0.000591, Val Acc: 0.9998\n",
      "Epoch [24/50], Train Loss: 0.000455, Train Acc: 0.9999, Val Loss: 0.000538, Val Acc: 0.9998\n",
      "Epoch [25/50], Train Loss: 0.000412, Train Acc: 0.9999, Val Loss: 0.000500, Val Acc: 0.9998\n",
      "Epoch [26/50], Train Loss: 0.000373, Train Acc: 0.9999, Val Loss: 0.000459, Val Acc: 0.9999\n",
      "Epoch [27/50], Train Loss: 0.000341, Train Acc: 0.9999, Val Loss: 0.000426, Val Acc: 0.9999\n",
      "Epoch [28/50], Train Loss: 0.000315, Train Acc: 0.9999, Val Loss: 0.000399, Val Acc: 0.9999\n",
      "Epoch [29/50], Train Loss: 0.000289, Train Acc: 0.9999, Val Loss: 0.000375, Val Acc: 0.9999\n",
      "Epoch [30/50], Train Loss: 0.000266, Train Acc: 0.9999, Val Loss: 0.000353, Val Acc: 0.9999\n",
      "Epoch [31/50], Train Loss: 0.000246, Train Acc: 0.9999, Val Loss: 0.000335, Val Acc: 0.9999\n",
      "Epoch [32/50], Train Loss: 0.000231, Train Acc: 0.9999, Val Loss: 0.000315, Val Acc: 0.9999\n",
      "Epoch [33/50], Train Loss: 0.000215, Train Acc: 0.9999, Val Loss: 0.000304, Val Acc: 0.9999\n",
      "Epoch [34/50], Train Loss: 0.000203, Train Acc: 0.9999, Val Loss: 0.000292, Val Acc: 0.9999\n",
      "Epoch [35/50], Train Loss: 0.000192, Train Acc: 0.9999, Val Loss: 0.000277, Val Acc: 0.9999\n",
      "Epoch [36/50], Train Loss: 0.000180, Train Acc: 1.0000, Val Loss: 0.000267, Val Acc: 0.9999\n",
      "Epoch [37/50], Train Loss: 0.000171, Train Acc: 1.0000, Val Loss: 0.000256, Val Acc: 0.9999\n",
      "Epoch [38/50], Train Loss: 0.000163, Train Acc: 1.0000, Val Loss: 0.000249, Val Acc: 0.9999\n",
      "Epoch [39/50], Train Loss: 0.000156, Train Acc: 1.0000, Val Loss: 0.000241, Val Acc: 0.9999\n",
      "Epoch [40/50], Train Loss: 0.000147, Train Acc: 1.0000, Val Loss: 0.000234, Val Acc: 0.9999\n",
      "Epoch [41/50], Train Loss: 0.000142, Train Acc: 1.0000, Val Loss: 0.000227, Val Acc: 0.9999\n",
      "Epoch [42/50], Train Loss: 0.000136, Train Acc: 1.0000, Val Loss: 0.000221, Val Acc: 0.9999\n",
      "Epoch [43/50], Train Loss: 0.000129, Train Acc: 1.0000, Val Loss: 0.000215, Val Acc: 0.9999\n",
      "Epoch [44/50], Train Loss: 0.000125, Train Acc: 1.0000, Val Loss: 0.000209, Val Acc: 0.9999\n",
      "Epoch [45/50], Train Loss: 0.000119, Train Acc: 1.0000, Val Loss: 0.000203, Val Acc: 0.9999\n",
      "Epoch [46/50], Train Loss: 0.000117, Train Acc: 1.0000, Val Loss: 0.000203, Val Acc: 0.9999\n",
      "Epoch [47/50], Train Loss: 0.000113, Train Acc: 1.0000, Val Loss: 0.000203, Val Acc: 0.9999\n",
      "Epoch [48/50], Train Loss: 0.000110, Train Acc: 1.0000, Val Loss: 0.000196, Val Acc: 0.9999\n",
      "Epoch [49/50], Train Loss: 0.000108, Train Acc: 1.0000, Val Loss: 0.000192, Val Acc: 0.9999\n",
      "Epoch [50/50], Train Loss: 0.000105, Train Acc: 1.0000, Val Loss: 0.000187, Val Acc: 0.9999\n",
      "Model saved to ffnn_model_1024_1024_0.0005.pth\n",
      "Evaluation result saved to hyperparameter/1024-1024-0.0005-edge_accuracy.csv\n",
      "Epoch [1/50], Train Loss: 0.085402, Train Acc: 0.9820, Val Loss: 0.068384, Val Acc: 0.9993\n",
      "Epoch [2/50], Train Loss: 0.124618, Train Acc: 0.9988, Val Loss: 0.154718, Val Acc: 0.9985\n",
      "Epoch [3/50], Train Loss: 0.154545, Train Acc: 0.9985, Val Loss: 0.154381, Val Acc: 0.9985\n",
      "Epoch [4/50], Train Loss: 0.154507, Train Acc: 0.9985, Val Loss: 0.154741, Val Acc: 0.9985\n",
      "Epoch [5/50], Train Loss: 0.154351, Train Acc: 0.9985, Val Loss: 0.154518, Val Acc: 0.9985\n",
      "Epoch [6/50], Train Loss: 0.154470, Train Acc: 0.9985, Val Loss: 0.154773, Val Acc: 0.9985\n",
      "Epoch [7/50], Train Loss: 0.154839, Train Acc: 0.9985, Val Loss: 0.154474, Val Acc: 0.9985\n",
      "Epoch [8/50], Train Loss: 0.154720, Train Acc: 0.9985, Val Loss: 0.154269, Val Acc: 0.9985\n",
      "Epoch [9/50], Train Loss: 0.154510, Train Acc: 0.9985, Val Loss: 0.154348, Val Acc: 0.9985\n",
      "Epoch [10/50], Train Loss: 0.100547, Train Acc: 0.9990, Val Loss: 0.068827, Val Acc: 0.9993\n",
      "Epoch [11/50], Train Loss: 0.068919, Train Acc: 0.9993, Val Loss: 0.068887, Val Acc: 0.9993\n",
      "Epoch [12/50], Train Loss: 0.068740, Train Acc: 0.9993, Val Loss: 0.068863, Val Acc: 0.9993\n",
      "Epoch [13/50], Train Loss: 0.068570, Train Acc: 0.9993, Val Loss: 0.068768, Val Acc: 0.9993\n",
      "Epoch [14/50], Train Loss: 0.068681, Train Acc: 0.9993, Val Loss: 0.068699, Val Acc: 0.9993\n",
      "Epoch [15/50], Train Loss: 0.068810, Train Acc: 0.9993, Val Loss: 0.069052, Val Acc: 0.9993\n",
      "Epoch [16/50], Train Loss: 0.068589, Train Acc: 0.9993, Val Loss: 0.069254, Val Acc: 0.9993\n",
      "Epoch [17/50], Train Loss: 0.068758, Train Acc: 0.9993, Val Loss: 0.069186, Val Acc: 0.9993\n",
      "Epoch [18/50], Train Loss: 0.068573, Train Acc: 0.9993, Val Loss: 0.069115, Val Acc: 0.9993\n",
      "Epoch [19/50], Train Loss: 0.068438, Train Acc: 0.9993, Val Loss: 0.068454, Val Acc: 0.9993\n",
      "Epoch [20/50], Train Loss: 0.068654, Train Acc: 0.9993, Val Loss: 0.068629, Val Acc: 0.9993\n",
      "Epoch [21/50], Train Loss: 0.068405, Train Acc: 0.9993, Val Loss: 0.068961, Val Acc: 0.9993\n",
      "Epoch [22/50], Train Loss: 0.068537, Train Acc: 0.9993, Val Loss: 0.068924, Val Acc: 0.9993\n",
      "Epoch [23/50], Train Loss: 0.068768, Train Acc: 0.9993, Val Loss: 0.068743, Val Acc: 0.9993\n",
      "Epoch [24/50], Train Loss: 0.068537, Train Acc: 0.9993, Val Loss: 0.068602, Val Acc: 0.9993\n",
      "Epoch [25/50], Train Loss: 0.068569, Train Acc: 0.9993, Val Loss: 0.068698, Val Acc: 0.9993\n",
      "Epoch [26/50], Train Loss: 0.068549, Train Acc: 0.9993, Val Loss: 0.068691, Val Acc: 0.9993\n",
      "Epoch [27/50], Train Loss: 0.068644, Train Acc: 0.9993, Val Loss: 0.068428, Val Acc: 0.9993\n",
      "Epoch [28/50], Train Loss: 0.077254, Train Acc: 0.9992, Val Loss: 0.068735, Val Acc: 0.9993\n",
      "Epoch [29/50], Train Loss: 0.068604, Train Acc: 0.9993, Val Loss: 0.068599, Val Acc: 0.9993\n",
      "Epoch [30/50], Train Loss: 0.068381, Train Acc: 0.9993, Val Loss: 0.068795, Val Acc: 0.9993\n",
      "Epoch [31/50], Train Loss: 0.070010, Train Acc: 0.9993, Val Loss: 0.068500, Val Acc: 0.9993\n",
      "Epoch [32/50], Train Loss: 0.068699, Train Acc: 0.9993, Val Loss: 0.068688, Val Acc: 0.9993\n",
      "Epoch [33/50], Train Loss: 0.068878, Train Acc: 0.9993, Val Loss: 0.068862, Val Acc: 0.9993\n",
      "Epoch [34/50], Train Loss: 0.068526, Train Acc: 0.9993, Val Loss: 0.068914, Val Acc: 0.9993\n",
      "Epoch [35/50], Train Loss: 0.068308, Train Acc: 0.9993, Val Loss: 0.068790, Val Acc: 0.9993\n",
      "Epoch [36/50], Train Loss: 0.068948, Train Acc: 0.9993, Val Loss: 0.068579, Val Acc: 0.9993\n",
      "Epoch [37/50], Train Loss: 0.068765, Train Acc: 0.9993, Val Loss: 0.068471, Val Acc: 0.9993\n",
      "Epoch [38/50], Train Loss: 0.068398, Train Acc: 0.9993, Val Loss: 0.068565, Val Acc: 0.9993\n",
      "Epoch [39/50], Train Loss: 0.068610, Train Acc: 0.9993, Val Loss: 0.068902, Val Acc: 0.9993\n",
      "Epoch [40/50], Train Loss: 0.068644, Train Acc: 0.9993, Val Loss: 0.068437, Val Acc: 0.9993\n",
      "Epoch [41/50], Train Loss: 0.068507, Train Acc: 0.9993, Val Loss: 0.068817, Val Acc: 0.9993\n",
      "Epoch [42/50], Train Loss: 0.068551, Train Acc: 0.9993, Val Loss: 0.068735, Val Acc: 0.9993\n",
      "Epoch [43/50], Train Loss: 0.068570, Train Acc: 0.9993, Val Loss: 0.069036, Val Acc: 0.9993\n",
      "Epoch [44/50], Train Loss: 0.068771, Train Acc: 0.9993, Val Loss: 0.068947, Val Acc: 0.9993\n",
      "Epoch [45/50], Train Loss: 0.068272, Train Acc: 0.9993, Val Loss: 0.068700, Val Acc: 0.9993\n",
      "Epoch [46/50], Train Loss: 0.068665, Train Acc: 0.9993, Val Loss: 0.068830, Val Acc: 0.9993\n",
      "Epoch [47/50], Train Loss: 0.068337, Train Acc: 0.9993, Val Loss: 0.068476, Val Acc: 0.9993\n",
      "Epoch [48/50], Train Loss: 0.068534, Train Acc: 0.9993, Val Loss: 0.068627, Val Acc: 0.9993\n",
      "Epoch [49/50], Train Loss: 0.068265, Train Acc: 0.9993, Val Loss: 0.068859, Val Acc: 0.9993\n",
      "Epoch [50/50], Train Loss: 0.068547, Train Acc: 0.9993, Val Loss: 0.069020, Val Acc: 0.9993\n",
      "Model saved to ffnn_model_1024_2048_0.01.pth\n",
      "Evaluation result saved to hyperparameter/1024-2048-0.01-edge_accuracy.csv\n",
      "Epoch [1/50], Train Loss: 0.142349, Train Acc: 0.9666, Val Loss: 0.014396, Val Acc: 0.9993\n",
      "Epoch [2/50], Train Loss: 0.012437, Train Acc: 0.9993, Val Loss: 0.010233, Val Acc: 0.9993\n",
      "Epoch [3/50], Train Loss: 0.008460, Train Acc: 0.9993, Val Loss: 0.007290, Val Acc: 0.9993\n",
      "Epoch [4/50], Train Loss: 0.006185, Train Acc: 0.9993, Val Loss: 0.005725, Val Acc: 0.9993\n",
      "Epoch [5/50], Train Loss: 0.005038, Train Acc: 0.9993, Val Loss: 0.004806, Val Acc: 0.9993\n",
      "Epoch [6/50], Train Loss: 0.004324, Train Acc: 0.9993, Val Loss: 0.004176, Val Acc: 0.9993\n",
      "Epoch [7/50], Train Loss: 0.003775, Train Acc: 0.9993, Val Loss: 0.003682, Val Acc: 0.9993\n",
      "Epoch [8/50], Train Loss: 0.003332, Train Acc: 0.9993, Val Loss: 0.003271, Val Acc: 0.9993\n",
      "Epoch [9/50], Train Loss: 0.002988, Train Acc: 0.9993, Val Loss: 0.002970, Val Acc: 0.9993\n",
      "Epoch [10/50], Train Loss: 0.002712, Train Acc: 0.9993, Val Loss: 0.002714, Val Acc: 0.9994\n",
      "Epoch [11/50], Train Loss: 0.002461, Train Acc: 0.9994, Val Loss: 0.002468, Val Acc: 0.9994\n",
      "Epoch [12/50], Train Loss: 0.002232, Train Acc: 0.9994, Val Loss: 0.002247, Val Acc: 0.9994\n",
      "Epoch [13/50], Train Loss: 0.002010, Train Acc: 0.9994, Val Loss: 0.002035, Val Acc: 0.9994\n",
      "Epoch [14/50], Train Loss: 0.001803, Train Acc: 0.9995, Val Loss: 0.001812, Val Acc: 0.9995\n",
      "Epoch [15/50], Train Loss: 0.001602, Train Acc: 0.9995, Val Loss: 0.001629, Val Acc: 0.9995\n",
      "Epoch [16/50], Train Loss: 0.001420, Train Acc: 0.9995, Val Loss: 0.001455, Val Acc: 0.9996\n",
      "Epoch [17/50], Train Loss: 0.001258, Train Acc: 0.9996, Val Loss: 0.001294, Val Acc: 0.9996\n",
      "Epoch [18/50], Train Loss: 0.001109, Train Acc: 0.9996, Val Loss: 0.001158, Val Acc: 0.9996\n",
      "Epoch [19/50], Train Loss: 0.000980, Train Acc: 0.9997, Val Loss: 0.001038, Val Acc: 0.9997\n",
      "Epoch [20/50], Train Loss: 0.000870, Train Acc: 0.9997, Val Loss: 0.000923, Val Acc: 0.9997\n",
      "Epoch [21/50], Train Loss: 0.000769, Train Acc: 0.9997, Val Loss: 0.000823, Val Acc: 0.9997\n",
      "Epoch [22/50], Train Loss: 0.000688, Train Acc: 0.9998, Val Loss: 0.000741, Val Acc: 0.9998\n",
      "Epoch [23/50], Train Loss: 0.000607, Train Acc: 0.9998, Val Loss: 0.000679, Val Acc: 0.9998\n",
      "Epoch [24/50], Train Loss: 0.000547, Train Acc: 0.9998, Val Loss: 0.000614, Val Acc: 0.9998\n",
      "Epoch [25/50], Train Loss: 0.000492, Train Acc: 0.9998, Val Loss: 0.000572, Val Acc: 0.9998\n",
      "Epoch [26/50], Train Loss: 0.000446, Train Acc: 0.9999, Val Loss: 0.000526, Val Acc: 0.9998\n",
      "Epoch [27/50], Train Loss: 0.000408, Train Acc: 0.9999, Val Loss: 0.000486, Val Acc: 0.9999\n",
      "Epoch [28/50], Train Loss: 0.000371, Train Acc: 0.9999, Val Loss: 0.000452, Val Acc: 0.9999\n",
      "Epoch [29/50], Train Loss: 0.000337, Train Acc: 0.9999, Val Loss: 0.000420, Val Acc: 0.9999\n",
      "Epoch [30/50], Train Loss: 0.000312, Train Acc: 0.9999, Val Loss: 0.000402, Val Acc: 0.9999\n",
      "Epoch [31/50], Train Loss: 0.000291, Train Acc: 0.9999, Val Loss: 0.000373, Val Acc: 0.9999\n",
      "Epoch [32/50], Train Loss: 0.000269, Train Acc: 0.9999, Val Loss: 0.000355, Val Acc: 0.9999\n",
      "Epoch [33/50], Train Loss: 0.000253, Train Acc: 0.9999, Val Loss: 0.000343, Val Acc: 0.9999\n",
      "Epoch [34/50], Train Loss: 0.000237, Train Acc: 0.9999, Val Loss: 0.000324, Val Acc: 0.9999\n",
      "Epoch [35/50], Train Loss: 0.000224, Train Acc: 0.9999, Val Loss: 0.000310, Val Acc: 0.9999\n",
      "Epoch [36/50], Train Loss: 0.000210, Train Acc: 0.9999, Val Loss: 0.000294, Val Acc: 0.9999\n",
      "Epoch [37/50], Train Loss: 0.000204, Train Acc: 0.9999, Val Loss: 0.000289, Val Acc: 0.9999\n",
      "Epoch [38/50], Train Loss: 0.000190, Train Acc: 0.9999, Val Loss: 0.000274, Val Acc: 0.9999\n",
      "Epoch [39/50], Train Loss: 0.000177, Train Acc: 1.0000, Val Loss: 0.000268, Val Acc: 0.9999\n",
      "Epoch [40/50], Train Loss: 0.000170, Train Acc: 1.0000, Val Loss: 0.000252, Val Acc: 0.9999\n",
      "Epoch [41/50], Train Loss: 0.000166, Train Acc: 1.0000, Val Loss: 0.000255, Val Acc: 0.9999\n",
      "Epoch [42/50], Train Loss: 0.000157, Train Acc: 1.0000, Val Loss: 0.000241, Val Acc: 0.9999\n",
      "Epoch [43/50], Train Loss: 0.000151, Train Acc: 1.0000, Val Loss: 0.000241, Val Acc: 0.9999\n",
      "Epoch [44/50], Train Loss: 0.000151, Train Acc: 1.0000, Val Loss: 0.000233, Val Acc: 0.9999\n",
      "Epoch [45/50], Train Loss: 0.000140, Train Acc: 1.0000, Val Loss: 0.000228, Val Acc: 0.9999\n",
      "Epoch [46/50], Train Loss: 0.000136, Train Acc: 1.0000, Val Loss: 0.000224, Val Acc: 0.9999\n",
      "Epoch [47/50], Train Loss: 0.000136, Train Acc: 1.0000, Val Loss: 0.000217, Val Acc: 0.9999\n",
      "Epoch [48/50], Train Loss: 0.000130, Train Acc: 1.0000, Val Loss: 0.000217, Val Acc: 0.9999\n",
      "Epoch [49/50], Train Loss: 0.000124, Train Acc: 1.0000, Val Loss: 0.000208, Val Acc: 0.9999\n",
      "Epoch [50/50], Train Loss: 0.000119, Train Acc: 1.0000, Val Loss: 0.000206, Val Acc: 0.9999\n",
      "Model saved to ffnn_model_1024_2048_0.001.pth\n",
      "Evaluation result saved to hyperparameter/1024-2048-0.001-edge_accuracy.csv\n",
      "Epoch [1/50], Train Loss: 0.242748, Train Acc: 0.9447, Val Loss: 0.008253, Val Acc: 0.9993\n",
      "Epoch [2/50], Train Loss: 0.008290, Train Acc: 0.9993, Val Loss: 0.007395, Val Acc: 0.9993\n",
      "Epoch [3/50], Train Loss: 0.006458, Train Acc: 0.9993, Val Loss: 0.005919, Val Acc: 0.9993\n",
      "Epoch [4/50], Train Loss: 0.005474, Train Acc: 0.9993, Val Loss: 0.005324, Val Acc: 0.9993\n",
      "Epoch [5/50], Train Loss: 0.005039, Train Acc: 0.9993, Val Loss: 0.005018, Val Acc: 0.9993\n",
      "Epoch [6/50], Train Loss: 0.004805, Train Acc: 0.9993, Val Loss: 0.004837, Val Acc: 0.9993\n",
      "Epoch [7/50], Train Loss: 0.004603, Train Acc: 0.9993, Val Loss: 0.004669, Val Acc: 0.9993\n",
      "Epoch [8/50], Train Loss: 0.004442, Train Acc: 0.9993, Val Loss: 0.004451, Val Acc: 0.9993\n",
      "Epoch [9/50], Train Loss: 0.004232, Train Acc: 0.9993, Val Loss: 0.004254, Val Acc: 0.9993\n",
      "Epoch [10/50], Train Loss: 0.004021, Train Acc: 0.9993, Val Loss: 0.004005, Val Acc: 0.9993\n",
      "Epoch [11/50], Train Loss: 0.003796, Train Acc: 0.9993, Val Loss: 0.003799, Val Acc: 0.9993\n",
      "Epoch [12/50], Train Loss: 0.003581, Train Acc: 0.9993, Val Loss: 0.003589, Val Acc: 0.9993\n",
      "Epoch [13/50], Train Loss: 0.003357, Train Acc: 0.9993, Val Loss: 0.003393, Val Acc: 0.9993\n",
      "Epoch [14/50], Train Loss: 0.003174, Train Acc: 0.9993, Val Loss: 0.003199, Val Acc: 0.9993\n",
      "Epoch [15/50], Train Loss: 0.002986, Train Acc: 0.9993, Val Loss: 0.003043, Val Acc: 0.9993\n",
      "Epoch [16/50], Train Loss: 0.002823, Train Acc: 0.9993, Val Loss: 0.002890, Val Acc: 0.9993\n",
      "Epoch [17/50], Train Loss: 0.002680, Train Acc: 0.9994, Val Loss: 0.002731, Val Acc: 0.9994\n",
      "Epoch [18/50], Train Loss: 0.002533, Train Acc: 0.9994, Val Loss: 0.002588, Val Acc: 0.9994\n",
      "Epoch [19/50], Train Loss: 0.002402, Train Acc: 0.9994, Val Loss: 0.002459, Val Acc: 0.9994\n",
      "Epoch [20/50], Train Loss: 0.002263, Train Acc: 0.9994, Val Loss: 0.002327, Val Acc: 0.9994\n",
      "Epoch [21/50], Train Loss: 0.002134, Train Acc: 0.9994, Val Loss: 0.002203, Val Acc: 0.9994\n",
      "Epoch [22/50], Train Loss: 0.002020, Train Acc: 0.9994, Val Loss: 0.002072, Val Acc: 0.9994\n",
      "Epoch [23/50], Train Loss: 0.001891, Train Acc: 0.9995, Val Loss: 0.001962, Val Acc: 0.9995\n",
      "Epoch [24/50], Train Loss: 0.001772, Train Acc: 0.9995, Val Loss: 0.001847, Val Acc: 0.9995\n",
      "Epoch [25/50], Train Loss: 0.001668, Train Acc: 0.9995, Val Loss: 0.001732, Val Acc: 0.9995\n",
      "Epoch [26/50], Train Loss: 0.001571, Train Acc: 0.9995, Val Loss: 0.001639, Val Acc: 0.9995\n",
      "Epoch [27/50], Train Loss: 0.001468, Train Acc: 0.9995, Val Loss: 0.001541, Val Acc: 0.9995\n",
      "Epoch [28/50], Train Loss: 0.001372, Train Acc: 0.9996, Val Loss: 0.001453, Val Acc: 0.9996\n",
      "Epoch [29/50], Train Loss: 0.001281, Train Acc: 0.9996, Val Loss: 0.001362, Val Acc: 0.9996\n",
      "Epoch [30/50], Train Loss: 0.001194, Train Acc: 0.9996, Val Loss: 0.001282, Val Acc: 0.9996\n",
      "Epoch [31/50], Train Loss: 0.001117, Train Acc: 0.9996, Val Loss: 0.001217, Val Acc: 0.9996\n",
      "Epoch [32/50], Train Loss: 0.001045, Train Acc: 0.9997, Val Loss: 0.001128, Val Acc: 0.9996\n",
      "Epoch [33/50], Train Loss: 0.000975, Train Acc: 0.9997, Val Loss: 0.001062, Val Acc: 0.9997\n",
      "Epoch [34/50], Train Loss: 0.000911, Train Acc: 0.9997, Val Loss: 0.001000, Val Acc: 0.9997\n",
      "Epoch [35/50], Train Loss: 0.000850, Train Acc: 0.9997, Val Loss: 0.000939, Val Acc: 0.9997\n",
      "Epoch [36/50], Train Loss: 0.000795, Train Acc: 0.9997, Val Loss: 0.000889, Val Acc: 0.9997\n",
      "Epoch [37/50], Train Loss: 0.000746, Train Acc: 0.9998, Val Loss: 0.000833, Val Acc: 0.9997\n",
      "Epoch [38/50], Train Loss: 0.000694, Train Acc: 0.9998, Val Loss: 0.000788, Val Acc: 0.9998\n",
      "Epoch [39/50], Train Loss: 0.000649, Train Acc: 0.9998, Val Loss: 0.000747, Val Acc: 0.9998\n",
      "Epoch [40/50], Train Loss: 0.000610, Train Acc: 0.9998, Val Loss: 0.000705, Val Acc: 0.9998\n",
      "Epoch [41/50], Train Loss: 0.000576, Train Acc: 0.9998, Val Loss: 0.000669, Val Acc: 0.9998\n",
      "Epoch [42/50], Train Loss: 0.000541, Train Acc: 0.9998, Val Loss: 0.000632, Val Acc: 0.9998\n",
      "Epoch [43/50], Train Loss: 0.000508, Train Acc: 0.9998, Val Loss: 0.000602, Val Acc: 0.9998\n",
      "Epoch [44/50], Train Loss: 0.000482, Train Acc: 0.9999, Val Loss: 0.000576, Val Acc: 0.9998\n",
      "Epoch [45/50], Train Loss: 0.000454, Train Acc: 0.9999, Val Loss: 0.000547, Val Acc: 0.9998\n",
      "Epoch [46/50], Train Loss: 0.000431, Train Acc: 0.9999, Val Loss: 0.000520, Val Acc: 0.9998\n",
      "Epoch [47/50], Train Loss: 0.000407, Train Acc: 0.9999, Val Loss: 0.000498, Val Acc: 0.9999\n",
      "Epoch [48/50], Train Loss: 0.000386, Train Acc: 0.9999, Val Loss: 0.000480, Val Acc: 0.9999\n",
      "Epoch [49/50], Train Loss: 0.000367, Train Acc: 0.9999, Val Loss: 0.000458, Val Acc: 0.9999\n",
      "Epoch [50/50], Train Loss: 0.000347, Train Acc: 0.9999, Val Loss: 0.000446, Val Acc: 0.9999\n",
      "Model saved to ffnn_model_1024_2048_0.0005.pth\n",
      "Evaluation result saved to hyperparameter/1024-2048-0.0005-edge_accuracy.csv\n",
      "Epoch [1/50], Train Loss: 0.073790, Train Acc: 0.9950, Val Loss: 0.068851, Val Acc: 0.9993\n",
      "Epoch [2/50], Train Loss: 0.068768, Train Acc: 0.9993, Val Loss: 0.069383, Val Acc: 0.9993\n",
      "Epoch [3/50], Train Loss: 0.068793, Train Acc: 0.9993, Val Loss: 0.069861, Val Acc: 0.9993\n",
      "Epoch [4/50], Train Loss: 0.068838, Train Acc: 0.9993, Val Loss: 0.069596, Val Acc: 0.9993\n",
      "Epoch [5/50], Train Loss: 0.068793, Train Acc: 0.9993, Val Loss: 0.069542, Val Acc: 0.9993\n",
      "Epoch [6/50], Train Loss: 0.068753, Train Acc: 0.9993, Val Loss: 0.068639, Val Acc: 0.9993\n",
      "Epoch [7/50], Train Loss: 0.068768, Train Acc: 0.9993, Val Loss: 0.069064, Val Acc: 0.9993\n",
      "Epoch [8/50], Train Loss: 0.068845, Train Acc: 0.9993, Val Loss: 0.069383, Val Acc: 0.9993\n",
      "Epoch [9/50], Train Loss: 0.068782, Train Acc: 0.9993, Val Loss: 0.069064, Val Acc: 0.9993\n",
      "Epoch [10/50], Train Loss: 0.068790, Train Acc: 0.9993, Val Loss: 0.069011, Val Acc: 0.9993\n",
      "Epoch [11/50], Train Loss: 0.068853, Train Acc: 0.9993, Val Loss: 0.069170, Val Acc: 0.9993\n",
      "Epoch [12/50], Train Loss: 0.068797, Train Acc: 0.9993, Val Loss: 0.068851, Val Acc: 0.9993\n",
      "Epoch [13/50], Train Loss: 0.068771, Train Acc: 0.9993, Val Loss: 0.069277, Val Acc: 0.9993\n",
      "Epoch [14/50], Train Loss: 0.068782, Train Acc: 0.9993, Val Loss: 0.069117, Val Acc: 0.9993\n",
      "Epoch [15/50], Train Loss: 0.068823, Train Acc: 0.9993, Val Loss: 0.069011, Val Acc: 0.9993\n",
      "Epoch [16/50], Train Loss: 0.068845, Train Acc: 0.9993, Val Loss: 0.068586, Val Acc: 0.9993\n",
      "Epoch [17/50], Train Loss: 0.068864, Train Acc: 0.9993, Val Loss: 0.069383, Val Acc: 0.9993\n",
      "Epoch [18/50], Train Loss: 0.068779, Train Acc: 0.9993, Val Loss: 0.068692, Val Acc: 0.9993\n",
      "Epoch [19/50], Train Loss: 0.068771, Train Acc: 0.9993, Val Loss: 0.069330, Val Acc: 0.9993\n",
      "Epoch [20/50], Train Loss: 0.068786, Train Acc: 0.9993, Val Loss: 0.068639, Val Acc: 0.9993\n",
      "Epoch [21/50], Train Loss: 0.068819, Train Acc: 0.9993, Val Loss: 0.069489, Val Acc: 0.9993\n",
      "Epoch [22/50], Train Loss: 0.068779, Train Acc: 0.9993, Val Loss: 0.069170, Val Acc: 0.9993\n",
      "Epoch [23/50], Train Loss: 0.068757, Train Acc: 0.9993, Val Loss: 0.069170, Val Acc: 0.9993\n",
      "Epoch [24/50], Train Loss: 0.068816, Train Acc: 0.9993, Val Loss: 0.068851, Val Acc: 0.9993\n",
      "Epoch [25/50], Train Loss: 0.068768, Train Acc: 0.9993, Val Loss: 0.069330, Val Acc: 0.9993\n",
      "Epoch [26/50], Train Loss: 0.068768, Train Acc: 0.9993, Val Loss: 0.068905, Val Acc: 0.9993\n",
      "Epoch [27/50], Train Loss: 0.068842, Train Acc: 0.9993, Val Loss: 0.069383, Val Acc: 0.9993\n",
      "Epoch [28/50], Train Loss: 0.068845, Train Acc: 0.9993, Val Loss: 0.069755, Val Acc: 0.9993\n",
      "Epoch [29/50], Train Loss: 0.068757, Train Acc: 0.9993, Val Loss: 0.069011, Val Acc: 0.9993\n",
      "Epoch [30/50], Train Loss: 0.068830, Train Acc: 0.9993, Val Loss: 0.069117, Val Acc: 0.9993\n",
      "Epoch [31/50], Train Loss: 0.068812, Train Acc: 0.9993, Val Loss: 0.068851, Val Acc: 0.9993\n",
      "Epoch [32/50], Train Loss: 0.068808, Train Acc: 0.9993, Val Loss: 0.068851, Val Acc: 0.9993\n",
      "Epoch [33/50], Train Loss: 0.068805, Train Acc: 0.9993, Val Loss: 0.069277, Val Acc: 0.9993\n",
      "Epoch [34/50], Train Loss: 0.068853, Train Acc: 0.9993, Val Loss: 0.069596, Val Acc: 0.9993\n",
      "Epoch [35/50], Train Loss: 0.068775, Train Acc: 0.9993, Val Loss: 0.068639, Val Acc: 0.9993\n",
      "Epoch [36/50], Train Loss: 0.068786, Train Acc: 0.9993, Val Loss: 0.068958, Val Acc: 0.9993\n",
      "Epoch [37/50], Train Loss: 0.068912, Train Acc: 0.9993, Val Loss: 0.068798, Val Acc: 0.9993\n",
      "Epoch [38/50], Train Loss: 0.068845, Train Acc: 0.9993, Val Loss: 0.069117, Val Acc: 0.9993\n",
      "Epoch [39/50], Train Loss: 0.068782, Train Acc: 0.9993, Val Loss: 0.069596, Val Acc: 0.9993\n",
      "Epoch [40/50], Train Loss: 0.068845, Train Acc: 0.9993, Val Loss: 0.068958, Val Acc: 0.9993\n",
      "Epoch [41/50], Train Loss: 0.068738, Train Acc: 0.9993, Val Loss: 0.069436, Val Acc: 0.9993\n",
      "Epoch [42/50], Train Loss: 0.068782, Train Acc: 0.9993, Val Loss: 0.068958, Val Acc: 0.9993\n",
      "Epoch [43/50], Train Loss: 0.068901, Train Acc: 0.9993, Val Loss: 0.069383, Val Acc: 0.9993\n",
      "Epoch [44/50], Train Loss: 0.068901, Train Acc: 0.9993, Val Loss: 0.068958, Val Acc: 0.9993\n",
      "Epoch [45/50], Train Loss: 0.068805, Train Acc: 0.9993, Val Loss: 0.069117, Val Acc: 0.9993\n",
      "Epoch [46/50], Train Loss: 0.068853, Train Acc: 0.9993, Val Loss: 0.069649, Val Acc: 0.9993\n",
      "Epoch [47/50], Train Loss: 0.068805, Train Acc: 0.9993, Val Loss: 0.068958, Val Acc: 0.9993\n",
      "Epoch [48/50], Train Loss: 0.068793, Train Acc: 0.9993, Val Loss: 0.069117, Val Acc: 0.9993\n",
      "Epoch [49/50], Train Loss: 0.068823, Train Acc: 0.9993, Val Loss: 0.068851, Val Acc: 0.9993\n",
      "Epoch [50/50], Train Loss: 0.068790, Train Acc: 0.9993, Val Loss: 0.068798, Val Acc: 0.9993\n",
      "Model saved to ffnn_model_4542_512_0.01.pth\n",
      "Evaluation result saved to hyperparameter/4542-512-0.01-edge_accuracy.csv\n",
      "Epoch [1/50], Train Loss: 0.041405, Train Acc: 0.9949, Val Loss: 0.030144, Val Acc: 0.9993\n",
      "Epoch [2/50], Train Loss: 0.028742, Train Acc: 0.9994, Val Loss: 0.027168, Val Acc: 0.9994\n",
      "Epoch [3/50], Train Loss: 0.026461, Train Acc: 0.9995, Val Loss: 0.024235, Val Acc: 0.9995\n",
      "Epoch [4/50], Train Loss: 0.022291, Train Acc: 0.9995, Val Loss: 0.017526, Val Acc: 0.9995\n",
      "Epoch [5/50], Train Loss: 0.012953, Train Acc: 0.9996, Val Loss: 0.007477, Val Acc: 0.9996\n",
      "Epoch [6/50], Train Loss: 0.004940, Train Acc: 0.9996, Val Loss: 0.002431, Val Acc: 0.9997\n",
      "Epoch [7/50], Train Loss: 0.001410, Train Acc: 0.9997, Val Loss: 0.000877, Val Acc: 0.9998\n",
      "Epoch [8/50], Train Loss: 0.000575, Train Acc: 0.9999, Val Loss: 0.000552, Val Acc: 0.9999\n",
      "Epoch [9/50], Train Loss: 0.000390, Train Acc: 0.9999, Val Loss: 0.000436, Val Acc: 0.9999\n",
      "Epoch [10/50], Train Loss: 0.000318, Train Acc: 0.9999, Val Loss: 0.000367, Val Acc: 0.9999\n",
      "Epoch [11/50], Train Loss: 0.000283, Train Acc: 1.0000, Val Loss: 0.000340, Val Acc: 0.9999\n",
      "Epoch [12/50], Train Loss: 0.000257, Train Acc: 1.0000, Val Loss: 0.000320, Val Acc: 0.9999\n",
      "Epoch [13/50], Train Loss: 0.000245, Train Acc: 1.0000, Val Loss: 0.000305, Val Acc: 0.9999\n",
      "Epoch [14/50], Train Loss: 0.000233, Train Acc: 1.0000, Val Loss: 0.000307, Val Acc: 0.9999\n",
      "Epoch [15/50], Train Loss: 0.000224, Train Acc: 1.0000, Val Loss: 0.000305, Val Acc: 0.9999\n",
      "Epoch [16/50], Train Loss: 0.000215, Train Acc: 1.0000, Val Loss: 0.000288, Val Acc: 0.9999\n",
      "Epoch [17/50], Train Loss: 0.000209, Train Acc: 1.0000, Val Loss: 0.000283, Val Acc: 0.9999\n",
      "Epoch [18/50], Train Loss: 0.000210, Train Acc: 1.0000, Val Loss: 0.000279, Val Acc: 0.9999\n",
      "Epoch [19/50], Train Loss: 0.000204, Train Acc: 1.0000, Val Loss: 0.000285, Val Acc: 0.9999\n",
      "Epoch [20/50], Train Loss: 0.000204, Train Acc: 1.0000, Val Loss: 0.000278, Val Acc: 0.9999\n",
      "Epoch [21/50], Train Loss: 0.000192, Train Acc: 1.0000, Val Loss: 0.000300, Val Acc: 0.9999\n",
      "Epoch [22/50], Train Loss: 0.000183, Train Acc: 1.0000, Val Loss: 0.000228, Val Acc: 0.9999\n",
      "Epoch [23/50], Train Loss: 0.000081, Train Acc: 1.0000, Val Loss: 0.000178, Val Acc: 0.9999\n",
      "Epoch [24/50], Train Loss: 0.000074, Train Acc: 1.0000, Val Loss: 0.000158, Val Acc: 0.9999\n",
      "Epoch [25/50], Train Loss: 0.000073, Train Acc: 1.0000, Val Loss: 0.000158, Val Acc: 1.0000\n",
      "Epoch [26/50], Train Loss: 0.000073, Train Acc: 1.0000, Val Loss: 0.000164, Val Acc: 1.0000\n",
      "Epoch [27/50], Train Loss: 0.000072, Train Acc: 1.0000, Val Loss: 0.000169, Val Acc: 0.9999\n",
      "Epoch [28/50], Train Loss: 0.000072, Train Acc: 1.0000, Val Loss: 0.000168, Val Acc: 0.9999\n",
      "Epoch [29/50], Train Loss: 0.000071, Train Acc: 1.0000, Val Loss: 0.000167, Val Acc: 0.9999\n",
      "Epoch [30/50], Train Loss: 0.000073, Train Acc: 1.0000, Val Loss: 0.000150, Val Acc: 1.0000\n",
      "Epoch [31/50], Train Loss: 0.000068, Train Acc: 1.0000, Val Loss: 0.000158, Val Acc: 1.0000\n",
      "Epoch [32/50], Train Loss: 0.000071, Train Acc: 1.0000, Val Loss: 0.000154, Val Acc: 1.0000\n",
      "Epoch [33/50], Train Loss: 0.000069, Train Acc: 1.0000, Val Loss: 0.000157, Val Acc: 1.0000\n",
      "Epoch [34/50], Train Loss: 0.000066, Train Acc: 1.0000, Val Loss: 0.000159, Val Acc: 1.0000\n",
      "Epoch [35/50], Train Loss: 0.000066, Train Acc: 1.0000, Val Loss: 0.000145, Val Acc: 1.0000\n",
      "Epoch [36/50], Train Loss: 0.000066, Train Acc: 1.0000, Val Loss: 0.000154, Val Acc: 1.0000\n",
      "Epoch [37/50], Train Loss: 0.000065, Train Acc: 1.0000, Val Loss: 0.000154, Val Acc: 1.0000\n",
      "Epoch [38/50], Train Loss: 0.000063, Train Acc: 1.0000, Val Loss: 0.000139, Val Acc: 1.0000\n",
      "Epoch [39/50], Train Loss: 0.000062, Train Acc: 1.0000, Val Loss: 0.000142, Val Acc: 1.0000\n",
      "Epoch [40/50], Train Loss: 0.000062, Train Acc: 1.0000, Val Loss: 0.000142, Val Acc: 1.0000\n",
      "Epoch [41/50], Train Loss: 0.000061, Train Acc: 1.0000, Val Loss: 0.000142, Val Acc: 1.0000\n",
      "Epoch [42/50], Train Loss: 0.000061, Train Acc: 1.0000, Val Loss: 0.000134, Val Acc: 1.0000\n",
      "Epoch [43/50], Train Loss: 0.000062, Train Acc: 1.0000, Val Loss: 0.000153, Val Acc: 1.0000\n",
      "Epoch [44/50], Train Loss: 0.000061, Train Acc: 1.0000, Val Loss: 0.000144, Val Acc: 1.0000\n",
      "Epoch [45/50], Train Loss: 0.000060, Train Acc: 1.0000, Val Loss: 0.000146, Val Acc: 1.0000\n",
      "Epoch [46/50], Train Loss: 0.000059, Train Acc: 1.0000, Val Loss: 0.000137, Val Acc: 1.0000\n",
      "Epoch [47/50], Train Loss: 0.000058, Train Acc: 1.0000, Val Loss: 0.000144, Val Acc: 1.0000\n",
      "Epoch [48/50], Train Loss: 0.000058, Train Acc: 1.0000, Val Loss: 0.000143, Val Acc: 1.0000\n",
      "Epoch [49/50], Train Loss: 0.000057, Train Acc: 1.0000, Val Loss: 0.000141, Val Acc: 1.0000\n",
      "Epoch [50/50], Train Loss: 0.000057, Train Acc: 1.0000, Val Loss: 0.000136, Val Acc: 1.0000\n",
      "Model saved to ffnn_model_4542_512_0.001.pth\n",
      "Evaluation result saved to hyperparameter/4542-512-0.001-edge_accuracy.csv\n",
      "Epoch [1/50], Train Loss: 0.030263, Train Acc: 0.9944, Val Loss: 0.005489, Val Acc: 0.9993\n",
      "Epoch [2/50], Train Loss: 0.003797, Train Acc: 0.9993, Val Loss: 0.003057, Val Acc: 0.9993\n",
      "Epoch [3/50], Train Loss: 0.002505, Train Acc: 0.9994, Val Loss: 0.002260, Val Acc: 0.9994\n",
      "Epoch [4/50], Train Loss: 0.001778, Train Acc: 0.9995, Val Loss: 0.001591, Val Acc: 0.9995\n",
      "Epoch [5/50], Train Loss: 0.001224, Train Acc: 0.9996, Val Loss: 0.001109, Val Acc: 0.9997\n",
      "Epoch [6/50], Train Loss: 0.000831, Train Acc: 0.9997, Val Loss: 0.000805, Val Acc: 0.9997\n",
      "Epoch [7/50], Train Loss: 0.000577, Train Acc: 0.9998, Val Loss: 0.000601, Val Acc: 0.9998\n",
      "Epoch [8/50], Train Loss: 0.000419, Train Acc: 0.9999, Val Loss: 0.000466, Val Acc: 0.9999\n",
      "Epoch [9/50], Train Loss: 0.000318, Train Acc: 0.9999, Val Loss: 0.000387, Val Acc: 0.9999\n",
      "Epoch [10/50], Train Loss: 0.000256, Train Acc: 0.9999, Val Loss: 0.000326, Val Acc: 0.9999\n",
      "Epoch [11/50], Train Loss: 0.000208, Train Acc: 0.9999, Val Loss: 0.000284, Val Acc: 0.9999\n",
      "Epoch [12/50], Train Loss: 0.000177, Train Acc: 0.9999, Val Loss: 0.000264, Val Acc: 0.9999\n",
      "Epoch [13/50], Train Loss: 0.000157, Train Acc: 1.0000, Val Loss: 0.000236, Val Acc: 0.9999\n",
      "Epoch [14/50], Train Loss: 0.000140, Train Acc: 1.0000, Val Loss: 0.000224, Val Acc: 0.9999\n",
      "Epoch [15/50], Train Loss: 0.000130, Train Acc: 1.0000, Val Loss: 0.000220, Val Acc: 0.9999\n",
      "Epoch [16/50], Train Loss: 0.000119, Train Acc: 1.0000, Val Loss: 0.000205, Val Acc: 0.9999\n",
      "Epoch [17/50], Train Loss: 0.000112, Train Acc: 1.0000, Val Loss: 0.000193, Val Acc: 0.9999\n",
      "Epoch [18/50], Train Loss: 0.000106, Train Acc: 1.0000, Val Loss: 0.000191, Val Acc: 0.9999\n",
      "Epoch [19/50], Train Loss: 0.000100, Train Acc: 1.0000, Val Loss: 0.000184, Val Acc: 0.9999\n",
      "Epoch [20/50], Train Loss: 0.000098, Train Acc: 1.0000, Val Loss: 0.000178, Val Acc: 0.9999\n",
      "Epoch [21/50], Train Loss: 0.000095, Train Acc: 1.0000, Val Loss: 0.000172, Val Acc: 0.9999\n",
      "Epoch [22/50], Train Loss: 0.000091, Train Acc: 1.0000, Val Loss: 0.000186, Val Acc: 0.9999\n",
      "Epoch [23/50], Train Loss: 0.000084, Train Acc: 1.0000, Val Loss: 0.000168, Val Acc: 0.9999\n",
      "Epoch [24/50], Train Loss: 0.000085, Train Acc: 1.0000, Val Loss: 0.000167, Val Acc: 0.9999\n",
      "Epoch [25/50], Train Loss: 0.000082, Train Acc: 1.0000, Val Loss: 0.000169, Val Acc: 0.9999\n",
      "Epoch [26/50], Train Loss: 0.000082, Train Acc: 1.0000, Val Loss: 0.000161, Val Acc: 1.0000\n",
      "Epoch [27/50], Train Loss: 0.000080, Train Acc: 1.0000, Val Loss: 0.000162, Val Acc: 0.9999\n",
      "Epoch [28/50], Train Loss: 0.000079, Train Acc: 1.0000, Val Loss: 0.000164, Val Acc: 0.9999\n",
      "Epoch [29/50], Train Loss: 0.000079, Train Acc: 1.0000, Val Loss: 0.000164, Val Acc: 0.9999\n",
      "Epoch [30/50], Train Loss: 0.000076, Train Acc: 1.0000, Val Loss: 0.000162, Val Acc: 0.9999\n",
      "Epoch [31/50], Train Loss: 0.000074, Train Acc: 1.0000, Val Loss: 0.000153, Val Acc: 1.0000\n",
      "Epoch [32/50], Train Loss: 0.000075, Train Acc: 1.0000, Val Loss: 0.000154, Val Acc: 1.0000\n",
      "Epoch [33/50], Train Loss: 0.000075, Train Acc: 1.0000, Val Loss: 0.000148, Val Acc: 1.0000\n",
      "Epoch [34/50], Train Loss: 0.000071, Train Acc: 1.0000, Val Loss: 0.000146, Val Acc: 1.0000\n",
      "Epoch [35/50], Train Loss: 0.000070, Train Acc: 1.0000, Val Loss: 0.000150, Val Acc: 1.0000\n",
      "Epoch [36/50], Train Loss: 0.000071, Train Acc: 1.0000, Val Loss: 0.000158, Val Acc: 1.0000\n",
      "Epoch [37/50], Train Loss: 0.000068, Train Acc: 1.0000, Val Loss: 0.000148, Val Acc: 1.0000\n",
      "Epoch [38/50], Train Loss: 0.000070, Train Acc: 1.0000, Val Loss: 0.000155, Val Acc: 1.0000\n",
      "Epoch [39/50], Train Loss: 0.000068, Train Acc: 1.0000, Val Loss: 0.000140, Val Acc: 1.0000\n",
      "Epoch [40/50], Train Loss: 0.000067, Train Acc: 1.0000, Val Loss: 0.000145, Val Acc: 1.0000\n",
      "Epoch [41/50], Train Loss: 0.000067, Train Acc: 1.0000, Val Loss: 0.000132, Val Acc: 1.0000\n",
      "Epoch [42/50], Train Loss: 0.000066, Train Acc: 1.0000, Val Loss: 0.000137, Val Acc: 1.0000\n",
      "Epoch [43/50], Train Loss: 0.000065, Train Acc: 1.0000, Val Loss: 0.000140, Val Acc: 1.0000\n",
      "Epoch [44/50], Train Loss: 0.000064, Train Acc: 1.0000, Val Loss: 0.000140, Val Acc: 1.0000\n",
      "Epoch [45/50], Train Loss: 0.000064, Train Acc: 1.0000, Val Loss: 0.000130, Val Acc: 1.0000\n",
      "Epoch [46/50], Train Loss: 0.000063, Train Acc: 1.0000, Val Loss: 0.000137, Val Acc: 1.0000\n",
      "Epoch [47/50], Train Loss: 0.000066, Train Acc: 1.0000, Val Loss: 0.000134, Val Acc: 1.0000\n",
      "Epoch [48/50], Train Loss: 0.000065, Train Acc: 1.0000, Val Loss: 0.000132, Val Acc: 1.0000\n",
      "Epoch [49/50], Train Loss: 0.000062, Train Acc: 1.0000, Val Loss: 0.000131, Val Acc: 1.0000\n",
      "Epoch [50/50], Train Loss: 0.000062, Train Acc: 1.0000, Val Loss: 0.000133, Val Acc: 1.0000\n",
      "Model saved to ffnn_model_4542_512_0.0005.pth\n",
      "Evaluation result saved to hyperparameter/4542-512-0.0005-edge_accuracy.csv\n",
      "Epoch [1/50], Train Loss: 0.078914, Train Acc: 0.9907, Val Loss: 0.069005, Val Acc: 0.9993\n",
      "Epoch [2/50], Train Loss: 0.068672, Train Acc: 0.9993, Val Loss: 0.069020, Val Acc: 0.9993\n",
      "Epoch [3/50], Train Loss: 0.068857, Train Acc: 0.9993, Val Loss: 0.069025, Val Acc: 0.9993\n",
      "Epoch [4/50], Train Loss: 0.068922, Train Acc: 0.9993, Val Loss: 0.069140, Val Acc: 0.9993\n",
      "Epoch [5/50], Train Loss: 0.068906, Train Acc: 0.9993, Val Loss: 0.069022, Val Acc: 0.9993\n",
      "Epoch [6/50], Train Loss: 0.068954, Train Acc: 0.9993, Val Loss: 0.069005, Val Acc: 0.9993\n",
      "Epoch [7/50], Train Loss: 0.068704, Train Acc: 0.9993, Val Loss: 0.069205, Val Acc: 0.9993\n",
      "Epoch [8/50], Train Loss: 0.068761, Train Acc: 0.9993, Val Loss: 0.069193, Val Acc: 0.9993\n",
      "Epoch [9/50], Train Loss: 0.068833, Train Acc: 0.9993, Val Loss: 0.069034, Val Acc: 0.9993\n",
      "Epoch [10/50], Train Loss: 0.068769, Train Acc: 0.9993, Val Loss: 0.068990, Val Acc: 0.9993\n",
      "Epoch [11/50], Train Loss: 0.068793, Train Acc: 0.9993, Val Loss: 0.069087, Val Acc: 0.9993\n",
      "Epoch [12/50], Train Loss: 0.068865, Train Acc: 0.9993, Val Loss: 0.069081, Val Acc: 0.9993\n",
      "Epoch [13/50], Train Loss: 0.068720, Train Acc: 0.9993, Val Loss: 0.069131, Val Acc: 0.9993\n",
      "Epoch [14/50], Train Loss: 0.068761, Train Acc: 0.9993, Val Loss: 0.069072, Val Acc: 0.9993\n",
      "Epoch [15/50], Train Loss: 0.068986, Train Acc: 0.9993, Val Loss: 0.069075, Val Acc: 0.9993\n",
      "Epoch [16/50], Train Loss: 0.068825, Train Acc: 0.9993, Val Loss: 0.068931, Val Acc: 0.9993\n",
      "Epoch [17/50], Train Loss: 0.068841, Train Acc: 0.9993, Val Loss: 0.069122, Val Acc: 0.9993\n",
      "Epoch [18/50], Train Loss: 0.068882, Train Acc: 0.9993, Val Loss: 0.069075, Val Acc: 0.9993\n",
      "Epoch [19/50], Train Loss: 0.068737, Train Acc: 0.9993, Val Loss: 0.069017, Val Acc: 0.9993\n",
      "Epoch [20/50], Train Loss: 0.068841, Train Acc: 0.9993, Val Loss: 0.069020, Val Acc: 0.9993\n",
      "Epoch [21/50], Train Loss: 0.069010, Train Acc: 0.9993, Val Loss: 0.069072, Val Acc: 0.9993\n",
      "Epoch [22/50], Train Loss: 0.068809, Train Acc: 0.9993, Val Loss: 0.069046, Val Acc: 0.9993\n",
      "Epoch [23/50], Train Loss: 0.068849, Train Acc: 0.9993, Val Loss: 0.069028, Val Acc: 0.9993\n",
      "Epoch [24/50], Train Loss: 0.068680, Train Acc: 0.9993, Val Loss: 0.069049, Val Acc: 0.9993\n",
      "Epoch [25/50], Train Loss: 0.068785, Train Acc: 0.9993, Val Loss: 0.069087, Val Acc: 0.9993\n",
      "Epoch [26/50], Train Loss: 0.068906, Train Acc: 0.9993, Val Loss: 0.069049, Val Acc: 0.9993\n",
      "Epoch [27/50], Train Loss: 0.068712, Train Acc: 0.9993, Val Loss: 0.069170, Val Acc: 0.9993\n",
      "Epoch [28/50], Train Loss: 0.068753, Train Acc: 0.9993, Val Loss: 0.069025, Val Acc: 0.9993\n",
      "Epoch [29/50], Train Loss: 0.068793, Train Acc: 0.9993, Val Loss: 0.069105, Val Acc: 0.9993\n",
      "Epoch [30/50], Train Loss: 0.068720, Train Acc: 0.9993, Val Loss: 0.069149, Val Acc: 0.9993\n",
      "Epoch [31/50], Train Loss: 0.068922, Train Acc: 0.9993, Val Loss: 0.069081, Val Acc: 0.9993\n",
      "Epoch [32/50], Train Loss: 0.068777, Train Acc: 0.9993, Val Loss: 0.069084, Val Acc: 0.9993\n",
      "Epoch [33/50], Train Loss: 0.068873, Train Acc: 0.9993, Val Loss: 0.069093, Val Acc: 0.9993\n",
      "Epoch [34/50], Train Loss: 0.068865, Train Acc: 0.9993, Val Loss: 0.069099, Val Acc: 0.9993\n",
      "Epoch [35/50], Train Loss: 0.068809, Train Acc: 0.9993, Val Loss: 0.069058, Val Acc: 0.9993\n",
      "Epoch [36/50], Train Loss: 0.068906, Train Acc: 0.9993, Val Loss: 0.069008, Val Acc: 0.9993\n",
      "Epoch [37/50], Train Loss: 0.068857, Train Acc: 0.9993, Val Loss: 0.068993, Val Acc: 0.9993\n",
      "Epoch [38/50], Train Loss: 0.068712, Train Acc: 0.9993, Val Loss: 0.069011, Val Acc: 0.9993\n",
      "Epoch [39/50], Train Loss: 0.068890, Train Acc: 0.9993, Val Loss: 0.069070, Val Acc: 0.9993\n",
      "Epoch [40/50], Train Loss: 0.068825, Train Acc: 0.9993, Val Loss: 0.069178, Val Acc: 0.9993\n",
      "Epoch [41/50], Train Loss: 0.068761, Train Acc: 0.9993, Val Loss: 0.069078, Val Acc: 0.9993\n",
      "Epoch [42/50], Train Loss: 0.068906, Train Acc: 0.9993, Val Loss: 0.068955, Val Acc: 0.9993\n",
      "Epoch [43/50], Train Loss: 0.068873, Train Acc: 0.9993, Val Loss: 0.069043, Val Acc: 0.9993\n",
      "Epoch [44/50], Train Loss: 0.068865, Train Acc: 0.9993, Val Loss: 0.069196, Val Acc: 0.9993\n",
      "Epoch [45/50], Train Loss: 0.068970, Train Acc: 0.9993, Val Loss: 0.068949, Val Acc: 0.9993\n",
      "Epoch [46/50], Train Loss: 0.068712, Train Acc: 0.9993, Val Loss: 0.069084, Val Acc: 0.9993\n",
      "Epoch [47/50], Train Loss: 0.068809, Train Acc: 0.9993, Val Loss: 0.069081, Val Acc: 0.9993\n",
      "Epoch [48/50], Train Loss: 0.068801, Train Acc: 0.9993, Val Loss: 0.069067, Val Acc: 0.9993\n",
      "Epoch [49/50], Train Loss: 0.068817, Train Acc: 0.9993, Val Loss: 0.069014, Val Acc: 0.9993\n",
      "Epoch [50/50], Train Loss: 0.068785, Train Acc: 0.9993, Val Loss: 0.069114, Val Acc: 0.9993\n",
      "Model saved to ffnn_model_4542_1024_0.01.pth\n",
      "Evaluation result saved to hyperparameter/4542-1024-0.01-edge_accuracy.csv\n",
      "Epoch [1/50], Train Loss: 0.052723, Train Acc: 0.9905, Val Loss: 0.033451, Val Acc: 0.9993\n",
      "Epoch [2/50], Train Loss: 0.031766, Train Acc: 0.9993, Val Loss: 0.030892, Val Acc: 0.9989\n",
      "Epoch [3/50], Train Loss: 0.030435, Train Acc: 0.9993, Val Loss: 0.029766, Val Acc: 0.9994\n",
      "Epoch [4/50], Train Loss: 0.029715, Train Acc: 0.9994, Val Loss: 0.029313, Val Acc: 0.9994\n",
      "Epoch [5/50], Train Loss: 0.029660, Train Acc: 0.9995, Val Loss: 0.030974, Val Acc: 0.9990\n",
      "Epoch [6/50], Train Loss: 0.029764, Train Acc: 0.9995, Val Loss: 0.029731, Val Acc: 0.9995\n",
      "Epoch [7/50], Train Loss: 0.029759, Train Acc: 0.9995, Val Loss: 0.029398, Val Acc: 0.9995\n",
      "Epoch [8/50], Train Loss: 0.029414, Train Acc: 0.9995, Val Loss: 0.029095, Val Acc: 0.9996\n",
      "Epoch [9/50], Train Loss: 0.029239, Train Acc: 0.9996, Val Loss: 0.028839, Val Acc: 0.9996\n",
      "Epoch [10/50], Train Loss: 0.028761, Train Acc: 0.9996, Val Loss: 0.029437, Val Acc: 0.9991\n",
      "Epoch [11/50], Train Loss: 0.028294, Train Acc: 0.9996, Val Loss: 0.027783, Val Acc: 0.9996\n",
      "Epoch [12/50], Train Loss: 0.027609, Train Acc: 0.9996, Val Loss: 0.027070, Val Acc: 0.9996\n",
      "Epoch [13/50], Train Loss: 0.027036, Train Acc: 0.9996, Val Loss: 0.026519, Val Acc: 0.9996\n",
      "Epoch [14/50], Train Loss: 0.026130, Train Acc: 0.9996, Val Loss: 0.025016, Val Acc: 0.9996\n",
      "Epoch [15/50], Train Loss: 0.024178, Train Acc: 0.9996, Val Loss: 0.021776, Val Acc: 0.9996\n",
      "Epoch [16/50], Train Loss: 0.019708, Train Acc: 0.9996, Val Loss: 0.016708, Val Acc: 0.9996\n",
      "Epoch [17/50], Train Loss: 0.014608, Train Acc: 0.9996, Val Loss: 0.011352, Val Acc: 0.9996\n",
      "Epoch [18/50], Train Loss: 0.008211, Train Acc: 0.9996, Val Loss: 0.004268, Val Acc: 0.9996\n",
      "Epoch [19/50], Train Loss: 0.003023, Train Acc: 0.9997, Val Loss: 0.001585, Val Acc: 0.9997\n",
      "Epoch [20/50], Train Loss: 0.001148, Train Acc: 0.9998, Val Loss: 0.000705, Val Acc: 0.9998\n",
      "Epoch [21/50], Train Loss: 0.000458, Train Acc: 0.9999, Val Loss: 0.000456, Val Acc: 0.9999\n",
      "Epoch [22/50], Train Loss: 0.000296, Train Acc: 0.9999, Val Loss: 0.000351, Val Acc: 0.9999\n",
      "Epoch [23/50], Train Loss: 0.000216, Train Acc: 0.9999, Val Loss: 0.000297, Val Acc: 0.9999\n",
      "Epoch [24/50], Train Loss: 0.000169, Train Acc: 0.9999, Val Loss: 0.000258, Val Acc: 0.9999\n",
      "Epoch [25/50], Train Loss: 0.000141, Train Acc: 1.0000, Val Loss: 0.000233, Val Acc: 0.9999\n",
      "Epoch [26/50], Train Loss: 0.000124, Train Acc: 1.0000, Val Loss: 0.000219, Val Acc: 0.9999\n",
      "Epoch [27/50], Train Loss: 0.000109, Train Acc: 1.0000, Val Loss: 0.000200, Val Acc: 0.9999\n",
      "Epoch [28/50], Train Loss: 0.000100, Train Acc: 1.0000, Val Loss: 0.000199, Val Acc: 0.9999\n",
      "Epoch [29/50], Train Loss: 0.000094, Train Acc: 1.0000, Val Loss: 0.000195, Val Acc: 0.9999\n",
      "Epoch [30/50], Train Loss: 0.000091, Train Acc: 1.0000, Val Loss: 0.000189, Val Acc: 0.9999\n",
      "Epoch [31/50], Train Loss: 0.000088, Train Acc: 1.0000, Val Loss: 0.000188, Val Acc: 0.9999\n",
      "Epoch [32/50], Train Loss: 0.000081, Train Acc: 1.0000, Val Loss: 0.000182, Val Acc: 0.9999\n",
      "Epoch [33/50], Train Loss: 0.000080, Train Acc: 1.0000, Val Loss: 0.000180, Val Acc: 0.9999\n",
      "Epoch [34/50], Train Loss: 0.000077, Train Acc: 1.0000, Val Loss: 0.000185, Val Acc: 0.9999\n",
      "Epoch [35/50], Train Loss: 0.000077, Train Acc: 1.0000, Val Loss: 0.000179, Val Acc: 0.9999\n",
      "Epoch [36/50], Train Loss: 0.000072, Train Acc: 1.0000, Val Loss: 0.000167, Val Acc: 0.9999\n",
      "Epoch [37/50], Train Loss: 0.000071, Train Acc: 1.0000, Val Loss: 0.000175, Val Acc: 0.9999\n",
      "Epoch [38/50], Train Loss: 0.000071, Train Acc: 1.0000, Val Loss: 0.000172, Val Acc: 0.9999\n",
      "Epoch [39/50], Train Loss: 0.000071, Train Acc: 1.0000, Val Loss: 0.000178, Val Acc: 0.9999\n",
      "Epoch [40/50], Train Loss: 0.000072, Train Acc: 1.0000, Val Loss: 0.000171, Val Acc: 0.9999\n",
      "Epoch [41/50], Train Loss: 0.000072, Train Acc: 1.0000, Val Loss: 0.000162, Val Acc: 0.9999\n",
      "Epoch [42/50], Train Loss: 0.000067, Train Acc: 1.0000, Val Loss: 0.000166, Val Acc: 1.0000\n",
      "Epoch [43/50], Train Loss: 0.000067, Train Acc: 1.0000, Val Loss: 0.000166, Val Acc: 0.9999\n",
      "Epoch [44/50], Train Loss: 0.000067, Train Acc: 1.0000, Val Loss: 0.000165, Val Acc: 1.0000\n",
      "Epoch [45/50], Train Loss: 0.000068, Train Acc: 1.0000, Val Loss: 0.000160, Val Acc: 1.0000\n",
      "Epoch [46/50], Train Loss: 0.000067, Train Acc: 1.0000, Val Loss: 0.000158, Val Acc: 1.0000\n",
      "Epoch [47/50], Train Loss: 0.000066, Train Acc: 1.0000, Val Loss: 0.000163, Val Acc: 1.0000\n",
      "Epoch [48/50], Train Loss: 0.000064, Train Acc: 1.0000, Val Loss: 0.000156, Val Acc: 1.0000\n",
      "Epoch [49/50], Train Loss: 0.000064, Train Acc: 1.0000, Val Loss: 0.000156, Val Acc: 1.0000\n",
      "Epoch [50/50], Train Loss: 0.000062, Train Acc: 1.0000, Val Loss: 0.000157, Val Acc: 1.0000\n",
      "Model saved to ffnn_model_4542_1024_0.001.pth\n",
      "Evaluation result saved to hyperparameter/4542-1024-0.001-edge_accuracy.csv\n",
      "Epoch [1/50], Train Loss: 0.052743, Train Acc: 0.9896, Val Loss: 0.010016, Val Acc: 0.9993\n",
      "Epoch [2/50], Train Loss: 0.007265, Train Acc: 0.9993, Val Loss: 0.005426, Val Acc: 0.9993\n",
      "Epoch [3/50], Train Loss: 0.004163, Train Acc: 0.9993, Val Loss: 0.003590, Val Acc: 0.9993\n",
      "Epoch [4/50], Train Loss: 0.003047, Train Acc: 0.9993, Val Loss: 0.002857, Val Acc: 0.9994\n",
      "Epoch [5/50], Train Loss: 0.002481, Train Acc: 0.9994, Val Loss: 0.002366, Val Acc: 0.9994\n",
      "Epoch [6/50], Train Loss: 0.002046, Train Acc: 0.9994, Val Loss: 0.001959, Val Acc: 0.9995\n",
      "Epoch [7/50], Train Loss: 0.001648, Train Acc: 0.9995, Val Loss: 0.001588, Val Acc: 0.9995\n",
      "Epoch [8/50], Train Loss: 0.001319, Train Acc: 0.9996, Val Loss: 0.001301, Val Acc: 0.9996\n",
      "Epoch [9/50], Train Loss: 0.001049, Train Acc: 0.9997, Val Loss: 0.001059, Val Acc: 0.9997\n",
      "Epoch [10/50], Train Loss: 0.000836, Train Acc: 0.9997, Val Loss: 0.000877, Val Acc: 0.9997\n",
      "Epoch [11/50], Train Loss: 0.000670, Train Acc: 0.9998, Val Loss: 0.000718, Val Acc: 0.9998\n",
      "Epoch [12/50], Train Loss: 0.000541, Train Acc: 0.9998, Val Loss: 0.000608, Val Acc: 0.9998\n",
      "Epoch [13/50], Train Loss: 0.000445, Train Acc: 0.9999, Val Loss: 0.000526, Val Acc: 0.9998\n",
      "Epoch [14/50], Train Loss: 0.000374, Train Acc: 0.9999, Val Loss: 0.000459, Val Acc: 0.9999\n",
      "Epoch [15/50], Train Loss: 0.000314, Train Acc: 0.9999, Val Loss: 0.000409, Val Acc: 0.9999\n",
      "Epoch [16/50], Train Loss: 0.000276, Train Acc: 0.9999, Val Loss: 0.000356, Val Acc: 0.9999\n",
      "Epoch [17/50], Train Loss: 0.000237, Train Acc: 0.9999, Val Loss: 0.000331, Val Acc: 0.9999\n",
      "Epoch [18/50], Train Loss: 0.000207, Train Acc: 0.9999, Val Loss: 0.000308, Val Acc: 0.9999\n",
      "Epoch [19/50], Train Loss: 0.000187, Train Acc: 0.9999, Val Loss: 0.000282, Val Acc: 0.9999\n",
      "Epoch [20/50], Train Loss: 0.000171, Train Acc: 1.0000, Val Loss: 0.000265, Val Acc: 0.9999\n",
      "Epoch [21/50], Train Loss: 0.000159, Train Acc: 1.0000, Val Loss: 0.000261, Val Acc: 0.9999\n",
      "Epoch [22/50], Train Loss: 0.000148, Train Acc: 1.0000, Val Loss: 0.000238, Val Acc: 0.9999\n",
      "Epoch [23/50], Train Loss: 0.000134, Train Acc: 1.0000, Val Loss: 0.000229, Val Acc: 0.9999\n",
      "Epoch [24/50], Train Loss: 0.000126, Train Acc: 1.0000, Val Loss: 0.000226, Val Acc: 0.9999\n",
      "Epoch [25/50], Train Loss: 0.000122, Train Acc: 1.0000, Val Loss: 0.000223, Val Acc: 0.9999\n",
      "Epoch [26/50], Train Loss: 0.000112, Train Acc: 1.0000, Val Loss: 0.000210, Val Acc: 0.9999\n",
      "Epoch [27/50], Train Loss: 0.000107, Train Acc: 1.0000, Val Loss: 0.000202, Val Acc: 0.9999\n",
      "Epoch [28/50], Train Loss: 0.000103, Train Acc: 1.0000, Val Loss: 0.000198, Val Acc: 0.9999\n",
      "Epoch [29/50], Train Loss: 0.000100, Train Acc: 1.0000, Val Loss: 0.000193, Val Acc: 0.9999\n",
      "Epoch [30/50], Train Loss: 0.000096, Train Acc: 1.0000, Val Loss: 0.000193, Val Acc: 0.9999\n",
      "Epoch [31/50], Train Loss: 0.000094, Train Acc: 1.0000, Val Loss: 0.000199, Val Acc: 0.9999\n",
      "Epoch [32/50], Train Loss: 0.000094, Train Acc: 1.0000, Val Loss: 0.000187, Val Acc: 0.9999\n",
      "Epoch [33/50], Train Loss: 0.000090, Train Acc: 1.0000, Val Loss: 0.000187, Val Acc: 0.9999\n",
      "Epoch [34/50], Train Loss: 0.000088, Train Acc: 1.0000, Val Loss: 0.000187, Val Acc: 0.9999\n",
      "Epoch [35/50], Train Loss: 0.000085, Train Acc: 1.0000, Val Loss: 0.000175, Val Acc: 0.9999\n",
      "Epoch [36/50], Train Loss: 0.000082, Train Acc: 1.0000, Val Loss: 0.000183, Val Acc: 0.9999\n",
      "Epoch [37/50], Train Loss: 0.000083, Train Acc: 1.0000, Val Loss: 0.000177, Val Acc: 0.9999\n",
      "Epoch [38/50], Train Loss: 0.000081, Train Acc: 1.0000, Val Loss: 0.000174, Val Acc: 0.9999\n",
      "Epoch [39/50], Train Loss: 0.000078, Train Acc: 1.0000, Val Loss: 0.000173, Val Acc: 0.9999\n",
      "Epoch [40/50], Train Loss: 0.000078, Train Acc: 1.0000, Val Loss: 0.000176, Val Acc: 0.9999\n",
      "Epoch [41/50], Train Loss: 0.000080, Train Acc: 1.0000, Val Loss: 0.000181, Val Acc: 0.9999\n",
      "Epoch [42/50], Train Loss: 0.000079, Train Acc: 1.0000, Val Loss: 0.000176, Val Acc: 0.9999\n",
      "Epoch [43/50], Train Loss: 0.000079, Train Acc: 1.0000, Val Loss: 0.000168, Val Acc: 0.9999\n",
      "Epoch [44/50], Train Loss: 0.000075, Train Acc: 1.0000, Val Loss: 0.000174, Val Acc: 0.9999\n",
      "Epoch [45/50], Train Loss: 0.000079, Train Acc: 1.0000, Val Loss: 0.000170, Val Acc: 0.9999\n",
      "Epoch [46/50], Train Loss: 0.000073, Train Acc: 1.0000, Val Loss: 0.000177, Val Acc: 0.9999\n",
      "Epoch [47/50], Train Loss: 0.000080, Train Acc: 1.0000, Val Loss: 0.000172, Val Acc: 0.9999\n",
      "Epoch [48/50], Train Loss: 0.000073, Train Acc: 1.0000, Val Loss: 0.000172, Val Acc: 0.9999\n",
      "Epoch [49/50], Train Loss: 0.000071, Train Acc: 1.0000, Val Loss: 0.000171, Val Acc: 0.9999\n",
      "Epoch [50/50], Train Loss: 0.000073, Train Acc: 1.0000, Val Loss: 0.000167, Val Acc: 1.0000\n",
      "Model saved to ffnn_model_4542_1024_0.0005.pth\n",
      "Evaluation result saved to hyperparameter/4542-1024-0.0005-edge_accuracy.csv\n",
      "Epoch [1/50], Train Loss: 0.088306, Train Acc: 0.9824, Val Loss: 0.069313, Val Acc: 0.9993\n",
      "Epoch [2/50], Train Loss: 0.068678, Train Acc: 0.9993, Val Loss: 0.068912, Val Acc: 0.9993\n",
      "Epoch [3/50], Train Loss: 0.068645, Train Acc: 0.9993, Val Loss: 0.069245, Val Acc: 0.9993\n",
      "Epoch [4/50], Train Loss: 0.068827, Train Acc: 0.9993, Val Loss: 0.069296, Val Acc: 0.9993\n",
      "Epoch [5/50], Train Loss: 0.069026, Train Acc: 0.9993, Val Loss: 0.069032, Val Acc: 0.9993\n",
      "Epoch [6/50], Train Loss: 0.068628, Train Acc: 0.9993, Val Loss: 0.068793, Val Acc: 0.9993\n",
      "Epoch [7/50], Train Loss: 0.068562, Train Acc: 0.9993, Val Loss: 0.069194, Val Acc: 0.9993\n",
      "Epoch [8/50], Train Loss: 0.068761, Train Acc: 0.9993, Val Loss: 0.069040, Val Acc: 0.9993\n",
      "Epoch [9/50], Train Loss: 0.068744, Train Acc: 0.9993, Val Loss: 0.069313, Val Acc: 0.9993\n",
      "Epoch [10/50], Train Loss: 0.069075, Train Acc: 0.9993, Val Loss: 0.069091, Val Acc: 0.9993\n",
      "Epoch [11/50], Train Loss: 0.068595, Train Acc: 0.9993, Val Loss: 0.069040, Val Acc: 0.9993\n",
      "Epoch [12/50], Train Loss: 0.068827, Train Acc: 0.9993, Val Loss: 0.068921, Val Acc: 0.9993\n",
      "Epoch [13/50], Train Loss: 0.068844, Train Acc: 0.9993, Val Loss: 0.069202, Val Acc: 0.9993\n",
      "Epoch [14/50], Train Loss: 0.068711, Train Acc: 0.9993, Val Loss: 0.069322, Val Acc: 0.9993\n",
      "Epoch [15/50], Train Loss: 0.068678, Train Acc: 0.9993, Val Loss: 0.068972, Val Acc: 0.9993\n",
      "Epoch [16/50], Train Loss: 0.068678, Train Acc: 0.9993, Val Loss: 0.069194, Val Acc: 0.9993\n",
      "Epoch [17/50], Train Loss: 0.068546, Train Acc: 0.9993, Val Loss: 0.069382, Val Acc: 0.9993\n",
      "Epoch [18/50], Train Loss: 0.068993, Train Acc: 0.9993, Val Loss: 0.069262, Val Acc: 0.9993\n",
      "Epoch [19/50], Train Loss: 0.068628, Train Acc: 0.9993, Val Loss: 0.069228, Val Acc: 0.9993\n",
      "Epoch [20/50], Train Loss: 0.068794, Train Acc: 0.9993, Val Loss: 0.068972, Val Acc: 0.9993\n",
      "Epoch [21/50], Train Loss: 0.068777, Train Acc: 0.9993, Val Loss: 0.068946, Val Acc: 0.9993\n",
      "Epoch [22/50], Train Loss: 0.069092, Train Acc: 0.9993, Val Loss: 0.068810, Val Acc: 0.9993\n",
      "Epoch [23/50], Train Loss: 0.068529, Train Acc: 0.9993, Val Loss: 0.069185, Val Acc: 0.9993\n",
      "Epoch [24/50], Train Loss: 0.069108, Train Acc: 0.9993, Val Loss: 0.068963, Val Acc: 0.9993\n",
      "Epoch [25/50], Train Loss: 0.068794, Train Acc: 0.9993, Val Loss: 0.069057, Val Acc: 0.9993\n",
      "Epoch [26/50], Train Loss: 0.068695, Train Acc: 0.9993, Val Loss: 0.068852, Val Acc: 0.9993\n",
      "Epoch [27/50], Train Loss: 0.068827, Train Acc: 0.9993, Val Loss: 0.068878, Val Acc: 0.9993\n",
      "Epoch [28/50], Train Loss: 0.068926, Train Acc: 0.9993, Val Loss: 0.068767, Val Acc: 0.9993\n",
      "Epoch [29/50], Train Loss: 0.068595, Train Acc: 0.9993, Val Loss: 0.069211, Val Acc: 0.9993\n",
      "Epoch [30/50], Train Loss: 0.068761, Train Acc: 0.9993, Val Loss: 0.068784, Val Acc: 0.9993\n",
      "Epoch [31/50], Train Loss: 0.068827, Train Acc: 0.9993, Val Loss: 0.068912, Val Acc: 0.9993\n",
      "Epoch [32/50], Train Loss: 0.068761, Train Acc: 0.9993, Val Loss: 0.068818, Val Acc: 0.9993\n",
      "Epoch [33/50], Train Loss: 0.068993, Train Acc: 0.9993, Val Loss: 0.069330, Val Acc: 0.9993\n",
      "Epoch [34/50], Train Loss: 0.068910, Train Acc: 0.9993, Val Loss: 0.069083, Val Acc: 0.9993\n",
      "Epoch [35/50], Train Loss: 0.068777, Train Acc: 0.9993, Val Loss: 0.069330, Val Acc: 0.9993\n",
      "Epoch [36/50], Train Loss: 0.068744, Train Acc: 0.9993, Val Loss: 0.069117, Val Acc: 0.9993\n",
      "Epoch [37/50], Train Loss: 0.068645, Train Acc: 0.9993, Val Loss: 0.068724, Val Acc: 0.9993\n",
      "Epoch [38/50], Train Loss: 0.068480, Train Acc: 0.9993, Val Loss: 0.069049, Val Acc: 0.9993\n",
      "Epoch [39/50], Train Loss: 0.069026, Train Acc: 0.9993, Val Loss: 0.069527, Val Acc: 0.9993\n",
      "Epoch [40/50], Train Loss: 0.069175, Train Acc: 0.9993, Val Loss: 0.069160, Val Acc: 0.9993\n",
      "Epoch [41/50], Train Loss: 0.068777, Train Acc: 0.9993, Val Loss: 0.068784, Val Acc: 0.9993\n",
      "Epoch [42/50], Train Loss: 0.068910, Train Acc: 0.9993, Val Loss: 0.068904, Val Acc: 0.9993\n",
      "Epoch [43/50], Train Loss: 0.068695, Train Acc: 0.9993, Val Loss: 0.069305, Val Acc: 0.9993\n",
      "Epoch [44/50], Train Loss: 0.068993, Train Acc: 0.9993, Val Loss: 0.069288, Val Acc: 0.9993\n",
      "Epoch [45/50], Train Loss: 0.068877, Train Acc: 0.9993, Val Loss: 0.068776, Val Acc: 0.9993\n",
      "Epoch [46/50], Train Loss: 0.068811, Train Acc: 0.9993, Val Loss: 0.068904, Val Acc: 0.9993\n",
      "Epoch [47/50], Train Loss: 0.069009, Train Acc: 0.9993, Val Loss: 0.069049, Val Acc: 0.9993\n",
      "Epoch [48/50], Train Loss: 0.068678, Train Acc: 0.9993, Val Loss: 0.069305, Val Acc: 0.9993\n",
      "Epoch [49/50], Train Loss: 0.068761, Train Acc: 0.9993, Val Loss: 0.069032, Val Acc: 0.9993\n",
      "Epoch [50/50], Train Loss: 0.068827, Train Acc: 0.9993, Val Loss: 0.069100, Val Acc: 0.9993\n",
      "Model saved to ffnn_model_4542_2048_0.01.pth\n",
      "Evaluation result saved to hyperparameter/4542-2048-0.01-edge_accuracy.csv\n",
      "Epoch [1/50], Train Loss: 0.070271, Train Acc: 0.9817, Val Loss: 0.035021, Val Acc: 0.9989\n",
      "Epoch [2/50], Train Loss: 0.035005, Train Acc: 0.9993, Val Loss: 0.034706, Val Acc: 0.9989\n",
      "Epoch [3/50], Train Loss: 0.033512, Train Acc: 0.9993, Val Loss: 0.032878, Val Acc: 0.9993\n",
      "Epoch [4/50], Train Loss: 0.032472, Train Acc: 0.9993, Val Loss: 0.031898, Val Acc: 0.9993\n",
      "Epoch [5/50], Train Loss: 0.031412, Train Acc: 0.9993, Val Loss: 0.030854, Val Acc: 0.9993\n",
      "Epoch [6/50], Train Loss: 0.031090, Train Acc: 0.9993, Val Loss: 0.030772, Val Acc: 0.9994\n",
      "Epoch [7/50], Train Loss: 0.031071, Train Acc: 0.9994, Val Loss: 0.031282, Val Acc: 0.9994\n",
      "Epoch [8/50], Train Loss: 0.031472, Train Acc: 0.9994, Val Loss: 0.031232, Val Acc: 0.9994\n",
      "Epoch [9/50], Train Loss: 0.031575, Train Acc: 0.9995, Val Loss: 0.031336, Val Acc: 0.9995\n",
      "Epoch [10/50], Train Loss: 0.031625, Train Acc: 0.9995, Val Loss: 0.031699, Val Acc: 0.9995\n",
      "Epoch [11/50], Train Loss: 0.031692, Train Acc: 0.9995, Val Loss: 0.031596, Val Acc: 0.9995\n",
      "Epoch [12/50], Train Loss: 0.032035, Train Acc: 0.9995, Val Loss: 0.031932, Val Acc: 0.9995\n",
      "Epoch [13/50], Train Loss: 0.032138, Train Acc: 0.9995, Val Loss: 0.031856, Val Acc: 0.9995\n",
      "Epoch [14/50], Train Loss: 0.032268, Train Acc: 0.9995, Val Loss: 0.031508, Val Acc: 0.9995\n",
      "Epoch [15/50], Train Loss: 0.031810, Train Acc: 0.9995, Val Loss: 0.031510, Val Acc: 0.9995\n",
      "Epoch [16/50], Train Loss: 0.032045, Train Acc: 0.9995, Val Loss: 0.031682, Val Acc: 0.9995\n",
      "Epoch [17/50], Train Loss: 0.032033, Train Acc: 0.9996, Val Loss: 0.031560, Val Acc: 0.9995\n",
      "Epoch [18/50], Train Loss: 0.032083, Train Acc: 0.9995, Val Loss: 0.031297, Val Acc: 0.9996\n",
      "Epoch [19/50], Train Loss: 0.031451, Train Acc: 0.9996, Val Loss: 0.031000, Val Acc: 0.9996\n",
      "Epoch [20/50], Train Loss: 0.031309, Train Acc: 0.9996, Val Loss: 0.030827, Val Acc: 0.9996\n",
      "Epoch [21/50], Train Loss: 0.030981, Train Acc: 0.9996, Val Loss: 0.030421, Val Acc: 0.9996\n",
      "Epoch [22/50], Train Loss: 0.030657, Train Acc: 0.9996, Val Loss: 0.030155, Val Acc: 0.9996\n",
      "Epoch [23/50], Train Loss: 0.030237, Train Acc: 0.9996, Val Loss: 0.029639, Val Acc: 0.9996\n",
      "Epoch [24/50], Train Loss: 0.029875, Train Acc: 0.9996, Val Loss: 0.029373, Val Acc: 0.9996\n",
      "Epoch [25/50], Train Loss: 0.028861, Train Acc: 0.9996, Val Loss: 0.028009, Val Acc: 0.9995\n",
      "Epoch [26/50], Train Loss: 0.027390, Train Acc: 0.9996, Val Loss: 0.026670, Val Acc: 0.9996\n",
      "Epoch [27/50], Train Loss: 0.026139, Train Acc: 0.9996, Val Loss: 0.025085, Val Acc: 0.9996\n",
      "Epoch [28/50], Train Loss: 0.025347, Train Acc: 0.9996, Val Loss: 0.024787, Val Acc: 0.9996\n",
      "Epoch [29/50], Train Loss: 0.024806, Train Acc: 0.9996, Val Loss: 0.024780, Val Acc: 0.9996\n",
      "Epoch [30/50], Train Loss: 0.024341, Train Acc: 0.9996, Val Loss: 0.024125, Val Acc: 0.9996\n",
      "Epoch [31/50], Train Loss: 0.023977, Train Acc: 0.9996, Val Loss: 0.023506, Val Acc: 0.9996\n",
      "Epoch [32/50], Train Loss: 0.023270, Train Acc: 0.9996, Val Loss: 0.022962, Val Acc: 0.9996\n",
      "Epoch [33/50], Train Loss: 0.022282, Train Acc: 0.9996, Val Loss: 0.021620, Val Acc: 0.9995\n",
      "Epoch [34/50], Train Loss: 0.020890, Train Acc: 0.9996, Val Loss: 0.020694, Val Acc: 0.9996\n",
      "Epoch [35/50], Train Loss: 0.019362, Train Acc: 0.9996, Val Loss: 0.018344, Val Acc: 0.9996\n",
      "Epoch [36/50], Train Loss: 0.017306, Train Acc: 0.9996, Val Loss: 0.015952, Val Acc: 0.9996\n",
      "Epoch [37/50], Train Loss: 0.015095, Train Acc: 0.9996, Val Loss: 0.014000, Val Acc: 0.9996\n",
      "Epoch [38/50], Train Loss: 0.013135, Train Acc: 0.9997, Val Loss: 0.011923, Val Acc: 0.9996\n",
      "Epoch [39/50], Train Loss: 0.011166, Train Acc: 0.9997, Val Loss: 0.009858, Val Acc: 0.9996\n",
      "Epoch [40/50], Train Loss: 0.009660, Train Acc: 0.9997, Val Loss: 0.008317, Val Acc: 0.9997\n",
      "Epoch [41/50], Train Loss: 0.008002, Train Acc: 0.9997, Val Loss: 0.006594, Val Acc: 0.9997\n",
      "Epoch [42/50], Train Loss: 0.006115, Train Acc: 0.9997, Val Loss: 0.004332, Val Acc: 0.9997\n",
      "Epoch [43/50], Train Loss: 0.003871, Train Acc: 0.9998, Val Loss: 0.002626, Val Acc: 0.9998\n",
      "Epoch [44/50], Train Loss: 0.002562, Train Acc: 0.9998, Val Loss: 0.001597, Val Acc: 0.9998\n",
      "Epoch [45/50], Train Loss: 0.001617, Train Acc: 0.9998, Val Loss: 0.001102, Val Acc: 0.9998\n",
      "Epoch [46/50], Train Loss: 0.001041, Train Acc: 0.9999, Val Loss: 0.000918, Val Acc: 0.9999\n",
      "Epoch [47/50], Train Loss: 0.000888, Train Acc: 0.9999, Val Loss: 0.000857, Val Acc: 0.9999\n",
      "Epoch [48/50], Train Loss: 0.000825, Train Acc: 0.9999, Val Loss: 0.000828, Val Acc: 0.9999\n",
      "Epoch [49/50], Train Loss: 0.000816, Train Acc: 0.9999, Val Loss: 0.000806, Val Acc: 0.9999\n",
      "Epoch [50/50], Train Loss: 0.000789, Train Acc: 0.9999, Val Loss: 0.000758, Val Acc: 0.9999\n",
      "Model saved to ffnn_model_4542_2048_0.001.pth\n",
      "Evaluation result saved to hyperparameter/4542-2048-0.001-edge_accuracy.csv\n",
      "Epoch [1/50], Train Loss: 0.092126, Train Acc: 0.9793, Val Loss: 0.014678, Val Acc: 0.9993\n",
      "Epoch [2/50], Train Loss: 0.012368, Train Acc: 0.9993, Val Loss: 0.010212, Val Acc: 0.9993\n",
      "Epoch [3/50], Train Loss: 0.008516, Train Acc: 0.9993, Val Loss: 0.007242, Val Acc: 0.9993\n",
      "Epoch [4/50], Train Loss: 0.005990, Train Acc: 0.9993, Val Loss: 0.005344, Val Acc: 0.9993\n",
      "Epoch [5/50], Train Loss: 0.004449, Train Acc: 0.9993, Val Loss: 0.004135, Val Acc: 0.9993\n",
      "Epoch [6/50], Train Loss: 0.003609, Train Acc: 0.9993, Val Loss: 0.003486, Val Acc: 0.9993\n",
      "Epoch [7/50], Train Loss: 0.003122, Train Acc: 0.9993, Val Loss: 0.003060, Val Acc: 0.9993\n",
      "Epoch [8/50], Train Loss: 0.002762, Train Acc: 0.9993, Val Loss: 0.002769, Val Acc: 0.9993\n",
      "Epoch [9/50], Train Loss: 0.002511, Train Acc: 0.9994, Val Loss: 0.002512, Val Acc: 0.9994\n",
      "Epoch [10/50], Train Loss: 0.002267, Train Acc: 0.9994, Val Loss: 0.002288, Val Acc: 0.9994\n",
      "Epoch [11/50], Train Loss: 0.002056, Train Acc: 0.9994, Val Loss: 0.002105, Val Acc: 0.9994\n",
      "Epoch [12/50], Train Loss: 0.001862, Train Acc: 0.9995, Val Loss: 0.001907, Val Acc: 0.9995\n",
      "Epoch [13/50], Train Loss: 0.001658, Train Acc: 0.9995, Val Loss: 0.001722, Val Acc: 0.9995\n",
      "Epoch [14/50], Train Loss: 0.001476, Train Acc: 0.9995, Val Loss: 0.001520, Val Acc: 0.9995\n",
      "Epoch [15/50], Train Loss: 0.001309, Train Acc: 0.9996, Val Loss: 0.001374, Val Acc: 0.9996\n",
      "Epoch [16/50], Train Loss: 0.001162, Train Acc: 0.9996, Val Loss: 0.001225, Val Acc: 0.9996\n",
      "Epoch [17/50], Train Loss: 0.001018, Train Acc: 0.9997, Val Loss: 0.001101, Val Acc: 0.9997\n",
      "Epoch [18/50], Train Loss: 0.000906, Train Acc: 0.9997, Val Loss: 0.000990, Val Acc: 0.9997\n",
      "Epoch [19/50], Train Loss: 0.000801, Train Acc: 0.9997, Val Loss: 0.000889, Val Acc: 0.9997\n",
      "Epoch [20/50], Train Loss: 0.000707, Train Acc: 0.9998, Val Loss: 0.000807, Val Acc: 0.9997\n",
      "Epoch [21/50], Train Loss: 0.000632, Train Acc: 0.9998, Val Loss: 0.000728, Val Acc: 0.9998\n",
      "Epoch [22/50], Train Loss: 0.000563, Train Acc: 0.9998, Val Loss: 0.000666, Val Acc: 0.9998\n",
      "Epoch [23/50], Train Loss: 0.000507, Train Acc: 0.9998, Val Loss: 0.000610, Val Acc: 0.9998\n",
      "Epoch [24/50], Train Loss: 0.000459, Train Acc: 0.9999, Val Loss: 0.000569, Val Acc: 0.9998\n",
      "Epoch [25/50], Train Loss: 0.000414, Train Acc: 0.9999, Val Loss: 0.000532, Val Acc: 0.9998\n",
      "Epoch [26/50], Train Loss: 0.000377, Train Acc: 0.9999, Val Loss: 0.000493, Val Acc: 0.9999\n",
      "Epoch [27/50], Train Loss: 0.000342, Train Acc: 0.9999, Val Loss: 0.000453, Val Acc: 0.9999\n",
      "Epoch [28/50], Train Loss: 0.000317, Train Acc: 0.9999, Val Loss: 0.000435, Val Acc: 0.9999\n",
      "Epoch [29/50], Train Loss: 0.000294, Train Acc: 0.9999, Val Loss: 0.000410, Val Acc: 0.9999\n",
      "Epoch [30/50], Train Loss: 0.000273, Train Acc: 0.9999, Val Loss: 0.000381, Val Acc: 0.9999\n",
      "Epoch [31/50], Train Loss: 0.000250, Train Acc: 0.9999, Val Loss: 0.000363, Val Acc: 0.9999\n",
      "Epoch [32/50], Train Loss: 0.000240, Train Acc: 0.9999, Val Loss: 0.000345, Val Acc: 0.9999\n",
      "Epoch [33/50], Train Loss: 0.000219, Train Acc: 0.9999, Val Loss: 0.000332, Val Acc: 0.9999\n",
      "Epoch [34/50], Train Loss: 0.000207, Train Acc: 0.9999, Val Loss: 0.000329, Val Acc: 0.9999\n",
      "Epoch [35/50], Train Loss: 0.000191, Train Acc: 1.0000, Val Loss: 0.000318, Val Acc: 0.9999\n",
      "Epoch [36/50], Train Loss: 0.000187, Train Acc: 1.0000, Val Loss: 0.000308, Val Acc: 0.9999\n",
      "Epoch [37/50], Train Loss: 0.000183, Train Acc: 1.0000, Val Loss: 0.000286, Val Acc: 0.9999\n",
      "Epoch [38/50], Train Loss: 0.000171, Train Acc: 1.0000, Val Loss: 0.000285, Val Acc: 0.9999\n",
      "Epoch [39/50], Train Loss: 0.000166, Train Acc: 1.0000, Val Loss: 0.000275, Val Acc: 0.9999\n",
      "Epoch [40/50], Train Loss: 0.000153, Train Acc: 1.0000, Val Loss: 0.000272, Val Acc: 0.9999\n",
      "Epoch [41/50], Train Loss: 0.000145, Train Acc: 1.0000, Val Loss: 0.000266, Val Acc: 0.9999\n",
      "Epoch [42/50], Train Loss: 0.000139, Train Acc: 1.0000, Val Loss: 0.000252, Val Acc: 0.9999\n",
      "Epoch [43/50], Train Loss: 0.000133, Train Acc: 1.0000, Val Loss: 0.000245, Val Acc: 0.9999\n",
      "Epoch [44/50], Train Loss: 0.000130, Train Acc: 1.0000, Val Loss: 0.000248, Val Acc: 0.9999\n",
      "Epoch [45/50], Train Loss: 0.000127, Train Acc: 1.0000, Val Loss: 0.000242, Val Acc: 0.9999\n",
      "Epoch [46/50], Train Loss: 0.000123, Train Acc: 1.0000, Val Loss: 0.000245, Val Acc: 0.9999\n",
      "Epoch [47/50], Train Loss: 0.000131, Train Acc: 1.0000, Val Loss: 0.000231, Val Acc: 0.9999\n",
      "Epoch [48/50], Train Loss: 0.000121, Train Acc: 1.0000, Val Loss: 0.000228, Val Acc: 0.9999\n",
      "Epoch [49/50], Train Loss: 0.000115, Train Acc: 1.0000, Val Loss: 0.000219, Val Acc: 0.9999\n",
      "Epoch [50/50], Train Loss: 0.000111, Train Acc: 1.0000, Val Loss: 0.000218, Val Acc: 0.9999\n",
      "Model saved to ffnn_model_4542_2048_0.0005.pth\n",
      "Evaluation result saved to hyperparameter/4542-2048-0.0005-edge_accuracy.csv\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "dict contains fields not in fieldnames: 'train_accuracy', 'epoch', 'train_loss', 'val_accuracy', 'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 96\u001b[0m\n\u001b[1;32m     93\u001b[0m     writer \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mDictWriter(csvfile, fieldnames\u001b[38;5;241m=\u001b[39mfieldnames)\n\u001b[1;32m     95\u001b[0m     writer\u001b[38;5;241m.\u001b[39mwriteheader()\n\u001b[0;32m---> 96\u001b[0m     \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriterows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHyperparameter tuning results saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ai-project/lib/python3.8/csv.py:157\u001b[0m, in \u001b[0;36mDictWriter.writerows\u001b[0;34m(self, rowdicts)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwriterows\u001b[39m(\u001b[38;5;28mself\u001b[39m, rowdicts):\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriterows\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dict_to_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrowdicts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ai-project/lib/python3.8/csv.py:149\u001b[0m, in \u001b[0;36mDictWriter._dict_to_list\u001b[0;34m(self, rowdict)\u001b[0m\n\u001b[1;32m    147\u001b[0m     wrong_fields \u001b[38;5;241m=\u001b[39m rowdict\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfieldnames\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wrong_fields:\n\u001b[0;32m--> 149\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict contains fields not in fieldnames: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m                          \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mrepr\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m wrong_fields]))\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (rowdict\u001b[38;5;241m.\u001b[39mget(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestval) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfieldnames)\n",
      "\u001b[0;31mValueError\u001b[0m: dict contains fields not in fieldnames: 'train_accuracy', 'epoch', 'train_loss', 'val_accuracy', 'val_loss'"
     ]
    }
   ],
   "source": [
    "from evaluation import evaluate\n",
    "\n",
    "hyper_param_tuning_done = True\n",
    "if not hyper_param_tuning_done:\n",
    "    ## Hyperparameter Tuning for the following values\n",
    "    hidden_dims = [128, 1024, 4542]\n",
    "    batch_sizes = [512, 1024, 2048]\n",
    "    learning_rates = [0.01, 0.001, 0.0005]\n",
    "\n",
    "    # Initialize the results list\n",
    "    results = []\n",
    "\n",
    "    # Iterate over all hyperparameter combinations\n",
    "    for hidden_dim in hidden_dims:\n",
    "        for batch_size in batch_sizes:\n",
    "            for learning_rate in learning_rates:\n",
    "                # Train the model\n",
    "                ffnn = FFNN(input_dim, hidden_dim, output_dim).to(device)\n",
    "                train_losses, val_losses, train_accuracies, val_accuracies = ffnn.train_ffnn(\n",
    "                    train_dataset=train_part_dataset,\n",
    "                    val_dataset=val_part_dataset,\n",
    "                    num_epochs=num_epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    learning_rate=learning_rate,\n",
    "                    device=device\n",
    "                )\n",
    "\n",
    "                # store the results as csvs\n",
    "                # Prepare results for saving\n",
    "                results = []\n",
    "                for epoch in range(len(train_losses)):\n",
    "                    results.append({\n",
    "                        \"epoch\": epoch + 1,\n",
    "                        \"train_loss\": train_losses[epoch],\n",
    "                        \"train_accuracy\": train_accuracies[epoch],\n",
    "                        \"val_loss\": val_losses[epoch],\n",
    "                        \"val_accuracy\": val_accuracies[epoch]\n",
    "                    })\n",
    "                \n",
    "\n",
    "                # Create the folder if it doesn't exist\n",
    "                output_folder = \"hyperparameter_2\"\n",
    "                os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "                # Save results to CSV\n",
    "                output_file = os.path.join(output_folder, f\"{hidden_dim}-{batch_size}-{learning_rate}-training_results.csv\")\n",
    "                with open(output_file, \"w\", newline=\"\") as csvfile:\n",
    "                    fieldnames = [\"epoch\", \"train_loss\", \"train_accuracy\", \"val_loss\", \"val_accuracy\"]\n",
    "                    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "                    writer.writeheader()\n",
    "                    writer.writerows(results)\n",
    "\n",
    "\n",
    "                # Save the trained model\n",
    "                file_path = f\"ffnn_model_{hidden_dim}_{batch_size}_{learning_rate}.pth\"\n",
    "\n",
    "                torch.save(ffnn.state_dict(), file_path)\n",
    "                print(f\"Model saved to {file_path}\")\n",
    "\n",
    "                # Initialize and load the model\n",
    "                builder = GraphBuilder()\n",
    "                builder.load_model(input_dim, hidden_dim, output_dim, file_path)\n",
    "\n",
    "                # Evaluate the model\n",
    "                edge_accuracy = evaluate(builder, validation_list)\n",
    "\n",
    "                # Save accuracy to CSV\n",
    "                output_file = os.path.join(output_folder, f\"{hidden_dim}-{batch_size}-{learning_rate}-edge_accuracy.csv\")\n",
    "                with open(output_file, \"w\", newline=\"\") as csvfile:\n",
    "                    writer = csv.writer(csvfile)\n",
    "                    writer.writerow([\"edge_accuracy\"])  # Write the header\n",
    "                    writer.writerow([edge_accuracy])\n",
    "                \n",
    "                print(f\"Evaluation result saved to {output_file}\")\n",
    "                \n",
    "\n",
    "                # Store the results\n",
    "                results.append({\n",
    "                    \"hidden_dim\": hidden_dim,\n",
    "                    \"batch_size\": batch_size,\n",
    "                    \"learning_rate\": learning_rate,\n",
    "                    \"accuracy\": edge_accuracy\n",
    "                })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Overall Model:\n",
      "File: 1024-512-0.001-edge_accuracy.csv\n",
      "Edge Accuracy: 96.82781649245064\n",
      "hidden_dim: 1024\n",
      "batch_size: 512\n",
      "learning_rate: 0.001\n",
      "\n",
      "Worst Overall Model:\n",
      "File: 4542-512-0.01-edge_accuracy.csv\n",
      "Edge Accuracy: 70.01669570267131\n",
      "hidden_dim: 4542\n",
      "batch_size: 512\n",
      "learning_rate: 0.01\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Directory containing the CSV files\n",
    "directory = \"hyperparameter\"\n",
    "\n",
    "# Initialize variables to track the best and worst models\n",
    "best_model = {\"file\": None, \"accuracy\": float(\"-inf\")}\n",
    "worst_model = {\"file\": None, \"accuracy\": float(\"inf\")}\n",
    "\n",
    "# Iterate through all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\"-edge_accuracy.csv\"):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        try:\n",
    "            # Read the CSV file (expecting one value in the edge_accuracy column)\n",
    "            df = pd.read_csv(filepath)\n",
    "\n",
    "            # Ensure the file contains the 'edge_accuracy' column\n",
    "            if \"edge_accuracy\" in df.columns:\n",
    "                accuracy = df[\"edge_accuracy\"].iloc[0]  # Get the single accuracy value\n",
    "\n",
    "                # Update best model if the current accuracy is higher\n",
    "                if accuracy > best_model[\"accuracy\"]:\n",
    "                    best_model = {\"file\": filename, \"accuracy\": accuracy}\n",
    "\n",
    "                # Update worst model if the current accuracy is lower\n",
    "                if accuracy < worst_model[\"accuracy\"]:\n",
    "                    worst_model = {\"file\": filename, \"accuracy\": accuracy}\n",
    "            else:\n",
    "                print(f\"File {filename} does not contain the expected 'edge_accuracy' column.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {filename}: {e}\")\n",
    "\n",
    "best_hidden_dim = best_model[\"file\"].split(\"-\")[0]\n",
    "best_batch_size = best_model[\"file\"].split(\"-\")[1]\n",
    "best_learning_rate = best_model[\"file\"].split(\"-\")[2]\n",
    "worst_hidden_dim = worst_model[\"file\"].split(\"-\")[0]\n",
    "worst_batch_size = worst_model[\"file\"].split(\"-\")[1]\n",
    "worst_learning_rate = worst_model[\"file\"].split(\"-\")[2]\n",
    "# Output the results\n",
    "print(\"Best Overall Model:\")\n",
    "print(f\"File: {best_model['file']}\")\n",
    "print(f\"Edge Accuracy: {best_model['accuracy']}\")\n",
    "print(f\"hidden_dim: {best_hidden_dim}\")\n",
    "print(f\"batch_size: {best_batch_size}\")\n",
    "print(f\"learning_rate: {best_learning_rate}\")\n",
    "print(\"\\nWorst Overall Model:\")\n",
    "print(f\"File: {worst_model['file']}\")\n",
    "print(f\"Edge Accuracy: {worst_model['accuracy']}\")\n",
    "print(f\"hidden_dim: {worst_hidden_dim}\")\n",
    "print(f\"batch_size: {worst_batch_size}\")\n",
    "print(f\"learning_rate: {worst_learning_rate}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final test with optimal hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 0.043806, Train Acc: 0.9910, Val Loss: 0.005898, Val Acc: 0.9993\n",
      "Epoch [2/50], Train Loss: 0.004387, Train Acc: 0.9993, Val Loss: 0.003551, Val Acc: 0.9993\n",
      "Epoch [3/50], Train Loss: 0.002902, Train Acc: 0.9993, Val Loss: 0.002562, Val Acc: 0.9994\n",
      "Epoch [4/50], Train Loss: 0.002098, Train Acc: 0.9994, Val Loss: 0.001850, Val Acc: 0.9995\n",
      "Epoch [5/50], Train Loss: 0.001450, Train Acc: 0.9995, Val Loss: 0.001301, Val Acc: 0.9996\n",
      "Epoch [6/50], Train Loss: 0.000979, Train Acc: 0.9997, Val Loss: 0.000888, Val Acc: 0.9997\n",
      "Epoch [7/50], Train Loss: 0.000670, Train Acc: 0.9998, Val Loss: 0.000666, Val Acc: 0.9998\n",
      "Epoch [8/50], Train Loss: 0.000480, Train Acc: 0.9999, Val Loss: 0.000494, Val Acc: 0.9998\n",
      "Epoch [9/50], Train Loss: 0.000360, Train Acc: 0.9999, Val Loss: 0.000415, Val Acc: 0.9999\n",
      "Epoch [10/50], Train Loss: 0.000287, Train Acc: 0.9999, Val Loss: 0.000349, Val Acc: 0.9999\n",
      "Epoch [11/50], Train Loss: 0.000234, Train Acc: 0.9999, Val Loss: 0.000307, Val Acc: 0.9999\n",
      "Epoch [12/50], Train Loss: 0.000198, Train Acc: 0.9999, Val Loss: 0.000265, Val Acc: 0.9999\n",
      "Epoch [13/50], Train Loss: 0.000173, Train Acc: 0.9999, Val Loss: 0.000248, Val Acc: 0.9999\n",
      "Epoch [14/50], Train Loss: 0.000153, Train Acc: 1.0000, Val Loss: 0.000234, Val Acc: 0.9999\n",
      "Epoch [15/50], Train Loss: 0.000139, Train Acc: 1.0000, Val Loss: 0.000216, Val Acc: 0.9999\n",
      "Epoch [16/50], Train Loss: 0.000129, Train Acc: 1.0000, Val Loss: 0.000200, Val Acc: 0.9999\n",
      "Epoch [17/50], Train Loss: 0.000118, Train Acc: 1.0000, Val Loss: 0.000188, Val Acc: 0.9999\n",
      "Epoch [18/50], Train Loss: 0.000110, Train Acc: 1.0000, Val Loss: 0.000181, Val Acc: 0.9999\n",
      "Epoch [19/50], Train Loss: 0.000108, Train Acc: 1.0000, Val Loss: 0.000199, Val Acc: 0.9999\n",
      "Epoch [20/50], Train Loss: 0.000104, Train Acc: 1.0000, Val Loss: 0.000175, Val Acc: 0.9999\n",
      "Epoch [21/50], Train Loss: 0.000101, Train Acc: 1.0000, Val Loss: 0.000186, Val Acc: 0.9999\n",
      "Epoch [22/50], Train Loss: 0.000095, Train Acc: 1.0000, Val Loss: 0.000179, Val Acc: 0.9999\n",
      "Epoch [23/50], Train Loss: 0.000094, Train Acc: 1.0000, Val Loss: 0.000172, Val Acc: 0.9999\n",
      "Epoch [24/50], Train Loss: 0.000090, Train Acc: 1.0000, Val Loss: 0.000172, Val Acc: 0.9999\n",
      "Epoch [25/50], Train Loss: 0.000087, Train Acc: 1.0000, Val Loss: 0.000162, Val Acc: 0.9999\n",
      "Epoch [26/50], Train Loss: 0.000086, Train Acc: 1.0000, Val Loss: 0.000159, Val Acc: 0.9999\n",
      "Epoch [27/50], Train Loss: 0.000086, Train Acc: 1.0000, Val Loss: 0.000154, Val Acc: 0.9999\n",
      "Epoch [28/50], Train Loss: 0.000084, Train Acc: 1.0000, Val Loss: 0.000165, Val Acc: 0.9999\n",
      "Epoch [29/50], Train Loss: 0.000082, Train Acc: 1.0000, Val Loss: 0.000159, Val Acc: 0.9999\n",
      "Epoch [30/50], Train Loss: 0.000080, Train Acc: 1.0000, Val Loss: 0.000167, Val Acc: 0.9999\n",
      "Epoch [31/50], Train Loss: 0.000080, Train Acc: 1.0000, Val Loss: 0.000170, Val Acc: 0.9999\n",
      "Epoch [32/50], Train Loss: 0.000081, Train Acc: 1.0000, Val Loss: 0.000147, Val Acc: 1.0000\n",
      "Epoch [33/50], Train Loss: 0.000078, Train Acc: 1.0000, Val Loss: 0.000150, Val Acc: 1.0000\n",
      "Epoch [34/50], Train Loss: 0.000076, Train Acc: 1.0000, Val Loss: 0.000157, Val Acc: 1.0000\n",
      "Epoch [35/50], Train Loss: 0.000074, Train Acc: 1.0000, Val Loss: 0.000158, Val Acc: 1.0000\n",
      "Epoch [36/50], Train Loss: 0.000075, Train Acc: 1.0000, Val Loss: 0.000162, Val Acc: 1.0000\n",
      "Epoch [37/50], Train Loss: 0.000075, Train Acc: 1.0000, Val Loss: 0.000154, Val Acc: 1.0000\n",
      "Epoch [38/50], Train Loss: 0.000074, Train Acc: 1.0000, Val Loss: 0.000152, Val Acc: 1.0000\n",
      "Epoch [39/50], Train Loss: 0.000073, Train Acc: 1.0000, Val Loss: 0.000144, Val Acc: 1.0000\n",
      "Epoch [40/50], Train Loss: 0.000073, Train Acc: 1.0000, Val Loss: 0.000150, Val Acc: 1.0000\n",
      "Epoch [41/50], Train Loss: 0.000074, Train Acc: 1.0000, Val Loss: 0.000149, Val Acc: 1.0000\n",
      "Epoch [42/50], Train Loss: 0.000071, Train Acc: 1.0000, Val Loss: 0.000143, Val Acc: 1.0000\n",
      "Epoch [43/50], Train Loss: 0.000067, Train Acc: 1.0000, Val Loss: 0.000138, Val Acc: 1.0000\n",
      "Epoch [44/50], Train Loss: 0.000067, Train Acc: 1.0000, Val Loss: 0.000141, Val Acc: 1.0000\n",
      "Epoch [45/50], Train Loss: 0.000068, Train Acc: 1.0000, Val Loss: 0.000137, Val Acc: 1.0000\n",
      "Epoch [46/50], Train Loss: 0.000067, Train Acc: 1.0000, Val Loss: 0.000140, Val Acc: 1.0000\n",
      "Epoch [47/50], Train Loss: 0.000068, Train Acc: 1.0000, Val Loss: 0.000144, Val Acc: 1.0000\n",
      "Epoch [48/50], Train Loss: 0.000067, Train Acc: 1.0000, Val Loss: 0.000132, Val Acc: 1.0000\n",
      "Epoch [49/50], Train Loss: 0.000066, Train Acc: 1.0000, Val Loss: 0.000141, Val Acc: 1.0000\n",
      "Epoch [50/50], Train Loss: 0.000066, Train Acc: 1.0000, Val Loss: 0.000127, Val Acc: 1.0000\n",
      "Model saved to ffnn_model_best.pth\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.000123, Test Acc: 1.0000\n",
      "Test Accuracy:  0.9999592022205247\n",
      "Test Loss:  0.00012344330109324522\n",
      "Accuracy of NeighbourGraphPredictionModel:  96.91627149189475\n"
     ]
    }
   ],
   "source": [
    "testing_list = []\n",
    "for parts, graph in testing_set:\n",
    "    tuple = (parts, graph)\n",
    "    testing_list.append(tuple)\n",
    "\n",
    "# train the model with the best hyperparameters\n",
    "ffnn = FFNN(input_dim, int(best_hidden_dim), output_dim).to(device)\n",
    "\n",
    "# Train the model\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = ffnn.train_ffnn(\n",
    "    train_dataset=train_part_dataset,\n",
    "    val_dataset=val_part_dataset,\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=int(best_batch_size),\n",
    "    learning_rate=float(best_learning_rate),\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "file_path = f\"ffnn_model_best.pth\"\n",
    "torch.save(ffnn.state_dict(), file_path)\n",
    "print(f\"Model saved to {file_path}\")\n",
    "\n",
    "# Visualize the training results\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label=\"Train Accuracy\")\n",
    "plt.plot(val_accuracies, label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Test the model\n",
    "test_part_dataset = PartDataset(testing_set)      # --> Länge 59275 alle parts in allen Graphen (Alle graph - PArtID combis)\n",
    "\n",
    "avg_test_loss, test_accuracy = ffnn.test_ffnn(test_part_dataset, batch_size=int(best_batch_size), device=device)\n",
    "\n",
    "print(\"Test Accuracy: \", test_accuracy)\n",
    "print(\"Test Loss: \", avg_test_loss)\n",
    "\n",
    "\n",
    "# Initialize and load the model\n",
    "builder = GraphBuilder()\n",
    "\n",
    "# Load the best model\n",
    "builder.load_model(input_dim, int(best_hidden_dim), output_dim, file_path)\n",
    "\n",
    "# Evaluate the model\n",
    "edge_accuracy = evaluate(builder, testing_list)\n",
    "print(\"Accuracy of NeighbourGraphPredictionModel: \", edge_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
